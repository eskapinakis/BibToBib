% #############################################################################
% This is Chapter 1
% !TEX root = ../Main.tex
% #############################################################################



\chapter{Complexity}

This chapter serves as an introduction to complexity theory from a ``Structural'' point of view. Here, structural means that were are concerned with complexity classes, rather than computational complexity of individual problems and algorithms.
A very compact description of the field is made by Hartmanis in \cite{hartmanis:90}:

\begin{quote}
Structural complexity theory is the study of the relations between various complexity classes and the global properties of individual complexity classes. It studies logical implications among certain unsolved problems about complexity classes, explores the power of various resource bounded reductions and the properties of the corresponding complete languages. It uses relativization to clarify the power of 
different access mechanisms to information, to investigate possible relationships among complexity classes and to try to assess the difficulty of unsolved problems by contradictory relativizations. 
\end{quote}

The two main references for the chapter are Balcázar, Díaz and Gabarró \cite{balcazar:88,balcazar:90} and Du and Ko \cite{du:11}.


\section{Introduction}


In this chapter we will introduce the main complexity classes and some of the known techniques to study their relationship. 
See Stearns \cite{stearns:90} for an overview of the birth and evolution of this field.

As mentioned before, we will see some generalizations of the Turing machine. The first is the nondeterministic Turing machine, in which the $\delta$ function can be taken to be a relation. We thus allow different outcomes for the same situation, hence the nondeterministic name. In the case of nondeterministic Turing machines, we cannot, in general, speak of the sequence of configurations, but rather of the tree of configurations, any branch of finite length being a computation.
%; the second is the probabilistic Turing machine, in which a step in the computation can be decided \textit{randomly}.



If we impose a restriction on the amount of a resource (such as time or space) that can be consumed during a computation, we expect the power of the Turing machine to decrease. This is actually not a trivial matter, and depends on the notion of constructibility, introduced by Yamada \cite{yamada:62}. Without imposing this constructibility restriction, we can prove the existence of arbitrarily large gaps between complexity classes. This result, known as the Gap theorem, was proven by Trakhtenbrot \cite{trakhtenbrot:64} and, independently, by Borodin \cite{borodin:72}.


We will briefly present results on time constructibility and the Gap theorem in Section \ref{sec:timeConstructible} from the Appendix. For now, it is only relevant to know the definition of time (or space) constructible functions and that the ``usual'' functions considered fall into both categories.


\begin{defn}\label{def:constructible}
A function $f:\mathbb{N}\to\mathbb{N}$ is time constructible (originally, real-time countable), if there exists a deterministic Turing machine $M$ and a number $p\in\N$ such that, for any input word of size $n\geq p$, $M$ halts after exactly $f(n)$ transitions.

A function $f:\mathbb{N}\to\mathbb{N}$ is space constructible\footnote{Savitch \cite{savitch:70} denotes these functions by \textit{measurable}.} if there exists a deterministic Turing machine $M$ and a number $p\in\N$ such that, for any input word of size $n\geq p$, $M$ halts in a configuration with exactly $f(n)$ non-blank cells, and no more cells were read during the computation.
\end{defn}


Although some studies suggesting structural complexity had appeared before, \cite{rabin:60,rabin:59,ritchie:63}, the first ``direct assault on the structure of deterministic time complexity'' \cite{selman:88} came in the seminal paper by Hartmanis and Stearns \cite{hartmanis:65}, communicated in the Fifth Annual Symposium on Switching Circuit Theory and Logical Design, in $1964$ \cite{hartmanis:64}. This marked the first time that the name ``computational complexity'' was used, a name that persists to this day.


Given a function $T$, we denote by $DTIME(T)$ (resp. $DSPACE(T)$) the class of problems that can be decided by a deterministic multi-tape Turing machine in time (resp. space) $T$. The non-deterministic counter-parts $NTIME$ and $NSPACE$ are defined analogously. For example, $NTIME(n)$ is the class of problems that can be solved by a nondeterministic Turing machine in linear time. Note that we are now considering multi-tape Turing machines, instead of Turing machines with just one tape, as they were introduced by Turing. This is because, while increasing the number of tapes does not affect what can be solved in a finite amount of time, it affects the time it takes to perform a computation. Thus, the multi-tape Turing machine is a more suitable model to classify the computational complexity of algorithms. This idea of increasing the number of tapes in a computational model appears earlier, for example in Rabin and Scott \cite{rabin:59b}, but applied to Automata, instead of Turing machines.

One may ask, of course, under which conditions of $T$ and $T'$, can we collapse or separate the classes $DTIME(T)$ and $DTIME(T')$.
For the first question, Hartmanis and Stearns present their ``speed-up theorem'', which states that multiplicative constants do not affect the computational class. 

\begin{thm}\label{thm:speed_up}
If $T$ is time constructible, $DTIME(T) = DTIME(kT)$, for any computable positive real number $k$.
\end{thm}

For the second question, they introduce a diagonalization process over Turing machines with a resource bound, to obtain a problem that cannot be solved by either diagonalized machine. To do this, however, we have to perform a call to a Universal Turing machine, i.e., one that can simulate any other, given its code, and take into account the ``time cost'' of the simulation. In Hartmanis and Stearns, this cost is calculated to $T^2$, for $T$ the time bound of the machine being simulated, a result that was improved later by Hennie and Stearns \cite{hennie:66}, to $T\log (T)$.

We thus have a first criteria for separating two complexity classes.

\begin{thm}\label{thm:inf_diagonal}
If $T(n)$ and $U(n)$ are two time functions such that 
\begin{align*}
    \inf \frac{T(n)\log T(n)}{U(n)} = 0
\end{align*}
then, there exists a function that can be computed within the time bound $U(n)$ but not within the time bound $T(n)$. 
\end{thm}


In Stearns, Hartmanis and Lewis \cite{stearns:65}, the above results are proven with respect to the resource of space. The first statement has the same formulation (with DSPACE instead of DTIME), and we shall refer to it as the memory compression theorem; the second is almost identical, but the term $\log T(n)$ is not needed. These results, which we prove in Section \ref{sec:multi_tape} from the Appendix, extend to nondeterministic Turing machines.

We can now start identifying simple complexity classes. From the speed-up and memory-compression theorems, we can condense the notation to omit multiplicative constants and, from the hierarchy theorems, we can easily verify that the classes below do not collapse. 

The central complexity classes are:
\begin{align*}
    &LOG = \cup_{c\geq 1}DSPACE(c\cdot \log(n))&\\
    &NLOG = \cup_{c\geq 1}NSPACE(c\cdot \log(n))&\\
    &P = \cup_{c\geq 1}DTIME(n^c)&\\
    &PSPACE = \cup_{c\geq 1}DSPACE(n^c)&\\
    &NP = \cup_{c\geq 1}NTIME(n^c)&\\
    &NPSPACE = \cup_{c\geq 1}NSPACE(n^c)&\\
    &DEXT = \cup_{c\geq 1}DTIME(2^{c\cdot n})&\\
    &NEXT = \cup_{c\geq 1}NTIME(2^{c\cdot n})&\\
    &EXPSPACE = \cup_{c\geq 1}DSPACE(2^{c\cdot n})&\\
    &EXPTIME = \cup_{c\geq 1}DTIME(2^{n^c})&\\
    &NEXPTIME = \cup_{c\geq 1}NTIME(2^{n^c})&\\
\end{align*}


First works to devote attention to the class P are by Cobham \cite{cobham:64} and Edmonds \cite{edmonds:65}, although the names P and NP were only coined later by Karp \cite{karp:72}. Cobham remarks how some simple functions, such as summation or multiplication, can be performed in a polynomial number of steps, in the size of the inputs, and how this seems to be a property of simple functions in general. He calls $\mathscr{L}$ the class of all functions with this property.
He also makes a mention to a class of functions that Bennet \cite{bennet:62} calls the \textit{extended rudimentary functions}.

Edmonds dedicates a section to discuss the meaning of an \textit{efficient algorithm} and introduces the order of difficulty of an algorithm by considering its behaviour with arbitrarily large input sizes. He presents an algorithm for finding a matching in a graph with an upper bound on the order of difficulty of $n^4$.

Before their work, various restricted classes of functions had been proposed as feasible, as the class of functions computable by finite automata. However, as Ritchie \cite{ritchie:63} remarks, this class is too severely restricted, as it does not even include multiplication. 
Using the notion of a predictably computable functions, one whose computational complexity can be ``predicted'', Ritchie characterizes the class of elementary functions \cite{kalmar:43,csillag:47}. 


The class PSPACE appears in \cite{berman:77} as PTAPE and in \cite{stockmeyer:73,even:76} as polynomial-space. In \cite{stockmeyer:77}, the name PSPACE already appears.

Because of the time hierarchy results, by Hartmanis and Stearns, exponential problems were known to exist. However the diagonalization method gives no insight on the properties of the language obtained. Meyer and Stockmeyer \cite{meyer:72} give an example of a ``natural'' language that requires exponential space (and thus time) to be decided.
In \cite{berman:77}, EXPSPACE appear as EXPTAPE and DEXT as EXPTIME. There seems to be no consensus in the literature about this second distinction, but we will use DEXT to mean Deterministic EXponential Time and EXPTIME to mean EXponential Polynomial exponent TIME.


In the next sections we will present some of the inclusion and separation results that are known for these classes. The main problems in complexity theory are the trade-of between determinism and non-determinism, i.e., the relationship between P and NP, and the trade-of between time and space, i.e., the relationship between P and PSPACE. 
In Section \ref{sec:natural_proofs} from the Appendix we will discuss why these classes seem to be so hard to separate, by presenting Razborov and Rudich's concept of a natural proof \cite{razborov:94}.



\subsection{Algorithms}\label{sec:algorithms}


Some of the development in computational complexity could not be made without the knowledge of specific algorithms. This section is thus devoted to the presentation of some of the procedures that have shaped the area.

Djikstra


The following algorithm is an adaptation of the procedure given by Savitch \cite{savitch:70}, and tests if two nodes of a graph, $u$ and $v$, can be connected in, at most, $2^j$ steps.
%, tests if there is a path between two configurations $I_1$ to $I_2$, of length at most $2^j$.
%Note how it is similar to the algorithm in Lewis, Stearns and Hartmanis \cite{stearns:65}.

\begin{center}
\begin{algorithm}[H]
\,\\
\textbf{Input:} $u,v,j$\;
\eIf{$n=0$}{
    \Return $v = v$ or $u\vdash v$
}{
    Search for a node $x$ such that $Reachable(u,x,j-1)$ and $Reachable(x,v,j-1)$\;
    Return True if and only if such $x$ is found
}
\,\\
\caption{Reachable}
\label{alg:reachable}
\end{algorithm}
\end{center}

If a graph has $n$ nodes, then we need $\log(n)$ bits to encode the different vertices. Since $n$ steps are enough to connect (if it's possible) any two vertices, testing the connectivity of two vertices can be done with $j = \log(n)$. If the recursive calls are erased from memory, if the base yielded false, then there will be, at most, $\log(n)$ processes being kept simultaneously. This gives an overall space complexity in $\Oo(\log^2(n))$ and time complexity in $\Oo(n)$.




The following is Rabin's probabilistic algorithm for testing the primality of a number

Let $n$ be a natural number and, for each integer $1<x<n$ denote by $W(x)$ the following condition: either $x^{n - 1}\not\equiv 1$ (mod $n$), or there exists an integer $m$ of the form $m = (n-1)/2^i$, such that $x^{m - 1}$ and $n$ have a common divisor different from $1$ and $n$.

The algorithm is based upon the following property: if $W(x)$ holds for some $x$, then $n$ must be composite; and if $n$ is composite, at least half of the integers $x$ between $1$ and $n$ satisfy $W(x)$.


\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $n$\; 
Choose randomly $m$ integers, $x_1, x_2,\dots, x_m$, between $1$ and $n-1$\;
For each $x_j$, test whether $W(x_j)$ holds\;
If $W(x_j)$ holds for some $x_j$, then accept\; 
else reject 
\caption{Probabilistic Algorithm for testing primality}
\label{alg:probabilistic_prime}
\end{algorithm}
\end{center}

If the algorithm accepts, then $n$ must be composite, because an $x$ has been found for which $W(x)$ holds.
Now suppose that $n$ is composite. Then, at least half of the integers $1\leq x<n$ satisfy $W(x)$, so the probability that all the chosen $x_i$ fail to witness the compositeness of $n$, and thus that the algorithm wrongly rejects the input, is smaller than $(1/2)^m$.










\section{Relationship between four basic complexity classes}


The results of this section are summarized in the diagram bellow. 

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[node distance=0.6cm]
%Complexity Classes
\node (P0) {$DTIME(t)$};
\node (P1) [right=of P0] {$NTIME(t)$};
\node (P2) [right=of P1] {$DSPACE(t)$};
\node (P3) [right=of P2] {$NSPACE(t)$};
\node (P4) [right=of P3] {$DTIME(2^{\Oo(t)})$};
\node (P5) [below=of P2] {$DSPACE(t^2)$};
\node (P6) [above=of P3] {$co-NSPACE(t)$};
%Propositions
\node at (3.95,0.4) {\ref{prop:space_search}}; % NTIME in DSPACE
\node at (8.2,-1) {\ref{prop:savitch}}; % Savitch
\node at (7.65,0.6) {\ref{prop:nspace_conspace}}; % Nspace co-Nspace
\node at (9.7,0.4) {\ref{prop:dijkstra}}; % NSPASCE in DTIME
%Inclusions
\path[->,thick]
    (P0) edge (P1)
    (P1) edge (P2)
    (P2) edge (P3)
    (P2) edge (P5)
    (P3) edge (P4)
    (P3) edge[bend left] (P5)
    (P3) edge (P6)
    (P6) edge (P3);       
\end{tikzpicture}
\caption{Diagram representing the structural relationship between four simple complexity classes, for $t$ a time constructible function.}\label{fig:complexity_diagram}
\end{center}
\end{figure}


\begin{prop}\label{prop:space_search}
$NTIME(t)\subseteq DSPACE(t)$
\end{prop}
\begin{proof}
Let $M$ be a nondeterministic Turing machine that decides $A$ in time $t$. Consider $M'$, a deterministic Turing machine containing one more tape than $M$

Since $t$ is time constructible , $M'$ can simulate $t$ transitions of every possible computation of $M$, until it finds an accepting one. If none is found, $M'$ reject the input.

Now, every computation can be simulated by reusing the same space occupied by $M$, we only need a sequence of $t$ cells in the extra tape to indicate which branch of the computational tree of $M$ was chosen.

Thus, the amount of space required is $2t$, whence, by the memory compression theorem (see Section \ref{sec:multi_tape} from the Appendix), $NTIME(t) \subseteq DSPACE(t)$.
\end{proof}


The result below was first proven by Savitch \cite{savitch:70}, and implies that PSPACE = NPSPACE, the space complexity version of the P vs NP problem. The proof requires a call to Algorithm \ref{alg:reachable}, with the configurations of a Turing machine.



\begin{prop}\label{prop:savitch}
If $s(n)\geq \log(n)$ is space constructible, $NSPACE(s)\subseteq DSPACE(s^2)$.
\end{prop}
\begin{proof}
Suppose that $A$ is decidable by a Turing machine $M$, whose space is bounded by $s$. Since we can always add the instruction ``clear the tape and move the head to the first cell'', before accepting an input, we may assume, without loss of generality, that $M$ has only one starting configuration $I_1$ and one accepting configuration $I_2$.

If $M$ accepts an input $w$, there is some computation in $M$ where $w$ is accepted and no configuration is repeated, so we may consider that the number of configurations of a computation of $M$ is bounded above by $2^{cs(n)}$, for some constant $c\in\N$.

Thus, using Algorithm \ref{alg:reachable}, with input ($I_1$, $I_2$, $cs(n)$), we can test if the final configuration is reachable from the first. Since the graph of configurations of $M$ has $2^{cs(n)}$ nodes, this procedure has space complexity $\Oo(s(n)^2)$.
\end{proof}

The restriction $s(n)\geq \log(n)$ appears because we need at least $\Oo(\log(n))$ bits to encode the configurations of $M$. Think, for example, that, for an input of size $n$, we need $\log(n)$ bits just to encode the position of the reading head.

Using the same reasoning, we get the following.


\begin{prop}\label{prop:dijkstra}
If $s$ is space constructible and $s(n) \geq \log(n)$, $NSPACE(s)\subseteq DTIME(2^{\Oo(s)})$.
\end{prop}

If we drop the space-constructibility assumption, we we cannot write down $s(n)$ initially, but we can repeat the entire procedure above for $S = 0, 1, \dots$ until the right $s(n)$ is found.
The time of this process is bounded by
\begin{align*}
    &\sum_{S=0}^{s(n)} d^S = \frac{d^{s(n)+1} - 1}{d-1} \in 2^{\Oo(s(n))}&
\end{align*}
and the space by
\begin{align*}
    &\sum_{S=0}^{s(n)} S^2 \in \Oo(s(n)^2)&
\end{align*}




In the year $1987$, some space bounded hierarchies were shown to be finite (see for example Toda \cite{toda:87} and Lange, Jenner and Kirsig \cite{lange:87}).
% structural complexity tem mais umas
These results can be seen as a consequence of the proposition below, due to Immerman \cite{immerman:88} and Szelepcs{\'e}nyi \cite{szelepcsenyi:88}, which states that non-deterministic space complexity classes are closed under complement, whenever the resource bound is, at least, logarithmic. 
We will see in Section \ref{sec:poly_hierarchy} how this closure property would imply the collapsing of the Polynomial Time Hierarchy (Proposition \ref{prop:complement_poly_collapse}).

One may ask why proving that $NSPACE(s)$ is closed for complement cannot be done by simply taking an $NSPACE(s)$ Turing machine and switching the accepting and rejecting states. This is because of the definition of acceptance by a nondeterministic Turing machine. 
If some input is accepted in a branch and rejected in another, then, switching accepting and rejecting states will not change this fact. The input will still be accepted, only now its acceptance will be witnessed by another branch.

The proof will be made in two steps. First, we will show that counting the exact number of reachable configurations of a $NSPACE(s)$ machine can be done in $NSPACE(s)$. Then, that, with this calculation, we can detect rejection and acceptance.


\begin{lemm}\label{lemm:counting_conf}
Counting the exact number of reachable configurations of a $NSPACE(s)$ machine can be done in $NSPACE(s)$.
\end{lemm}
\begin{proof}
Let $M$ be an NSPACE(s) machine with initial configuration $C_0$. We will show, by induction, that we can calculate in $NSPACE(s)$ the number of configurations reachable from $C_0$ in, at most, $d$ steps. Call this number $N_d$.
Then, the number of reachable configurations $N$ will the first $N_d$ such that $N_d = N_{d+1}$.

For the base case, $N_0 = 1$, so, supposing we can obtain $N_d$, we show how to calculate $N_{d+1}$. 

\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $M$\;
Set a counter $k = N_d$\;
\For{every configuration $C_t$ of size $s(n)$}{
    \For{every configuration $C$ reachable from $C_0$ in $d$ steps}{
        \If{$C_t = C$ or $C\vdash C_t$}{
            $k = k+1$
        }
    }    
}
\Return $k$
\caption{Counting reachable configurations}
\label{alg:counting_reachable}
\end{algorithm}
\end{center}

Since $N$ is in $2^{\Oo(s(n))}$, the space needed to store $N$ is in $\Oo(s(n))$.
\end{proof}


\begin{prop}\label{prop:nspace_conspace}
If $s$ is space constructible and $s(n) \geq \log(n)$, then $NSPACE(s) = co-NSPACE(s)$.
\end{prop}
\begin{proof}
Suppose we are given an $NSPACE(s)$ machine $M$. We specify the following algorithm to decide $co-NSPACE(s)$.
\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $x$\;
Let $C_0$ be the initial configuration of $M$ and $N$ be the number of configurations of size $s(n)$ reachable by $M$ from $C_0$\;
Set a counter $k = 0$\;
\For{every configuration $C_t$ of length $s(n)$}{
    Guess a computation of $M$ from $C_0$ to $C_t$\;
    \If{Path is found}{
        \If{$C_t$ is an accepting configuration}{
            Reject $x$\,
        }
        $k = k+1$\;
    }
}
Accept $x$ iff $k = N$
\caption{Deciding co-NSPACE(s(n))}
\label{alg:complement_space}
\end{algorithm}
\end{center}

If $M$ accepts $x$, the accepting configuration will eventually be found, so the algorithm rejects $x$; if $M$ rejects $x$, then all the $N$ reachable configurations must be rejecting, so the algorithm accepts $x$.
\end{proof}


Hartmanis \cite{hartmanis:90} remarks how it took over $20$ years to prove the above equality, stating that few people expected nondeterministic space to be closed under complementation.
The power of ``counting'' in this context seems to have been unexpected. For more applications of counting in structural complexity, see Schöning \cite{schoning:90}.


\section{Small and fast universal Turing machine}


From \cite{neary:06}.




\section{Propagation techniques}

We discuss techniques allow one to ``propagate'' results between different complexity classes. The idea is that some results may not have to be proven directly, but in a different setting and then propagated to our original goal.  

\subsection{Many-to-one reducibility}

We start by definition the notion of many-to-one reducibility.

\begin{defn}\label{def:m_reducibility}
Given two sets $A$ and $B$, we say that $A$ is polynomial time many-to-one reducible to $B$ if there is a function $f\,:\,\Sigma^*\to\Sigma^*$, computable in polynomial time, such that $x\in A$ if and only if $f(x)\in B$, for every $x\in \Sigma^*$.
\end{defn}

If the above conditions are met, we say that $A\leq_m B$ \textit{via} $f$. Intuitively, if $A\leq_m B$, we can decide whether a word belongs to $A$ by testing whether its output belongs to $B$. Since $f$ is computable in polynomial time, if $B$ is decidable in polynomial time, so will $A$ be.

One can verify that both $\leq_m$ defines an equivalence relation, which may be used to define ``degrees'' of difficulty. Using this concept of reducibility, Cook and Karp \cite{cook:71,karp:72} defined the notion of NP-complete problems. Karp used the $m$-reducibility, while Cook used the Turing-reducibility (Section \ref{sec:reducibility_relativization}). 
Berman and Hartmanis \cite{hartmanis:76} showed that all known NP-complete sets are P-isomorphic, i.e., between any two NP-complete sets there are polynomial-time bijective reductions, with polynomial-time computable inverses. This led them to conjecture that all NP-complete sets are P-isomorphic, which would imply that $P \neq NP$.


\begin{defn}\label{def:complete}
Let $\Cc$ be some complexity class and $A$ a set. We say that $A$ is $\Cc$-hard if every $B\in \Cc$ can be reduced to $A$; we say that $A$ is $\Cc$-complete if, besides the previous condition, $A\in\Cc$.
\end{defn}


In \cite{cook:71} Cook proved that SAT is NP-complete, by showing that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be reduced to that of determining whether a given propositional formula is a tautology.
The notion of ``brute force search'' (or \textit{perebor}) led Levin \cite{levin:73} to the same concept. He presented six problems with the following ``completeness property'': if perebor is inevitable for some algorithms (another way to phrase $P\neq NP$), then it is inevitable for these. Problem $3$ was SAT. He writes the following:

\begin{quote}
If we assume (...) that there exists some (...) problem (...) that is unsolvable by simple (...) algorithms, then it can be (...) shown that many ``classical'' search problems have the same property.
\end{quote}

See Trakhtenbrot \cite{trakhtenbrot:84} for a historical account of the research being conducted in this period in the Soviet Union. It contains, in the Appendix, an English translation of Levin's paper, which is were we extracted the above quote. 


The first known PSPACE-complete problems appears in Karp \cite{karp:72}. We will give the example of QBF (quantified boolean formulas), from Meyer and Stockmeyer \cite{stockmeyer:73}, as a PSPACE complete set, which will be used as an example of why the question of whether P equals NP does not relativize (Proposition \ref{prop:oracle_p_np}).

A question that may arise is whether every class admits complete sets. We will see later that this is not the case (Proposition \ref{prop:dext_no_complete}). 
For now, think of the class NP and consider the following set
\begin{align*}
    &K = \{(M,x,1^t)\,:\,M\,\text{ is a nondeterministic machine accepting $x$ in less than $t$ setps}\}&
\end{align*}
Let $A\in NP$ be some set, decided by a nondeterministic machine $M$, working in time $p(n)$. Then, testing whether $x$ belongs to $A$ is reduced to testing whether $(M,x,1^{p(n)})\in K$, i.e., $A\leq_m K$. This means that $K$ is NP-hard. One can verify, further, that $K\in NP$, whence NP admits complete sets.
Of course that it is not a very ``satisfactory'' complete set, in the sense that it was built exactly to be complete (one may recognize that it is the analogous of the Halting Problem for NP), but that is not the point here. The above set suggest a ``uniform'' way of constructing $\Cc$-hard sets, by considering the ``$\Cc$-version'' of the halting problem.

One may ask then if there are ``natural'' complete sets. We give two examples in Section \ref{sec:complete_sat_qbf} from the Appendix: SAT for NP (Proposition \ref{prop:sat_complete}) and QBF for PSPACE (Proposition \ref{prop:qbf_complete}). These will be the only problems whose completeness will be shown directly. After their completeness has been established, we can show, for example, that a set $A$ is NP-complete, by showing that $SAT\leq_m A$ (Proposition \ref{prop:clique_complete}).


To illustrate this method, we show the following example from \cite{karp:72}. CLIQUE is the problem of, given a graph $G = (V,E)$ and a natural number $k$, determining whether $G$ contains a complete subgraph with at least $k$ vertices.


\begin{prop}\label{prop:clique_complete}
CLIQUE is NP-complete
\end{prop}
\begin{proof}
The fact that CLIQUE is in NP can be easily observed. A nondeterministic Turing machine only has to guess a subset $V' \subseteq V$ of size $k$ and check that every pair in $V'$ is connected by some edge in $E$.

To see that CLIQUE is NP-complete, we will show how to reduce CNF-SAT to CLIQUE. Since CNF-SAT is NP-complete, so will CLIQUE be.


Consider a formula $\phi = \phi_1\,\wedge\,\dots\,\wedge\,\phi_n$, where each $\phi_i$ is a disjunction of $n_i$ literals $\phi_{ij}$. 
The following graph $G$ can be constructed in polynomial time:
\begin{itemize}
    \item $V = \{(i, j)\,:\, 1 \leq i \leq n\,,\,1 \leq j \leq  n_i\}$
    \item $E = \{((i,j)\,,\,(k,l))\,:\,i\neq k\,,\,\neg\phi_{i,j}\neq\phi_{k,l}\,,\,\phi_{i,j}\neq\neg\phi_{k,l}\}$
\end{itemize}
Essentially, the vertices contain every possible ``pairing'' of literals and the edges connect every pair of literals that are not the negation of each other.
We will see that $\phi$ is satisfiable iff $G$ has a clique of order $n$.

If $\phi$ is satisfiable by some attribution, there is, in each formula $\phi_i$, a literal $\phi_{i,j}$ that is set to $1$. We construct the following clique of order $n$: $V' = \{(i, j_i)\,:\, i = 1,\dots,n\}$.
If $i\neq k$ and the edge $(i,j_i)\to(k,j_k)$ is not in $E$, then either $\neg\phi_{i,j}=\phi_{k,l}$ or $\phi_{i,j}=\neg\phi_{k,l}$, which is impossible since both literals have to have been set to $1$. Thus, $V'$ produces a complete subgraph of $G$ of order $n$.

If $G$ has a clique of order $n$, we construct the following attribution: if $x$ appears in the clique, we set $V(x) = 1$; otherwise, if $\neg x$ appears in the clique, we set $V(x) = 0$. Since, in each formula $\phi_i$, an element $\phi_{i,j}$ of the clique has to appear, it will make the formula true. Note that the definition of $E$ prevents some variable $x$ from receiving both the value $0$ and $1$.
\end{proof}


Besides allowing for a stratification of complexity classes, the notion of reduction provide an interesting separation tool. If, for example, we suspect that some class $\Cc$ is properly contained in another class $\Cc'$, we may be able prove that they are different by showing that $\Cc'$ has complete problems and $\Cc$ does not (Proposition \ref{prop:dext_no_complete}). 


\subsection{Sparse and tally sets}

In this section we examine languages that are easy to recognize. If proven that any of them are complete for hard complexity classes, we obtain a collapsing result. We start with two definitions.


\begin{defn}
A language is \textit{tally} if its alphabet contains only one symbol. We say that a set $A$ is \textit{sparse} if there is a polynomial $p$, such that the number of strings in $A$ of length at most $n$ is at most $p(n)$.
\end{defn}

Clearly, every tally language is sparse (there are, at most, $n+1$ words of length bounded by $n$), but the reverse is not true. For $\Sigma = \{0,1\}$, for example, let $A$ be the language which, for every $n$, contains the first $n$ words of length $n$. Then, $A$ is not a tally set and $C_A(n)\leq n^2$.

Given a set $A$, we define its tally version $tally(A) = \{0^n\,:\,n\in A\}$. Note that the tally version of a set is much simpler to decide than the original. The decision $0^n\in tally(A)$ can be made by testing if $n\in A$, which is a test with an input of logarithmic size in comparison with our original problem. We thus have, for example, that $A\in DEXT$ (resp. $NEXT)$ iff $tally(A) \in P$ (resp. $NP$). This observation leads to the following result from Book \cite{book:74}.

\begin{prop}
$DEXT \neq NEXT$ if and only if there are tally sets in $NP\setminus P$.
\end{prop}
\begin{proof}
From the above remark $tally(A)\in NP\setminus P$ if and only if $A \in NEXT$ and $A\notin DEXT$, which concludes the result.
\end{proof}

Replacing $NEXT$ with $EXPSPACE$ and $NP$ with $PSPACE$ we can get a similar result.

The following result is due to Berman \cite{berman:78}.

\begin{prop}
If a Tally set is NP-complete, then P = NP.
\end{prop}
\begin{proof}
(TODO)
\end{proof}


If the conjecture by Berman and Hartmanis \cite{hartmanis:76}, that all NP-complete sets are P-isomorphic, holds, then all NP-complete sets have to have similar density. Since SAT has an exponential density, this would imply that no sparse set could be NP-complete, unless P = NP. 

An interesting consequences of assuming the existence of sparse NP-complete sets is that they could be precomputed up to some given length and stored in a table; then, to solve instances up to some appropriate length of an arbitrary NP-complete problem, one would compute a reduced value of the instance and look up that value in the table. Note how this relates to non-uniform complexity (Section \ref{sec:non_uniform_complexity}).



Fortune \cite{fortune:79} strengthened Berman's result, by showing that, if a co-NP-complete set is reducible to a sparse set, then P = NP.
To extend these results to sets in NP, Hartmains and Mahaney \cite{hartmanis:80b} introduced the notion of a census function.

\begin{defn}
The census function of a set $A$ is the function that, given a natural number $n$, returns the number of words in $A$ having length at most $n$. We denote it by $C_A(n)$.
\end{defn}


They showed the following.


\begin{prop}
If $A$ is a sparse set in $NP$ with a census function computable in polynomial time, then $\overline A\in NP$. 
\end{prop}
\begin{proof}
Consider the following algorithm.

\begin{center}
\begin{algorithm}[H]
\textbf{Input:} x\;
Compute $k := C_A(|x|)$\;
Guess $k$ different strings $y_1,\dots,y_k$\; 
Verify that $y_1\dots y_k\in A$\;
Verify that each $y_i$ is different from $x$\; 
Accept $x$ iff both conditions are met
\label{alg:census_complement}
\caption{Deciding the complement with the census function}
\end{algorithm}
\end{center}

So, if $x\notin A$, the algorithm will guess $C_A(|x|)$ words in $A$ that are different from $x$. Since $A$ is sparse, there are only a polynomial number of words to be guessed. Sine $x$ is not one of them, the algorithm can conclude that $x\notin A$.

If $x\in A$, then, in the guessing process, for the first verification to succeed, $x$ will have to be included in the list of guessed words, whence the second verification must fail.
\end{proof}


Thus, if there is an NP-complete sparse set, with polynomial time computable census, we have that NP = co-NP, whence, by Fortune's result, the equality P = NP the follows.


These considerations were settled by Mahaney \cite{mahaney:82}, who showed that, if there are sparse NP complete sets, then P = NP. The corresponding question for Turing reductions will be discussed in Section \ref{sec:non_uniform_poly_hierarchy}.




\subsection{Lifting and Padding}

The lifting and padding technique consists in adding to the words of a hard language an easy-to-recognize tail, increasing their length and creating a language that is easier to recognize than the initial one.

We will present some examples of this technique. Book \cite{book:76} contains a general framework for the study of these ``translation lemmas''.

%savitch:70,simon:75

We start with simplified versions of two results from Book \cite{book:72}.

\begin{prop}\label{prop:padding_dspacen}
If $DSPACE(n)\subseteq P$, then $P = PSPACE$.
\end{prop}
\begin{proof}
Let $L\in PSPACE$ and $M$ be a machine that decides $L$ is polynomial space $p(n)$. Consider the following set:
\begin{align*}
    &L' = \{w10^{p(|w|)}\,:\,w\in L\}&
\end{align*}
Essentially, $L'$ contains the words $w\in L$, \textit{padded} with the amount of space that $M$ requires to decide if $w\in L$. Then, we can decide $L'$ is linear space by calling $M$ on $w$. When receiving $w10^{p(|w|)}$, we just have to remove the final $10^{p(|w|)}$ part.
Thus, if $DSPACE(n)\subseteq P$, we can decide $L'$ in polynomial time using some machine $N'$. To decide $L$, we just have to consider a machine $N$, which, on input $w$, appends to it $10^{p(|w|)}$ and calls $N'$.
\end{proof}

This is an interesting result, since it implies that $P = PSPACE$ if and only if $DSPACE(n)\subseteq P$. 

Note that we are implicitly using the fact that P is closed under $m$-reducibility, when we claim that $L'\in P$ implies that $L\in P$. 

To see how this can be used as a separation technique, consider the example below.

\begin{prop}\label{prop:p_neq_dspacen}
$P\neq DSPACE(n)$.
\end{prop}
\begin{proof}
If $P = DSPACE(n)$, then, by the previous result, $PSPACE = P = DSPACE(n)$, which contradicts the space hierarchy theorem.
\end{proof}


As a propagation result, consider a weaker version of a result from Hartmanis and Hunt \cite{hartmanis:74} and Book \cite{book:74}. Savitch \cite{savitch:70} contains a similar result.


\begin{prop}\label{prop:padding_p_np_dext_next}
If $P=NP$, then $DEXT = NEXT$.
\end{prop}
\begin{proof}
Since $DEXT\subseteq NEXT$, we only have to prove one inclusion. Let $L\in NEXT$ and $M$ be a nondeterministic Turing machine that decides $L$ in time $2^{cn}$. Consider the padded version of $L$
\begin{align*}
    &L' = \{w10^{2^{cn}}\,:\,w\in L\}&
\end{align*}
Then, a nondeterministic machine can decide $L'$ in linear time by making a call to $M$ on the truncated input. Thus, since $N=NP$, $L'$ can be decided by a deterministic machine $N'$ in linear time. To see that $L\in DEXT$, we just have to consider the machine $N$, which, receiving an input $w$, creates the word $w10^{2^{cn}}$ and calls $N'$ to make the decision.
\end{proof}


One can see that the above result can be generalized for $DTIME(2^{\Oo(f)})$ and $NEXT(2^{\Oo(f)})$, for any time constructible function $f$.

To see how these techniques may be used in conjunction with the notion of reducibility, consider the two examples below.


\begin{prop}\label{prop:nspace_has_np_complete}
$NSPACE(n)$ has NP-complete problems.
\end{prop}
\begin{proof}
Let $L$ be an NP-complete set and define the padded version of $L$
\begin{align*}
    &L'=\{w10^{p(|w|)}\,:\,w\in A\} &
\end{align*}
Then, the function $f(w):=w10^{p(|w|)}$ is a reduction from $A$ to $A'\in NTIME(n)$. Since $A$ is NP-complete, so is $A'$, whence $NSPACE(n)$ has NP-complete problems.
\end{proof}


\begin{prop}\label{prop:dext_no_complete}
$PSPACE \neq DEXT$
\end{prop}
\begin{proof}
We will prove this by showing that DEXT is not closed for $m$-reducibility. Since PSPACE has a complete problem, the statement follows immediately.

Suppose that DEXT is closed for $m$-reducibility and let $A\in EXPTIME$ be decided in time $2^{p(n)}$. Consider the padded version of $A$,
\begin{align*}
    &A'=\{w10^{p(|w|)}\,:\,w\in A\}&
\end{align*}
Since the function $f(w):=w10^{p(|w|)}$ is computable in polynomial time, $A\leq_m A'$. Then, since DEXT is closed under $m$-reducibility, we have that that $A\in DEXT$.

This would imply that $EXPTIME\subseteq DEXT$, which contradicts the time hierarchy theorem (see Section \ref{sec:multi_tape} from the Appendix).
\end{proof}



\section{Classes of functions}\label{sec:classes_functions}


Adding the prefix F to the name of a complexity class, we have the class of functions that can be computed by a Turing machine under the same bound. These functions will be the focus of the next chapters, since they have nice inductive definitions, in the style of the Partial Recursive functions from Section \ref{sec:partial_functions}.
In this section we show that their study is closely related to the study of classes of sets, using the prefix techniques introduced in Allender's Doctoral dissertation \cite{allender:85}. 

\subsection{Complexity relations with respect to functions}

\begin{prop}
FPSPACE = FP if and only if PSPACE = P.
\end{prop}
\begin{proof}\,\\
``$\Rightarrow$''\\
If FPSPACE = FP, then, given a set $A$ in PSPACE, since its characteristic function is in FPSPACE, we can obtain a characteristic function in FP, whence $A$ is in P. 

``$\Leftarrow$''\\
If P = PSPACE, the proof is trickier, and uses a technique called ``prefix searching''. First, for $f$ in FPSPACE, let
\begin{align*}
    &prefix(f) := \{(x,y)\,:\,\text{ there is $z$ such that }\,f(x)=yz\}&
\end{align*}
Then, since we can search for $z$ in an amount of space bounded by $|f(x)|$, $prefix(f)$ is in PSPACE, whence it is in P. We now give the algorithm that computes $f$:

\begin{center}
\begin{algorithm}[H]
\,\\
\textbf{Input:} $x$\;
$y:=\vare$\;
\While{TRUE}{
    If $(x,yi)\in prefix(f)$, then $y:=yi$, for $i\in\{0,1\}$\;
    Else break\;
}
\Return $y$;
\,\\
\caption{Computing $f(x)$ by ``prefix searching''}
\label{alg:f_prefix_searching}
\end{algorithm}
\end{center}
\end{proof}


One may wonder why, in the previous proposition, assuming that P = PSPACE does not immediately imply that FP = FPSPACE, by converting the PSPACE machine into a P machine. This is because, when we assume that P = PSPACE, we are assuming an equality between classes of sets. Converting a PSPACE machine into a P machine means only that the new machine accepts the same set, not that it writes the same symbols on the output tape. 

The P and NP version of the previous result is proven similarly.

\begin{prop}
FNP = FP if and only if NP = P.
\end{prop}
\begin{proof}
Suppose that FNP = FP and let $A\in NP$. Then the characteristic function of $A$ is in FNP = FP, whence $A \in P$. From here, we get that P = NP.

Now, if $P=NP$, let $f\in FNP$ and consider the set of prefixes of $f$
\begin{align*}
    &prefix(f) := \{(x,y)\,:\,\text{ there is $z$ such that }\,f(x)=yz\}&
\end{align*}
A nondeterministic Turing machine can decide $prefix(f)$ by ``guessing'' the suffix $z$, whose size is bounded by $f(x)$, and check that $f(x)=yz$. Thus, by hypothesis, $prefix(f)\in P$.
Using Algorithm \ref{alg:f_prefix_searching}, we get that $f\in FP$.
\end{proof}


\subsection{NP and One-way functions}


Before moving on, we present other ways of relating the P vs NP question with classes of functions.


\begin{defn}[Honest function]
A function $f$ is honest if, for every value $y$ in its range, there is some $x$ such that $f(x) = y$ and $|x|\leq p(|y|)$, for some fixed polynomial $p$.
\end{defn}


\begin{prop}
If $f$ has an inverse computable in polynomial time, then $f$ is honest
\end{prop}
\begin{proof}
If $f^{-1}$ is in $FP$, a machine can compute it in a polynomial number of steps, whence, given $y$, the $x$ that satisfies $f(x)=y$ and is obtained by $f^{-1}$ must have a size bounded polynomially in $y$.
\end{proof}


\begin{prop}\label{prop:inverting_honest}
If $f\in FP$ is honest, its inverse is in $FNP$. 
\end{prop}
\begin{proof}
Let $p$ be the polynomial from the definition of honest function. We describe below the non-deterministic machine that implements $f^{-1}$:
\begin{center}
\begin{algorithm}[H]
\,\\
\textbf{Input:} $y$\;
Guess $x$ with $|x|\leq p(|y|)$\;
Compute $f(x)$\;
If $f(x) = y$ output $x$ and accept $y$
\,\\
\caption{Inverting an honest function nondeterministically}
\label{alg:inverting_f_nd}
\end{algorithm}
\end{center}
\end{proof}


\begin{defn}[One-way function]
An honest function $f\in FP$ is called one-way if its inverse is not computable in polynomial time.
\end{defn}

\begin{prop}
P=NP iff and only if there are no one-way functions.
\end{prop}
\begin{proof}
If P=NP, let $f$ be an honest function. Then, by Proposition \ref{prop:inverting_honest}, its inverse can be computed by a non-deterministic Turing machine in polynomial time. Since P=NP, $f^{-1}\in FP$, whence $f$ is not a one-way function.

For the converse, let $M$ be a non-deterministic Turing machine, clocked in polynomial time $p(n)$. Let
\begin{align*}
    &comp(M) = \{(x,y)\,:\,x\,\text{ encodes an accepting computation of $M$ with input $y$}\}&
\end{align*}
and define the function 
\begin{align*}
    &f(x,y)=
    \begin{cases}
    y & (x,y)\in comp(M)\\
    \uparrow & c.c.
    \end{cases}&
\end{align*}

Since $comp(M)\in P$, we have that $f\in FP$. Furthermore, for $y$ in the range of $f$, there is a pair $(x,y)$ such that $f(x,y)=y$ and $|(x,y)|\leq \Oo(p(|y|) + |y|)$. Thus, $f\in FP$ is honest, whence, by hypothesis, its inverse is computable in polynomial time.

Now, since $f^{-1}\in FP$, the range of $f$ is in $P$. But $y$ is in the range of $f$ if and only if there is an encoding $x$ of an accepting computation of $M$ on input $y$, which is the same as saying that $M$ accepts $y$.

Thus, the set accepted by $M$ is the range of $f$, which we showed to be in $P$. This concludes the proof that P=NP.
\end{proof}



\section{Probabilistic computation}


\subsection{Introduction}


Computability by probabilistic Turing machines was first studied by De Leeuw, Moore, Shannon and Shapiro \cite{leeuw:56}, who prove that, under some conditions, access to random inputs does not increase the power of a Turing machine, if it is required to never make a mistake. 
The probabilistic Turing machine model we use nowadays was first defined and studied by Gill \cite{gill:72,gill:74,gill:77} and allows for a procedure to compute with a small, but nonzero, error probability.

The interest in studying such algorithms is that randomness can imply a tremendous decrease in the running time, although with a cost: the answer may be incorrect.

Two seminal papers on randomized algorithms for testing primality are due to Rabin \cite{rabin:80}, who expands on a idea by Miller \cite{miller:76}, and Solovay and Strassen \cite{solovay:77} (see Section \ref{sec:algorithms}).

We will introduce the concept of a probabilistic Turing machine and present four probabilistic complexity classes, which always impose a polynomial time restriction on the Turing machines that defines them.

A probabilistic Turing machine $M$ is a type of non-deterministic Turing machine, where each nondeterministic step, called a \textit{coin-flip step}, has two possible next moves. We will make the assumption that all the computations of a probabilistic Turing machine have the same depth.
Let $k$ be the number of coin-flip steps that occur on branch $b$, i.e., the depth of branch $b$. Then, the probability of $b$ is defined as $P(b) = 2^{-k}$.
The probability of a probabilistic Turing machine $M$ accepting a word $w$ is defined as the sum of the probability of each accepting branch of $M$.
The probability of $M$ rejecting $w$ is defined as $P(M\text{ rejects }w) = 1 - P(M\text{ accepts }w)$.

A word $w$ is considered accepted by a probabilistic machine $M$ if and only if more than half the computations of $M$ on $w$ end in the accepting final state. Since we suppose that all computations have the same length, we can state the acceptance criteria as: ``the ratio of accepting path to the total number of computations is greater than $1/2$''.
The error probability of a probabilistic Turing machine is the probability of it wrongly accepting or rejecting a word.


\subsection{Main probabilistic classes}\label{sec:probabilistic_classes}


We will now define four probabilistic complexity classes, according to the probabilistic Turing machines that can decide them. 
These Turing machines are of the ``Monte Carlo'' type, if they are allowed to make a mistake, and of the ``Las Vegas'' type, if the may terminate with the symbol ``?'', but are not allowed to make a mistake. For this type of algorithms, we add to the definition of a Turing machine the final state ``don't know''. The error probability of these machines is calculated considering that the result ``don't know'' is not an error. We may refer to these machines as 3-output probabilistic Turing machines. The first three classes are of the Monte Carlo type and the last of the Las Vegas type.

Besides the class $RP$ (or $R$), which is due to Miller \cite{miller:75}, and which is studied in Adleman and Manders \cite{adleman:77}, all the probabilistic complexity classes below were introduced in Gill \cite{gill:77}. The class $RP$ appears there as $VPP$.
% PP is also defined by Janos Simon \cite{simon:75}

\begin{defn}
The class $PP$, for Polynomial Probabilistic time, is the class of languages accepted by polynomially clocked probabilistic Turing machines.
\end{defn}

Thus, a set $A$ is in $PP$ if there is a polynomially clocked probabilistic Turing machine $M$ such that, for every $x$, $x\in A$ iff more than half of the computations of $M$ end in an accepting state. As Gill noted PP is closed under complement, since we can just switch the accepting and rejecting states. Beigelm Reingold and Spielman \cite{beigel:91} later showed that PP was closed under union and intersection and Russo \cite{russo:85} that PP is closed under symmetric differences.


\begin{prop}
PP is closed under symmetric differences.
\end{prop}
\begin{proof}
Let $A, B \in PP$ be decided by $M_1$ and $M_2$, respectively. We specify a PP algorithm to decide $A\Delta B$.

\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $x$\;
simulate $M_1$ and $M_2$ on $x$\;
\eIf{Exactly one of the machines accepts $x$}{
    Accept $x$\;
    }{
    Reject $x$
}
\caption{Deciding a symmetric difference}
\label{alg:pp_symmetric_dif}
\end{algorithm}
\end{center}

If both $M_1$ and $M_2$ produce correct results, then $M$ produces a correct result. Moreover, if both Ml and M2 produce incorrect results, then $M$ produces also a correct result. Thus, $M$ produces an incorrect result if and 
only if exactly one of $M_1$ and $M_2$ produces an incorrect result. Hence, if $1/2+\vare$ computations of $M_1$ are correct and $1/2+\vare'$ computations of $M_2$ are correct, then the number of correct computations of $M$ is given by
\begin{align*}
    &(1/2+\vare)(1/2+\vare') + (1/2-\vare)(1/2-\vare') = 1/2 + 2\vare\vare'&
\end{align*}
which is more than $1/2$. 
\end{proof}


\begin{prop}
PP is closed under $m$-reducibility.
\end{prop}
\begin{proof}
Let $A\in PP$ be decided by a machine $M$ in time $p(n)$, and assume that $B\leq_m A$ via $f$, which is computed by a machine $M'$ in time $q(n)$. Consider the following algorithm

\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $x$\;
Compute $f(x)$, branching into two identical computations at each step\;
Simulate $M$ on the result $f(x)$, branching with $M$\;
\caption{PP algorithm to decide a symmetric difference}
\label{alg:pp_m_reducibility}
\end{algorithm}
\end{center}

The total running time of the algorithm is $p(q(n)) + q(n)$, whence $B\in PP$. 
\end{proof}

The problem with the class $PP$ is that we have no guarantee that the answered produced is correct. Moreover, repeating a computation a polynomial number of times does not necessarily guarantee a better reliability, since an algorithm may have an error probability which approaches $1/2$, as the size of the input increases. If this is the case, we may require an exponential number of trials to achieve a fixed error probability.

We will thus consider classes for which the error probability, or, in the last case, the probability of having the result ``don't know'', is limited by a constant smaller than $1/2$.


\begin{defn}
The class $RP$, for Randomized Polynomial time (also denoted by $R$), is the class of languages accepted by polynomially clocked probabilistic Turing machines, which have zero error probability, for inputs not in the language, and error probability bounded above by some positive constant $\vare < 1/2$, for inputs in the language.
\end{defn}

$RP$-algorithms are known as yes-biased Monte Carlo algorithms, because a ``yes'' answer is always correct (if the input did not belong to the language, the algorithm would have rejected it), but a ``no'' answer might be incorrect. The sets in $RP$ are also called randomly decidable.

\begin{defn}
The class $BPP$, for bounded-error probabilistic polynomial time, is the class of languages recognized by polynomially clocked probabilistic Turing machines, whose error probability is bounded above by some positive constant $\vare < 1/2$.
\end{defn}

BPP is often identified as the class of feasible problems for a computer with access to a genuine random-number source.


\begin{defn}
The class $ZPP$, for Zero-error Probabilistic Polynomial time, consists of the languages recognized by polynomially clocked $3$-output probabilistic Turing machines, with zero error probability, such that the probability of the outcome being ``don't know'' is bounded above by some positive constant $\vare < 1/2$.
\end{defn}

This means that a ``yes'' or ``no'' answer will always be correct, but, with a given probability, the machine may terminate in the state ``don't know''. Adleman and Huang \cite{adleman:87} prove that PRIMES is in ZPP.


Regarding the relationship amongst these classes, we have the following inclusions.

\begin{prop}\label{prop:inclusions_probabilistic}
$P\subseteq ZPP\subseteq RP\subseteq BPP\subseteq PP$.
\end{prop}
\begin{proof}
The first and last inclusions hold by definition and, since $RP$ machines have an error probability bounded by $\vare<1/2$, we also have that $RP\subseteq BPP$.

Now, suppose that a set $L$ is decided by a $ZPP$ machine $M$. We construct a machine $M'$ which, on input $x$, simulates $M$ on $x$, accepting if $M$ accepts and rejecting if $M$ rejects or returns ``don't know''.
If $x\notin L$, then $M$ will either reject $x$ or return don't know, whence $x$ will be rejected by $M'$; if $x\in L$, then the probability of $M$ returning ``don't know'' is bounded $\vare<1/2$, whence the probability that $M'$ wrongly rejects $x$ is bounded by $\vare$ as well. Thus, $L\in RP$.
Note that analogously, we can prove that $ZPP\subseteq co-RP$.
\end{proof}

Although these complexity classes appear stronger than $P$, any strict inclusion is still unknown. 
We conclude with an adaptation of a results result from Gill \cite{gill:77}, that motivates the preference for these ``bounded-error'' classes, over PP.

\begin{prop}\label{prop:amplification_lemma}
A set $A$ is in $BPP$ if and only if, for each polynomial $p$, a polynomial time probabilistic machine can be constructed, which accepts $A$ with an error probability bounded by $(1/2)^{p(|x|)}$.

A set $A$ is in $RP$ if and only if, for each polynomial $p$, a polynomial time probabilistic machine can be constructed which accepts $A$ with an error probability bounded by $(1/2)^{p(|x|)}$, on inputs in $A$, and zero error probability, on inputs not in $A$.

A set $A$ is in $ZPP$ if and only if, for each polynomial $p$, a polynomial time probabilistic machine can be constructed, which has a zero error probability and returns ``don't know'' with a probability bounded by $(1/2)^{p(|x|)}$.
\end{prop}
\begin{proof}
Given a probabilistic Turing machine $M$, we construct another machine $M'$ that simulates $M$ a polynomial number of times.
For the case of $BPP$, $M'$ returns the most often outcome from $M$; for the case of $RP$, $M'$ returns ``yes'', if there is a run from $M$ that returned ``yes'', and ``no'', otherwise; finally, for the case of $ZPP$, $M'$ returns ``don't know'', if every run on $M$ returned ``don't know'', and, otherwise, the ``yes'' or ``no'' answer that occurred (which must be unique, by definition of $ZPP$).
\end{proof}

Therefore, we can, in polynomial time, reduce the error associated with a probabilistic Turing machine to a value so small, that it will be more likely for an undetected hardware failure to occur, than for a program failing due to its inherent error probability.




\subsection{Recycling of random numbers}


In the previous section we saw that, under some conditions, we could reduce the error of a probabilistic algorithm to a value arbitrarily small. 
To do so, however, requires the repetition of a computation, in each of which we have to generate random numbers, otherwise we would just be repeating the same experiment.
If the cost of obtaining random numbers is high, we may want to minimize the number of generated random numbers. This section introduces the technique of recycling random numbers, which shows that we can drastically reduce the randomly generated values.
This was first shown by Impagliazzo and Zuckerman \cite{impagliazzo:89}.


The idea is to generate $r$ random numbers once and then reuse them using a hash function (a function from $\{0,1\}^r$ to $\{0,1\}^s$, with $r>s$). We then only have to generate $r-s$ random numbers to fill in the gap.

Suppose the first random number requires $r$ bits and that choosing a hash function $h$ requires $\Oo(r)$ bits. Then, for $k$ trials, we will need $r + \Oo(r) + (r-s)(k-1)$ random bits. We will see that we can choose $r-s\in \Oo(k)$, so we will only need $\Oo(r + k^2)$ random bits, instead of $\Oo(rk)$.


\begin{defn}
A class $H$ of hash functions, from $\{0,1\}^r$ to $\{0,1\}^s$ ($s<r$) is called almost universal if, for every $x,y\in\{0,1\}^r$, with $x\neq y$, $P(h(x)=h(y)) = 1/2^s+1/2^r$.
\end{defn}

In the above definition, the probability $h$ is taken over $H$.


Let $p\geq 2^r$ be a prime number with length $\Oo(r)$. For each $a,b\in[0,p-1]$, consider the function $h_{a,b,p}(x) = ((ax+b)\text{ mod }p)\text{ mod }2^p$, which returns the $s$ least significant bits of $(ax+b)\text{ mod }p$. Denote by $H$ the set of all function $h_{a,b,p}$, with $p$ fixed. We then have:

\begin{lemm}
$H$ is an almost universal class of hash functions.    
\end{lemm}
\begin{proof}
(TODO)
\end{proof}


\begin{defn}
Let $D$ and $D'$ be probability distributions on a finite set $S$. We say that $D$ and $D'$ are $\vare$-\textit{similar} if, for every $X\subseteq S$, $|D(X)-D'(X)|\leq \vare$, where $D(X) = \sum_x\in X p_D(x)$.
\end{defn}

A distribution $D$ is $\vare$-\textit{almost uniformly distributed} if $D$ and the uniform distribution are $\vare$-similar.

The \textit{collision probability} of a distribution $D$ is the probability that two elements $x$ and $y$, chosen independently according to $D$, are the same.


\begin{lemm}[Leftover Hash Lemma]
Let $X$ be an arbitrary subset of $\{0,1\}^r$ with $|X|\geq2^l$. Let $H$ be an almost universal class of hash functions from $\{0,1\}^r$ to $\{0,1\}^s$ with $r\geq l>s$.
Then, the distribution of $(h,h(x))$ is $1/2^{(l-s)/2}$-almost uniform on the set $H\times\{0,1\}^s$.
\end{lemm}
\begin{proof}
We will prove this in two steps, which we separated in two lemmas below (Lemmas \ref{lemm:hash_step1} and \ref{lemm:hash_step2}).
The result then follows easily.
Note that $|H\times\{0,1\}^s| = |H|2^s$. 
Since the collision probability of $(h,h(x))$ is at most $(1+2/2^{l-s})/(|H|2^s)$ (Lemma \ref{lemm:hash_step1}), we have that $D$ is $1/2^{(l-s)/2}$-almost uniformly distributed in $S$ (Lemma \ref{lemm:hash_step2}).
\end{proof}

\begin{lemm}\label{lemm:hash_step1}
The collision probability of the distribution $(h,h(x))$ is at most $(1+2/2^{l-s})/(|H|2^s)$.    
\end{lemm}
\begin{proof}

The collision probability is the probability that for $h_1, h_2 \in H$, $x_1, x_2 \in X$ all randomly chosen, both
$h_1 = h_2$ and $h_1(x_1) = h_2(x_2)$. 
This is $1/|H|$ times the probability that, for $x_1, x_2 \in X$, $h \in H$ randomly chosen, $h(x_1) = h(x_2)$.

If $x_1 \neq x_2$, this probability is at most $1/2^s - 1/2^r$, by the definition of almost universal.

Since $|X| \geq 2^l$, the probability that $x_1 = x_2$ is at most $1/2^l$. Therefore, the collision probability is bounded by
\begin{align*}
    1/|H| \cdot (1/2^s + 1/2^r + 1/2^l) &\leq 1/(|H|2^s) \cdot (1 + 1/2^{r-s} + 1/2^{l-s})&\\
    &\leq 1/(|H|2^s) \cdot (1 + 1/2^{l-s} + 1/2^{l-s})&\\
    &= (1+2/2^{l-s})/(|H|2^s)&
\end{align*}
(recall that $r>l>s$).
\end{proof}

\begin{lemm}\label{lemm:hash_step2}
Let $D$ be a distribution on a finite set $S$. If the collision probability of $D$ is at most $(1+2\delta^2)/|S|$, then $D$ is $\delta$-almost uniformly distributed in $S$.
\end{lemm}
\begin{proof}
Suppose that $D$ is not $\delta$-almost uniformly distributed. Then, there is a subset $Y$ of $S$ with $D(Y)>|Y|/|S|+\delta$. Let $\beta$ be such that $D(Y) = |Y|/|S|+\beta$.
We will show a contradictory lower bound on the collision probability of $D$, i.e., on $P(d_1=d_2)$, with $d_1,d_2$ randomly chosen according to $D$. 

The probability for $d_1=d_2$, under the condition $d_1,d_2\in Y$, is at least $1/|Y|$ and the probability that $d_1,d_2\in Y$ is the product of the probability that $d_1\in Y$ and $d_2\in Y$. Thus, 
\begin{align*}
    \frac{P(d_1=d_2\,\wedge\,d_1,d_2\in Y)}{P(d_1,d_2\in Y)}\geq \frac{1}{|Y|} \,\,&\Leftrightarrow\,\, P(d_1=d_2\,\wedge\,d_1,d_2\in Y) \geq \frac{P(d_1,d_2\in Y)}{|Y|}&\\
    &\Leftrightarrow\,\, P(d_1=d_2\,\wedge\,d_1,d_2\in Y) \geq \frac{D(Y)^2}{|Y|}&
\end{align*}

Analogously to the previous reasoning, the probability for $d_1=d_2$, under the condition $d_1,d_2\notin Y$, is $1/(|S|-|Y|)$, whence
\begin{align*}
    &P(d_1=d_2\,\wedge\,d_1,d_2\notin Y) \geq \frac{(1-D(Y))^2}{|S|-|Y|}&
\end{align*}
Thus,  
\begin{align*}
    P(d_1=d_2) &\geq \frac{D(Y)^2}{|Y|} + \frac{(1-D(Y))^2}{|S|-|Y|} = \frac{(|Y|/|S|+\beta))^2}{|Y|} + \frac{(1-(|Y|/|S|+\beta))^2}{|S|-|Y|}&\\   
    &=\frac{|Y|^2/|S|^2+2|Y|/|S|\beta+\beta^2}{|Y|} + \frac{1-2(|Y|/|S|+\beta) + |Y|^2/|S|^2+2|Y|/|S|\beta+\beta^2}{|S|-|Y|}&\\
    &=\frac{\beta^2}{|Y|}+\frac{\beta^2}{|S|-|Y|} + \frac{|Y|^2/|S|+2|Y|\beta - |Y|^3/|S|^2-2|Y|^2/|S|^2\beta}{{|Y|(|S|-|Y|)}}&\\
    &\qquad + \frac{|Y|-2|Y|(|Y|/|S|+\beta) + |Y|^3/|S|^2+2|Y|^2/|S|\beta}{|Y|(|S|-|Y|)}&\\
    &=\frac{\beta^2}{|Y|}+\frac{\beta^2}{|S|-|Y|} + \frac{|Y| - |Y|^2/|S|}{|Y|(|S|-|Y|)}=\frac{\beta^2}{|Y|}+\frac{\beta^2}{|S|-|Y|} + \frac{1 - |Y|/|S|}{|S|-|Y|}&\\
    &=\frac{\beta^2}{|Y|} + \frac{\beta^2}{|S|-|Y|} + \frac{1}{|S|}&
\end{align*}
This expression is minimized when $|Y|=|S|/2$, in which case the collision probability is at least
\begin{align*}
    &\frac{2\beta^2}{|S|} + \frac{2\beta^2}{|S|} + \frac{1}{|S|} = \frac{1+4\beta^2}{|S|}>\frac{1+4\delta^2}{|S|}>\frac{1+2\delta^2}{|S|} &
\end{align*}
contradicting our assumption.    
\end{proof}




Suppose $M$ is a BPP algorithm for a language $L$ that requires $r$ random bits to process an $n$-bit input. Suppose the error probability of $M$ is between $1/3$ and $1/4$.

From now on, let $x_1,\dots,x_k$ be the sequence of random numbers generated by the method described in the beginning of the section (each $x_i$ will correspond to a computation of $M$). Note that $x_1\in\{0,1\}^r$ is the only genuinely random number.
We will study the behaviour of the algorithm that uses the $x_i$'s as random bits in $k$ simulations of $M$ and then decides whether $x\in L$ based on a majority vote.

Since we only generated $\Oo(r+k^2)$ random numbers, most of the sequences in $\{0,1\}^{rk}$ have probability $0$ of occurring, so the $x_i$'s aren't uniformly distributed over $\{0,1\}^{rk}$. What we want to show is that the bit sequence $b(x_1)\dots b(x_k)$, where 
\begin{align*}
    &b(x_i) = \begin{cases}
        1 & M \text{ with random number $x_i$ accepts}\\
        0 & M \text{ with random number $x_i$ rejects}
    \end{cases}&
\end{align*}
approximates the distribution that one gets by running $M$ $k$ times using new random numbers in each simulation. Call the ``genuine sequence'' $b_1,\dots,b_k$.

%(TODO) - Quanto é que $\vare$ needs to be.


Suppose $x\in L$, so $2/3\leq P(b_i=1) = p\leq 3/4$. We will see that there is a small $\vare$ such that $b_1,\dots,b_k$ and $b(x_1)\dots b(x_k)$ are $\vare$-similar.

This will be proven by induction but we will factor out the $i=1$ case, since it is very illustrative and shows how the Leftover Hash Lemma can be used.

\begin{lemm}
Consider the distribution $y_1,h,y_{2}$ obtained as follows: $y_1$ is chosen independently at random to be $1$, with probability $p$, and $0$, with probability $1-p$; then, $h$ is chosen uniformly at random from $H$; finally, $y_2$ is chosen uniformly at random from $\{0,1\}^r$.

Then, there is a small number $\vare_1$ such that the distribution of $b(x_1),h,h(x_1)$ is $\vare_i$-similar to $y_1hy_2$.
\end{lemm}
\begin{proof}
Since $x_1$ is chosen uniformly at random and $b(x_1)$ is $1$ (resp. $0$) with probability $p$ (resp. $1-p$) -- recall that $p = P(b_1=1)$ -- we have that $b(x_1)$ and $y_1$ follow the same distribution. Thus, we only have to deal with $h,h(x_1)$ and $h,y_2$. Since $h,y_2$ is chosen uniformly from $H\times\{0,1\}^s$, we just have to show that $h,h(x_1)$ is $\vare_1$-almost uniform in $H\times\{0,1\}^s$, which is where the Leftover Hash Lemma comes from.


Under the condition that $b(x_1) = 1$ (resp. $b(x_1) = 0$), there are $p2^r$ (resp. $(1-p)2^r$) possible choices for $x_1$ (recall that $p = P(b_i=1)$). Since $2/3\leq p\leq 3/4$, we have that both $p$ and $(1-p)$ are at least $1/4$, so, in each case, there are, at least, $2^r/4$ choices for $x_1$.

Now, let $r > l = r-2 > s$. Then, from the Leftover Hash Lemma, $(h,h(x_1))$ is $\vare = 2^{(s-l)/2}$-almost uniform on the set $H\times\{0,1\}^s$.
\end{proof}

For the general induction step, we will use the following lemma.

\begin{lemm}
Let $F$, $G$ and $H$ be random variables such that the distribution of $F$ is $\delta_1$-similar to the distribution of $G$. Let $t$ be a transformation such that the distribution of $t(G)$ is $\delta_2$-similar to $H$. Then, $H$ and $t(F)$ are $(\delta_1+\delta_2)$-similar.
\end{lemm}
\begin{proof}
Since a transformation does not alter the \textit{similarity} of distributions, we have
\begin{align*}
    |t(F)(X)-H(X)|&\leq |t(F)(X)-t(G)(X)|+|t(G)(x)-H(X)| &\\
    &= |F(X)-G(X)|+|t(G)(x)-H(X)|&\\
    &\leq \delta_1+\delta_2&
\end{align*}
   
\end{proof}

The following shows that our recycled random number can approximate the random numbers.

\begin{prop}
Consider the distribution $y_1,\dots,y_{i}hy_{i+1}$ obtained as follows: the first $i$ bits are chosen independently at random to be $1$, with probability $p$, and $0$, with probability $1-p$; then, $h$ is chosen uniformly at random from $H$; this is followed by a string chosen uniformly at random from $\{0,1\}^r$.

For all $i\geq 0$, there are numbers $\vare_i$ such that the distribution of $b(x_1),\dots,b(x_i)h,x_{i+1}$ is $\vare_i$-similar to the above distribution.
\end{prop}
\begin{proof}
If $i=0$, we only have to compare $hy_1$ with $hx_1$. In this case, since both $h$, $y_1$ and $x_i$ are chosen randomly, $\vare_0=0$.

For the induction step, suppose the result is true for $i-1$, so $b(x_1),\dots,b(x_i)h,x_{i}$ is $\vare_{i-1}$-similar to the correct distribution. Now, we put
\begin{align*}
    &F = b(x_1),\dots,b(x_{i-1}),h,x_i&\\
    &G = b_1,\dots,b_{i-1},h,z&\\
    &H = b_1,\dots,b_{i-1},b_1,h,v&
\end{align*}
and define the transformation $t$ by
\begin{align*}
    &t(b_1,\dots,b_{i-1},h,z) = b_1,\dots,b_{i-1},b(z),h,h(z)&
\end{align*}
Then, we get
\begin{align*}
    &t(F) = b(x_1),\dots,b(x_{i-1}),b(x_i),h,h(x_i)&\\
    &t(G) = b_1,\dots,b_{i-1},b(z),h,h(z)&\\
\end{align*}

$F$ and $G$ are $\vare_{i-1}$-similar by induction hypothesis and $t(G)$ and $H$ are $\vare_1$-similar, as in the case $i=1$. Thus, $t(F)$ and $H$ are $\vare_i$-similar, with $\vare_i = \vare_{i-1}+\vare_1$.
\end{proof}

For this proposition with $i=k$, we get that $b(x_1),\dots,b(x_k)$ are $k\vare_i$-similar to the distribution that sets each bit to $1$ or $0$, with probability $p$ and $1-p$.

If we set $(r-s-2)/2 = k$, we get that $s = r-2k-2$, whence we get a bound of $k/2^k\in 2^{-\Omega(k)}$ on the similarity, which is good enough for the desired probability amplification.



\subsection{Connections to non-probabilistic classes}


We present some relationships between probabilistic and non-probabilistic classes. Besides the last one, they can all be found in Gill \cite{gill:77}. We summarize the results in the following diagram. The inclusions between probabilistic classes were proven in Proposition \ref{prop:inclusions_probabilistic}.



\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[node distance=0.6cm]
%Complexity Classes
\node (P0) {$P$};
\node (P1) [right=of P0] {$ZPP$};
\node (P2) [right=of P1] {$RP$};
\node (P3) [above right=of P2] {$BPP$};
\node (P4) [below right =of P2] {$NP$};
\node (P5) [below right =of P3] {$PP$};
\node (P6) [right=of P5] {$PSPACE$};
%Propositions
\node at (3,-0.6) {\ref{prop:rp_np}}; % RP NP
\node at (5.4,-0.6) {\ref{prop:np_pp_pspace}}; % NP PP
\node at (6.4,0.3) {\ref{prop:simulate_probabilistic}}; % PP PSPACE
%Inclusions
\path[->,thick]
    (P0) edge (P1)
    (P1) edge (P2)
    (P2) edge (P3)
    (P2) edge (P4)
    (P3) edge (P5)
    (P4) edge (P5)
    (P5) edge (P6);       
\end{tikzpicture}
\caption{Diagram representing the structural relationship between basic probabilistic complexity classes.}\label{fig:complexity_diagram_probabilistic}
\end{center}
\end{figure}


Thus, separations between probabilistic classes would also yield separations amongst the ``regular'' classes. For example, if $P\neq RP$, then $P\neq NP$ and, if $P\neq BPP$, then $P\neq PSPACE$.



\begin{prop}\label{prop:simulate_probabilistic}
Let $t$ be space constructible. A probabilistic machine $M$ with running time $t$ can be simulated by a deterministic machine in space $\Oo(t)$ and time $\Oo(2^t)$
\end{prop}
\begin{proof} We present the algorithm below. In each iteration of the for loop, we can erase the previous computation and only need to keep the counter, which is, at most, $2^t$, which can be written in binary with $t$ cells. From this consideration and Proposition \ref{prop:dijkstra}, the statement follows. 

\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $x$\;
Initialize a counter to $0$\;
\For{$w\in \{0,1\}^*$ of length $t(|x|)$}{
    Simulate step by step a computation of $M$ on $x$, reading a bit of $w$ at each step, by walking through the computation tree of $M$, going left if the bit of $w$ is $0$, and right if it is $1$\; 
    If the computation accepts, increment the counter by $1$ 
}
Accept iff the counter is greater than $2^{t(|x|)-1}$ 
\label{alg:determinisic_probabilistic}
\caption{Deterministic simulation of a probabilistic machine}
\end{algorithm}
\end{center}
\end{proof}

This implies, for example, that $PP \subseteq PSPACE$.


\begin{prop}\label{prop:np_pp_pspace}
$NP\subseteq PP$.
\end{prop}
\begin{proof}
Let $M$ be an NP machine deciding a set $L$. Consider the machine $M'$ which, given an input $x$, chooses with probability $1/2$ to accept $x$ or to simulate $M$ on $x$ (each nondeterministic step is transformed into a $1/2$ probabilistic step).
If $x\notin L$, then only half of the computations of $M'$ will accept $x$, whence $M'$ does not accept $L$; otherwise, ate least one of the computations of $M$ accepts $x$, whence more than half of the computations accept $x$. 
From here, $M'$ decides $L$.
\end{proof}


Note that, since $PP$ is closed under complement (we can just switch the accepting and rejecting states), $co-NP\subseteq co-PP = PP$, whence $co-NP\cup NP \subseteq PP$. 



\begin{prop}\label{prop:rp_np}
$RP\subseteq NP$.
\end{prop}
\begin{proof}
Let $L\in RP$ and $M$ be the $RP$ machine that decides it, and consider the machine $M'$ that chooses nondeterministically the step to take when simulating $M$.

For inputs in $L$, the probability of accepting is more than $1/2$, whence some computation accepts; for inputs not in $L$, the probability of accepting is zero, and thus no computation accepts. From here, $M'$ is an NP machine that decides $L$.
\end{proof}


\section{Turing reducibility and relativization}\label{sec:reducibility_relativization}


\subsection{Turing reducibility}


In this section we will define another notion of reducibility, which will be used to create hierarchies of complexity classes, achieve separation results and prove the inadequacy of some separation methods.


To talk about Turing reducibility, we have to introduce a generalization of the Turing machine. The Oracle Turing machines is a Turing machine with a special state, the \textit{query} state, in which it can test, in one transition, if a word belongs to a given set, the \textit{oracle}. We denote by $L(M,B)$ the language of the machine $M$, when supplied with the oracle $B$.

Interestingly, these machines first appear in Turing \cite{turing:39} and are used, only briefly, to construct a problem which is not number-theoretic. Turing writes

\begin{quote}
Let us suppose that we are supplied with some unspecified means of solving number-theoretic problems; a kind of oracle as it were. We shall not go any further into the nature of this oracle apart from saying that it cannot be a machine.
\end{quote}

Five years later, Post \cite{post:44} uses these machines to define reducibilities between problems, developing the concept of degree of unsolvability. The name ``Turing reducibility'' was given by him. We will be considering a restricted form of Turing reducibility, called polynomial time Turing reducibility. 


\begin{defn}\label{def:turing_reducibility}
Given two sets $A$ and $B$, we say that $A$ is polynomial time Turing reducible to $B$ if there is a deterministic polynomial time oracle machine $M$ such that $A = L(M,B)$.
\end{defn}

If the above conditions are met, we say that $A\leq_T B$ \textit{via} $M$. Intuitively, if $A\leq_T B$ meas that we can decide $A$ efficiently, if we have access to a subroutine that decides $B$. The difference to the $m$-reducibility, is that this subroutine is assumed to return an answer in one machine step.


We can extend the previous definition in two directions. First, we may consider a complexity class $\Cc$ and denote by $\Cc(A)$ the class of sets that are decidable in the bounds given by $\Cc$, when the oracle $A$ is provided. We may also can consider, instead of a single oracle $A$, a class of oracles. In this sense, for example, $P(NP)$ is the class of sets that are reducible in deterministic polynomial time to some set in $NP$. This idea will come back in Section \ref{sec:poly_hierarchy} and is the basis for the study of relativization.


As for $\leq_m$, one can verify $\leq_T$ defines equivalence relation. The notion of completeness is defined as in Definition \ref{def:complete}, replacing $m$-reducibility with Turing reducibility.


\subsection{Contradictory relativizations}

%chang:89 mostra uma prova diagonal para um resultado que não relativiza


Turing-reducibility introduces an interesting area in complexity: relativization. 
If a result can be extended to hold relative to any oracle set, we say that such result relativizes. Then, by showing that a result between two complexity classes does not relativize, we get that it cannot be proven by relativizable proof techniques. This type of \textit{contradictory relativization} is a sort of independence result, in that it proves that it proves the independence of the problem from relativizable techniques. 

The result below, proven Baker, Gill and Solovay \cite{baker:75}, shows that the question of whether P equals NP does not relativize, which is the motivation to seek new unrelativizable proof techniques. For example, the algebraic technique used by Shamir \cite{shamir:92}, to prove that PSPACE equals IP, is unrelativizable.  


\begin{prop}\label{prop:oracle_p_np}
There is an oracle $A$, such that $P(A) = NP(A)$, and an oracle $B$, such that $P(B)\neq NP(B)$.
\end{prop}
\begin{proof}
For the first statement, let $A$ be a PSPACE-complete set (such as QBF -- Proposition \ref{prop:qbf_complete}). Then,
\begin{align*}
    &NP(A)\subseteq PSPACE(A) = PSPACE = P(A)&
\end{align*}

For the second one, let $\{P_i(\,\cdot\,)\}_{i\geq 1}$ be an enumeration of deterministic polynomial-time-bounded oracle Turing machines, such that for every oracle $B$, $P(B) = \{P_i(B)\}_{i\geq1}$. For each $i \geq 1$, $p_i$ is an upper-bound on the running time of $P_i$.

Define
\begin{align*}
    &L(B) = \{x\,:\,\exists y\,,\,|y|=|x|\,\wedge\,y\in B\}&
\end{align*}
For any set $B$, $L(B) \in NP(B)$, since we can just guess $y$. We will define $B$ such that, for every, $i \geq 1$, $L(B)\neq L(P_i(B))$, which will yield the result.

We define $B$ in stages. At each stage $i > 0$, $B$ is already defined for all words of length less than some length $n_i$, and we call this set $B(n_i)$. At stage $0$, we set $n_0 = 0$ and $B(0) = \emptyset$.

We do the following at stage $i$: select the smallest positive integer $n$ such that $n_i \leq n$ and $p_i(n) < 2^n$ and simulate $P_i(B(n_i))$ on input $0^n$. 
This computation can query at most $p_i(n)$ strings, and the length of every query is bounded $p_i(n)$, so there must be some string of length $n$ that this computation does not query. Let $z$ be the lexicographically smallest such string and set $n_{i+1} = 2^n$. 

If $P_i(B(n_i))$ rejects $0^n$, then we define $B(n_{i+1}) = B(n_i)\cup\{z\}$. Thus, we have that $P(B(n_{i+1}))$ rejects $0^n$, since the word $z$ is not queried. Since $B$ will be an extension of $B(n_{i+1})$, $P_i(B)$ rejects $0^n$ as well. Since $|z| = |0^n|$ and $z\in B$, $0^n \in L(B)$.

If $P_i(B(n_i))$ accepts $0^n$, we define $B(n_{i+1}) = B(n_i)$. Then, $P_i(B(n_{i+1}))$ accepts $0^n$ as well and, since $B$ will be an extension of $B(n_{i+1})$, $P_i(B)$ accepts $0^n$. However, $0^n\notin L(B)$, since there is no word of length $n$ in $B$.

Thus, $L(B) \neq L(P_i(B))$ for any $i$, which means that $L(B) \notin P(B)$.
\end{proof}

Note that the previous result is also applicable to $P$ and $PSPACE$. Considering the same oracles: 
\begin{align*}
    &PSPACE(A) = PSPACE = P(A)&\\
    &P(B)\subsetneq NP(B) \subseteq PSPACE(B)&
\end{align*}


Similar results, applied to the second level of the Polynomial hierarchy, can be found in Baker and Selman \cite{baker:77} and in Angluin \cite{angluin:80}.


\subsubsection{Independence results}


A contradictory relativization result forms a sort of independence result from relativizable proof techniques. It is thus natural to ask if we can prove that unrelativizable results are completely independent of a formal theory. We present Du and Ko's \cite{du:11} argument for something similar.

\begin{prop}
For any formal axiomatizable sound theory $F$, for which Turing machines form a submodel, there is a Turing machine $M$, such that $L(M)=\emptyset$ and neither $P(L(M))=NP(L(M))$ nor $P(L(M))\neq NP(L(M))$ are provable in $F$.
\end{prop}
\begin{proof}
Let $A$ and $B$ be two recursive sets such that $P(A) = NP(A)$ and $P(B)\neq NP(B)$.
Define a Turing machine $M$ such that $M$ accepts $(j, x)$ if and only if, among the first $x$ proofs in $F$, there is a proof for the statement ``$P(L(M_j)) = NP(L(M_j))$'' and $x\in B$, or ``$P(L(M_j)) \neq NP(L(M_j))$'' and $x\in A$.

By the recursion theorem, there exists an index $e$ such that $M_e(x) = M(e,x)$, so $M$ accepts $x$ if and only if $M$ accepts $(e, x)$. 
Suppose there is a proof in $F$ for $P(L(M_e)) = NP(L(M_e))$. Then, for almost all $x$, $M_e$ accepts $x$ if and only if $x\in B$, whence $L(M_e)$ is only a finite difference of $B$, whence $P(L(M_e)) \neq NP(L(M_e))$, which contradicts the soundness of $F$. Analogously, if there is a proof in $F$ for $P(L(M_e)) \neq NP(L(M_e))$, then $L(M_e)$ is a finite difference of $A$.

Thus, we conclude that neither statement is provable in $F$. Furthermore, since neither statement is provable in $F$, $M_e$ cannot accept any input, so $L(M_e) = \emptyset$.
\end{proof}

Note that, just because $L(M_e)=\emptyset$, this does not mean that $P=NP$ or $P\neq NP$ is not provable in $F$, since the fact that $L(M_e)=\emptyset$ may not be itself provable in $F$. Whether the statement $P = NP$ is independent of any specific formal proof system is yet still unknown





TODO - Ver Aaronson \cite{aaronson:06}



\subsection{With probability 1 \texorpdfstring{$P\neq NP$}{P neq NP}}


Since Baker Gill and Solovay \cite{baker:75} showed that there were oracles that collapsed and oracles that separated P from NP, many such \textit{contradictory relativizations} were found for different classes (see Hartmanis, Chang, Ranjan and Rohatgi \cite{hartmanis:90b} for an overview).

At the time, Baker Gill and Solovay stated that contradictory relativizations were evidence that a result could not be proven by current techniques. For example, if one could diagonalize over all P machines with an NP machine $M$, then, endowing $M$ with any oracle $A$ one would separate $P(A)$ from $NP(A)$. Since the relationship between P and NP does not relativize, we should not be able to separate these classes by diagonalization. This is actually not correct, since there are examples of separation results with a contradictory relativization (see Chang \cite{chang:89}), but a contradictory relativization is still seen as evidence that a problem is \textit{hard} to solve.

A few years later, Bennet and Gill \cite{bennett:81} showed that, when we consider a random oracle, the probability that it separates P from NP is $1$. On the other hand, they show that, with probability 1, P equals BPP.
This sort of result has been shown for many different complexity classes and gave rise to the Random Oracle Hypothesis: if the probability that an oracle separates two classes is $1$, then they are different.

This hypothesis has been shown to be false (see Kurtz \cite{kurtz:83}), which leaves the question of what can we deduce from these results, if anything at all.
By Kolmogorov's zero-one law, we know that the probability that a random oracle separates a class is either $0$ or $1$, but, as far as I know, there is no known consequence to this \textit{needle} pointing in either direction.



First, we say that a language $A$ is chosen at random if we create it by flipping a fair coin, i.e., if, for any $x\in\{0,1\}^*$, $P(x\in A)=1/2$.

For a language $A$, define $L(A)$ as follows: given $x$ of size $n$, consider the first $n2^n$ words that are greater than $x$, in lexicographic order, divided in $2^n$ blocks of size $n$ each; $x\in L(A)$, if and only if one of these blocks contains only words in $A$.

%Let $M_i$ be an enumeration of all oracle $P$ machines. Then, $P(A) = \{M_i(A)\,:\,i\geq 1\}$. 
We will see that, with probability $1$, if we create a random language $A$, then $L(A)$ separates $P(A)$ from $NP(A)$.

\begin{lemm}
For any $A$, $L(A)\in NP(A)$.
\end{lemm}
\begin{proof}
Consider the following algorithm:
\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $x$\;
Nondeterministically choose a block $k$, with $1\leq k \leq2^n$\;
For each word $y$ in the $k$th block, test if $y\in A$, calling the oracle $A$\;
Accept $x$ if and only if every word in $k$th block is in $A$
\caption{Deciding $L(A)$ with an NP(A) machine}
\end{algorithm}
\end{center}

The algorithm decides $L(A)$ in nondeterministic polynomial time, if an oracle for $A$ is provided, so $L(A)\in NP(A)$.
\end{proof}

Note that, while in a single branch of the computational tree of the algorithm, only $n$ queries are made, there is, overall, an exponential number of \textit{potential} queries in the tree.
This is what allows one to separate $P$ from $NP$ with an oracle. If we bound the number of queries that can be made in the overall tree, the relationship between $P$ and $NP$ does relativize (see Book, Long and Selman \cite{book:84}). 


Note that, intuitively, even if $x\in L(A)$, a deterministic machine cannot guess the that witnesses this belonging, so it must make about $n2^n$ queries, instead of just $n$.

Let $M_i$ be an enumeration of all oracle $P$ machines.
Since $P(A) = NP(A)$ implies that $L(A)\in P(A)$, we can make the following bound:
\begin{align*}
    P(P(A)=NP(A))&\leq P(L(A)\in P(A)) = P(\exists i\,:\,L(M_i(A)) = L(A))&\\
    &\leq \sum_i P(L(M_i(A))=L(A))&\\
    & = \sum_i P(\forall x\,,\, x\in L(M_i(A))\,\square\,L(A))&
\end{align*}

Where $A\,\square\, B = \{x\,:\,x\in A\iff x\in B\}$.


%For every machine $M_i$, 
Let $x_1<x_2<\dots$ be a sequence of strings.
%such that $|x_{j+1}|>2^{|x_j|}$.
% and denote by $n_j$ the size of $x_j$. 
%In this way, we ensure that the events $x_j\in L(A)$ are independent of the next $x_j$'s (the blocks of each $x_j$ are all disjoint) and, for a sufficiently large $j$, the machine $M_i$ on input $x_j$ cannot query strings of size $|x_{j+1}|$.
We continue the approximation above.
\begin{align*}
    P(P(A)=NP(A))&\leq \sum_i P(\forall x\,,\, x\in L(M_i(A))\,\square\,L(A))&\\
    &\leq \sum_i P(\forall j\,,\, x_j\in L(M_i(A))\,\square\,L(A))&
\end{align*}
The inner probability can be unfolded with a conditional, so we get that $P(P(A)=NP(A))$ is bounded by
\begin{align}
    & \sum_i \prod_j P(x_j\in L(M_i(A))\,\square\,L(A))\,|\,\forall k<j\,,\,x_k\in L(M_i(A))\,\square\,L(A))&\label{eq:bound_probability}
\end{align}
%. Since the belonging of $x_{j+1}$ to $M_i(A)$ can be influenced by the belonging of $x_{j}$, this intersection can be split, but a conditional for the first expressions must remain. We thus have that the above expression is equal to


We want to prove an upper bound on the probability that $x_j\in L(M_i(A))\,\square\,L(A)$, under the condition that $\forall k<j\,,\,x_k\in L(M_i(A))\,\square\,L(A)$, which we will refer to as condition $C$.

We will use the following denotations:
\begin{itemize}
    \item $(1)$ is the probability that $x_j\in L(M_i(A))$ and $x_j\in L(A)$% assuming $C$
    \item $(2)$ is the probability that $x_j\notin L(M_i(A))$ and $x_j\in L(A)$% assuming $C$
    \item $(3)$ is the probability that $x_j\in L(M_i(A))$ and $x_j\notin L(A)$% assuming $C$
    \item $(4)$ is the probability that $x_j\notin L(M_i(A))$ and $x_j\notin L(A)$% assuming $C$
\end{itemize}

The condition $C$ is omitted but assumed. Note that the belonging of $x_j$ to $L(A)$ is independent of $C$, since $L(A)$ is defined with the words that come after $x_j$. The upper bound we want to prove is thus on $(1)+(4)$. We will show that, after some order, $(1)+(4)\leq0.92$. This will imply that the product in Equation \ref{eq:bound_probability} will be zero.


For a random $x$ of size $m$, the probability that a block contains some words not in $A$ is $1-(1/2)^m$, so the probability that $x\in L(A)$ is $1-(1-(1/2)^m)^{2^m}$, which converges to $1-1/e = 0.632\dots$ as $m$ tend to infinity. Thus, for a great enough $j$,
\begin{align*}
    &P(x_j\in L(A)) = (1) + (2) \simeq 0.632 > 0.6&\\ 
    &P(x_j\notin L(A)) = (3) + (4) \simeq 0.367 > 0.3&
\end{align*}

To bound $(1)+(4)$, we will give a lower bound on the probability that a ``mistake'' is made, i.e., we will show that $(2)+(3)$, is greater than $0$. We start by approximating the probability 
\begin{align*}
    \frac{(2)}{(2)+(4)} &= \frac{P(x_j\in L(A)\,\wedge\,x_j\notin L(M_i(A)))}{P(x_j\notin L(M_i(A))))}&\\
    &=P(x_j\in L(A)\,|\,x_j\notin L(M_i(A)))&
\end{align*}
(again, we may omit $C$ since the event $x_j\in L(A)$ is independent of $C$) 

Since, in the course of its computation on input $x_j$, with size $m$, the machine $M_i$ can only query $p_i(m)$ many strings of the oracle, for $p_i$ some polynomial, its queries can lie in, at most, $p_i(m)$ different blocks. Thus, there are $2^m-p_i(m)$ blocks where no queried words are present.

For a large $m$, $p_i(m)\leq 2^m/2$, so we can approximate as follows:
\begin{align*}
    P(x_j\in L(A)\,|\,x_j\notin L(M_i(A)))&\geq 1 - (1-1/2^m)^{2^m-p_i(m)}&\\
    &\geq 1 - (1-1/2^m)^{2^m/2}&\\
    &\to 1 - \sqrt{1/e} > 0.3&
\end{align*}
The first bound is due to the fact that one of the blocks we disregarded could have contained only words in $A$, so the probability that $x_j\in L(A)$ would be greater.

We now consider two cases: 
if $(3)\geq 0.1$, then the value of $(2)+(3)$ is at least $0.1$;
$(3)<0.1$, then, since $(3)+(4)>0.3$, we have that $(4)>0.2$. Thus,
\begin{align*}
    &\frac{(2)}{(2)+(4)} > 0.3\iff (2) > \frac{0.3}{0.7}(4) > \frac{0.6}{0.7}>0.08&
\end{align*}
So, in either case, we have that $(2)+(3)>0.08$, whence $(1)+(4)<0.92$. Thus 
\begin{align*}
    &P(P(A) = NP(A))\leq \sum_i\prod_j 0.92 = 0&
\end{align*}
and we have proven that $P(P(A) \neq NP(A))=1$.




\subsection{Bounded oracle queries}


One possible view of the above unrelativization result, is that some restriction should be made on how a nondeterministic Turing machine can access an oracle. This is because, although, in a single branch, only a polynomial number of queries are made, overall, a nondeterministic Turing machine can query an oracle an exponential number of time, while clocked in polynomial time, something that a deterministic Turing machine cannot. The oracle $B$ that separates P from NP is constructed exactly by exploring the fact that a P machine can only make a polynomial number of queries.

In his doctoral dissertation, Kintala \cite{kintala:77} showed that there is a recursive set $A$, such that
\begin{align*}
    &P(A)\subsetneq P(A)_{n^k} \subsetneq P(A)_{n^{k+1}} \subsetneq NP(A)&
\end{align*}
where $P(A)_{n^k}$ is the class of sets decided by a polynomial time-bounded oracle machine that is allowed to make, at most, $n^k$ nondeterministic moves (thus restricting the uncontrolled access to the oracle).
Similar results for different bounds are obtained in Kintala and Fisher \cite{kintala:77b,kintala:80}.
 
Book and Wrathal \cite{book:81,book:81b} introduce a new type of relativization, in order to relativize the relationship between NP and PSPACE, where bounds are placed, not only on the spacial resources, but also on the amount of times the oracle can be queried in a computation. 

We present one of the results from Book, Long and Selman \cite{book:84}, which contains many positive relativization results, when quantitative restrictions are placed on the oracle access.

Denote by $NP_b(A)$ the class of sets decided by polynomial-time oracle nondeterministic Turing machines, whose number of queries made during a computation is bounded by a polynomial in the length of the input.
Then, we have the following result.

\begin{prop}
$P=NP$ if and only if, for every oracle $A$, $P(A) = NP_b(A)$.
\end{prop}
\begin{proof}
The backward direction holds by considering an empty oracle, so suppose that $P = NP$ and let $L \in NP_b(A)$. Let $M$ be an NP oracle machine such that $L = L(M,A)$ and $p$ a polynomial such that the length of a computation and the number of queries of $M$ on $x$ is bounded by $p(|x|)$.

The idea is that, since P=NP, the ``guessing'' part of $M$ can be simulated polynomially. The ``query'' part is covered by searching through all possibilities, which are polynomially bounded.

Let $C_1$ be a configuration of $M(x)$, $\ov y = \cod{y_1,\dots,y_m}$ be a sequence queries and $\ov b = \cod{b_1,\dots,b_m}$ be a sequence of bits, such that $b_i = \chi_A(y_i)$. We say that $C_2$ is a $(\ov y,\ov b)$-successor of $C_1$ if the two following conditions hold:
\begin{itemize}
    \item $C_2$ is an accepting configuration or a query configuration with a query $z\notin \ov y$ 
    \item $C_2$ is reachable from $C_1$ in $p(|C|)$ steps, with only possible queries $y_1\dots,y_m$
\end{itemize}

Let $f$ be the function, which, given a triple $(C_1,\ov y,\ov b)$, returns the least $(\ov y,\ov b)$-successor of $C_1$, or, if no such sucessor exists, returns the empty word. 
Since an NP machine can guess the $(\ov y,\ov b)$-sucessor of $C_1$, $f\in FNP$, whence, by hypothesis, $f\in FP$ (see Section \ref{sec:classes_functions}).

We describe a recursive algorithm that simulates the computation of $M$ over $x$, by searching the computation tree of $M(x)$. A P machine with oracle $A$ can decide $L$ by accepting $x$ if and only if Search($C_0,\emptyset,\emptyset$) returns True, where $C_0$ is the initial configuration of $M$ with input $x$.


\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $C_1,\ov y,\ov b$\;
\While{True}{
    $C_2 = f(C_1,\ov y,\ov b)$\;
    \If{$C_2 = \vare$}{
        \Return False\;
    }
    \If{$C_2$ is an accepting configuration}{
        \Return True\;
    }
    Let $z$ be the query in $C_2$ and $c$ be the answer of querying $A$ with $z$\;
    Let $C_3$ be the configuration after $C_2$, $\ov y = \ov y\conc z$ and $\ov b = \ov b\conc c$\;
    \If{not Search$(C_2,\ov y,\ov b)$}{
        Continue
    }
}
\caption{Search}
\label{alg:search}
\end{algorithm}
\end{center}

Note that the algorithm halts in one of two situations: when an accepting configuration has been found, or when $f(C,\ov y,\ov b) = \vare$. Since the queries and answers are added in $\ov y$ and $\ov b$, if $x$ is accepted there is indeed a path in $M$ to accept $x$; otherwise $\ov y$ contains all queries of $M$ and there is no accepting computation, so $x$ is correctly rejected.

To see that the algorithm runs in polynomial time, note that the recursive calls generated by the procedure Search$(C_0,\emptyset,\emptyset)$ form a tree $T_x$, where each node has a label $C$, a configuration, and each edge is a pair $(\ov y,\ov b)$, of queries and answers.
Since the computation tree of $M(x)$ contains at most $p(|x|)$ queries, and since each internal node of $T_x$ corresponds to a unique query, there are at most $p(|x|)$ nodes in $T_x$, whence the depth-first-search performed by the algorithm runs in polynomial time. 
\end{proof}


We conclude this section with the Separating Theorem, from Xu, Donner and Book \cite{xu:83}, where bounds on the number of oracle queries and the nondeterministic moves are considered to obtain a general technique for relativized separations. In particular, we recover the existence of an oracle that separates $P$ from $NP$, as a corollary.


\begin{prop}
Let $\M$ be a class of nondeterministic Turing machines and $T$ be a set of functions restricting the number of oracle queries in computations of the machines in $\M$. 
Let $g_1,g_2\dots$ be a set of non-decreasing functions such that 
\begin{itemize}
    \item $\log n \leq g_1(n)$, for all but finitely many $n$
    \item $g_i \in \Oo(g_j)$ for every $i<j$
    \item for every $t\in T$, $log t \in \Oo(g_1)$
    \item for every $i$ and $X$, $Q_{g_i}(X) \in D(M,T,X)_{g_i}$
    \item for every $i$, there is a machine $M \in \M$ such that $M$ operates in nondeterminism $g_i$
\end{itemize}

Then there exists a set $A$ such that, for every $i<j$, 
\begin{align*}
    &D(\M, T, A)_0 \subsetneq D(\M, T, A)_{g_i} \subsetneq D(\M, T, A)_{g_j} \subsetneq D(\M, T, A)_\infty& 
\end{align*}
\end{prop}
\begin{proof}
(TODO)
\end{proof}


To see how we recover the oracle that separates $P$ from $NP$, let $\M$ be the class of nondeterministic polynomial-time-bounded oracle machines, $T = poly$, and $g_1(n) = g(n) = n$. Then, $D(\M, T, A)_0 = P(A)$ and $D(\M, T, A)_\infty = NP(A)$.







\section{The polynomial time hierarchy}\label{sec:poly_hierarchy}

\subsection{Introduction}

By means of a restricted Turing reducibility, Stockmeyer and Meyer \cite{meyer:72} define the polynomial time hierarchy, as a polynomial analogue for the arithmetical hierarchy (Section \ref{sec:arithmetical_hierarchy}). A different construction for the hierarchy, using alternating bounded quantifiers to construct each new level, is stated by Stockmeyer \cite{stockmeyer:73} and proven by Wrathal \cite{wrathall:76}. A characterisation via alternating Turing machines is presented in Chandra Kozen and Stockmeyer\cite{kozen:81}.

The hierarchy can be used for the classification of problems whose solution is not known to require more than polynomial time, but for which no polynomial-time algorithm is known to exist. Even if we cannot prove how hard the problem, we may be able to characterize its hardness \textit{relative} to some problem in the hierarchy.

We will adopt the following notation:
\begin{itemize}
    \item $\Delta_1^p = \Sigma_0^p = \Pi_0^p = P$
    \item $\Sigma_{i+1}^p = NP(\Sigma_i^p)$ for $i \geq 0$
    %\item $\Sigma_{i+1}^p = P(\Sigma_i^p)$ for $i\geq 1$
    \item $\Pi_i^p = co-\Sigma_i^p$ for $i\geq 0$
    \item $\Delta_{i+1}^p = P(\Sigma_i^p)$ for $i\geq 0$
    \item $PH = \cup_{k\geq 0}\Sigma_{k}^p$
\end{itemize} 
We also introduce the function version of the hierarchy, by adding the prefix F to the name of each class.

Many important problem lie between NP and $\Sigma_2^p$ \cite{schaefer:02}. To obtain a ``finer structure'' in this range, two intertwined hierarchies have been defined: the Boolean Hierarchy and the Query Hierarchy. 
Kadin \cite{kadin:88,kadin:88b} proves that if either of these is finite, then so is the polynomial time hierarchy.
In this section we will present some basic properties of the polynomial time hierarchy from Stockmeyer \cite{stockmeyer:77}.

\subsection{Alternating quantifiers}

We start with alternative constructions of the hierarchy. The first proposition states that upper levels of the hierarchy can be obtained with a different oracles and the second is the characterization of PH by means of alternating bounded quantifiers.


\begin{prop}\label{prop:ph_different_oracles}
For every $k$, $\Sigma_{k+1}^p = NP(\Pi_k^p)$ and $\Delta_{k+1}^p = P(\Pi_k^p)$.
\end{prop}
\begin{proof}
We will only cover the first case, the second one is proven analogously.
By definition, $\Sigma_{k+1}^p = NP(\Sigma_{k}^p)$. Let $A \in NP(\Sigma_{k}^p)$ and $M$ be a nondeterministic Turing machine such that $A = L(M,B)$, for $B\in\Sigma_k$.
Then, switching the states of $M$ when it receives a yes/no answer from the oracle, we get that $A = L(M,\overline{B})$ and $\overline B\in\Pi_k$.
\end{proof}


To prove the alternating quantifier characterization, we introduce a lemma that provides a different characterization of NP.

\begin{lemm}\label{lemm:np_verify}
A set $A$ is in NP if and only if there is $B\in P$ such that, for every $x$, 
\begin{align*}
    &x\in A\,\,\iff\,\,\exists^{p(|x|)}y\,\,(x,y)\in B&
\end{align*}
\end{lemm}
\begin{proof}
For the first inclusion, let $A\in NP$ and $M$ be a NP Turing machine that decides $A$. Let $q(|w|)$ be the polynomial that bounds the size of the computations of $M$ on input $x$ and define $B$ as follows:
\begin{align*}
    &B = \{(x,y)\,:\,y\,\,\text{is the code of an accepting computation of $M$ of size bounded by $q(|x|)$}\}&
\end{align*}
Since $q$ is a polynomial and we can simulate $M$ in linear time in $y$, we can decide $B$ in polynomial time simply by simulating $M$ on $x$. Thus, $B\in P$ and the inclusions follows.

For the converse, suppose that $B\in P$, let $M$ be the polynomial time Turing machine that decides $B$ and let $A$ be defined by the condition in the statement. Then, to decide $A$, all a Turing machine has to do is, given an input $x$, guess some $y$ with $|y|\leq p(|x|)$ and simulate $M$ on $(x,y)$. show that $A\in NP$.
\end{proof}


The above characterization yields an interesting aspect of the class NP. It can be seen as the class of problems whose solution can be tested in polynomial time. The P vs NP question can thus be seen as the question of whether there are problems whose solution can be efficiently \textit{tested}, but not \textit{obtained}.


The characterization of PH with bounded alternating quantifiers is essentially a generalization of the above result, for the different layers of the hierarchy. To prove it we will introduce the following notation: given a class $\Cc$, we let $\exists\Cc$ denote the class of sets $A$, for which there is $B\in \Cc$, such that 
\begin{align*}
    &x\in A\,\,\iff\,\,\exists^{p(|x|)} y\,(x,y)\in B&
\end{align*}
The class $\forall\Cc$ is defined analogously.


\begin{prop}\label{prop:wrathal_ph}
For every $k$, $A\in\Sigma_k^p$ iff there is a polynomial $p$ and a $(k + 1)$-ary relation $R \in P$, such that, for all, $x$,
\begin{align*}
    &x\in A\,\,\iff\,\,\exists y_1\,\forall y_2\,\dots\,Q y_k\,\,R(x,y_1,\dots,y_k)&
\end{align*}
where $Q\in\{\forall,\exists\}$ is such that all the quantifiers are polynomially bounded and alternate. The level $\Pi_k^p$ is defined analogously.
\end{prop}
\begin{proof}
We will prove that $\exists\Pi_k^p = \Sigma_{k+1}^p$. The proof can be adapted to prove that $\forall\Sigma_k^p = \Pi_{k+1}^p$, which, by induction on $k$, yields the stated result.

For the right inclusion, let $A\in \exists\Pi_k^p$ and $B\in \Pi_k^p$ and $p$ a polynomial such that $x\in A$ if and only if $\exists^{p(|x|)} y\,(x,y)\in B$. Since $B\in \Pi_k^p$, we consider the nondeterministic Turing machine $M$, which, with the oracle $B$, when given an input $x$, guesses the word $y$ to ask the oracle whether $(x,y)\in B$. Then, $A = L(M,B)$, whence $A\in\Sigma_{k+1}^p$

For the converse, we do the proof by induction. For $k=0$, $\exists \Pi_0^p = \exists P = NP = \Sigma_1^p$ (see Lemma \ref{lemm:np_verify}). Now, suppose that $\exists\Pi_k^p = \Sigma_{k+1}^p$, let $A\in\Sigma_{k+2}^p$ and $B\in\Sigma_{k+1}^p$ and $M$ be such that $A = L(M,B)$. Now the proof follows similarly to the one from Lemma \ref{lemm:np_verify}, except that now the computation of $M$ has to include oracle calls. Let
\begin{align*}
    C &= \{(x,y)\,:\,y\,\,\text{is the code of an accepting computation of $M$ on $x$ of size}&\\
    &\qquad\text{bounded by $q(|x|)$, with queries $\alpha_1,\dots,\alpha_i\in B$ and $\beta_1,\dots,\beta_j\in \overline B$ }\}&
\end{align*}
Then, $x\in A$ if and only if $\exists^{q(|x|)} y\,(x,y)\in C$. Note that $\overline B \in \Pi_{k+1}^p$.

Now, since $B\in\Sigma_{k+1}^p = \exists \Pi_k^p$, there is $D\in \Pi_k^p\subseteq\Pi_{k+1}^p$ and $q'$ a polynomial such that $\alpha \in B$ if and only if $\exists^{q'(|\alpha|)} t\,(\alpha,t)\in D$. Thus, we can replace the condition $\alpha_1,\dots,\alpha_i\in B$, in the definition of $C$, with a query to $D\in\Pi_{k+1}^p$. Thus, $A\in\exists\Pi_{k+1}$.
\end{proof}


\subsection{Properties of PH}

We now show some inclusions and closure properties of the levels of the polynomial hierarchy.



\begin{prop}\label{prop:inclusions_sigma_pi_delta}
For every $k$, $\Sigma_k^p\cup\Pi_k^p\subseteq \Delta_{k+1}^p$ and $\Delta_k^p \subseteq \Sigma_k^p\cap\Pi_k^p$.
\end{prop}
\begin{proof}
We will use Proposition \ref{prop:ph_different_oracles}.
For the first inclusion,
\begin{align*}
    &\Sigma_k \subseteq P(\Sigma_k) = \Delta_{k+1}^p&\\
    &\Pi_k \subseteq P(\Pi_k) = \Delta_{k+1}^p& 
\end{align*}
For the second inclusion, 
\begin{align*}
    &\Delta_k^p = P(\Sigma_{k-1}^p)\subseteq NP(\Sigma_{k-1}^p) = \Sigma_k^p&\\
    &\Delta_k^p = P(\Pi_{k-1}^p)\subseteq co-NP(\Pi_{k-1}^p) = \Pi_k^p&
\end{align*}
\end{proof}


\begin{prop}\label{prop:ph_in_pspace}
$PH \subseteq PSPACE$.
\end{prop}
\begin{proof}
The result is proven by induction. For $k = 0$, $\Sigma_0^p = P \subseteq PSPACE$. For the induction step,
\begin{align*}
    &\Sigma_{k+1}^p = NP(\Sigma_k) \subseteq NP(PSPACE) \subseteq PSPACE(PSPACE) = PSPACE&
\end{align*}
\end{proof}


\begin{prop}
For every $k$, $\Delta_k^p$ is closed under complement and polynomial Turing reducibility.
\end{prop}
\begin{proof}
For the first statement, let $A\in\Delta_{k+1}^p = P(\Sigma_k^p)$. Then, we can decide $\overline A$ by switching the accepting and rejecting states from the polynomial time Turing machine that decides $A$ with an oracle in $\Sigma_k^p$. 

For the second statement, 
\begin{align*}
    &P(\Delta_{k+1}^p) = P(P(\Sigma_k^p)) = P(\Sigma_k^p) = \Delta_{k+1}^p&
\end{align*}
\end{proof}


\begin{prop}\label{prop:sigmak_closure_existencial}
For every $k$, $\Sigma_k^p$ is closed under the operation of polynomial-bounded existential quantification over variables of relations, $A \in \Sigma_k^p$
\end{prop}
\begin{proof}
By Proposition \ref{prop:wrathal_ph}, $\Sigma_k^p$ can be defined by alternating quantifiers, ending in an existential quantifier. Then, a new bounded existential quantifier can be eliminated by compressing two quantifiers in one, which searches for the paring of two variables, which are then unpaired in the relation $R$.
\end{proof}


Finally, we show that if there is some $k\geq 1$ such that $\Sigma_k^p$ is closed under complement, the polynomial hierarchy collapses onto $\Sigma_k^p$.


\begin{prop}\label{prop:complement_poly_collapse}
If there is some $k\geq1$ such that $\Sigma_k^p = \Pi_k^p$, then $PH = \Sigma_k^p$.
\end{prop}
\begin{proof}
Suppose that $\Sigma_k^p = \Pi_k^p$, for some $k \geq 1$.
The proof is by induction on $j$. 
The basis $j = k$ is immediate, so assume $\Sigma_{j-1}^p = \Pi_{j-1}^p = \Sigma_k$ for some $j > k$. We 
show that $\Sigma_j^p\subseteq \Sigma_k^p$, which will imply the statement.
Let $A \in \Sigma_j^p$. By Wrathal's characterization (Proposition \ref{prop:wrathal_ph}), there is a $2$-ary relation $R \in \Pi_{j-1}^p$ and a polynomial $p$ such that, for all words $x$
\begin{align*}
    &x\in A\,\,\iff\,\,\exists^{p(|x|)} y\,\,R(x,y)&
\end{align*}
By induction, we have that $R \in \Sigma_{j-1}^p$. Now, since $\Sigma_k^p$ is closed under the operation of polynomial-bounded existential quantification over variables of relations, $A \in \Sigma_k^p$, which concludes the proof.
\end{proof}


The above result implies that, if we can prove that $P\neq \Sigma_k^p$, for some $k\geq 1$, then it must be the case that $P\neq NP$. Otherwise, $NP$ would be closed under complement and thus $PH$ would collapse onto $NP = P$.



\subsection{Connections to probabilistic classes}



That BPP is contained in PH was first proven by Sipser \cite{sipser:83}, who showed that $BPP\subseteq \Sigma_4^p\cap\Pi_4^p$. This upper bound was improved by Gacs (also in Sipser \cite{sipser:83}) to $\Sigma_2^p\cap\Pi_2^p$. Lautemann \cite{lautemann:83} presents a simpler proof of this result by ``pure counting arguments'', which we show bellow.

Note that, since BPP is closed under complement, it is enough to show that $BPP\subseteq \Sigma_2^p$ to obtain the above result.

\begin{prop}
$BPP\subseteq \Sigma_2^p$
\end{prop}
\begin{proof}
Let $M$ be a BPP machine working in time $p(n)$ deciding a set $L$. By Proposition \ref{prop:amplification_lemma}, we can assume that, for every input $x$, of size $n$, there are more than $2^{p(n)} - 2^{p(n)-n}$ computations of $M$ with the correct result.

To simplify the notation, let $p= p(n)$. Consider the %following $Y := \{v\in\{0,1\}^{p} \,:\, M(v, x) =$ ``yes''$\}$ and 
set $N := \{v\in\{0,1\}^{p} \,:\, M(v, x) =$ ``no''$\}$, where $M(v, x)$ denotes the computation of $M$ with choice $v$.
Setting $k : = \lceil p/n\rceil$, and choosing $m$ big enough such that, for all $n \geq m$, $k < 2^n$, we define a set $K$ as follows: 
\begin{itemize}
    \item If $|x|<m$, then $(x, y, z) \in K\iff x\in L$
    \item If $|x|\geq m$, then, for $y = (y_1,\dots,y_k)\in\{0,1\}^{p\cdot k}$ and $z\in\{0,1\}^{p}$, $(x, y, z) \in K\iff (y_1\oplus z,\dots,y_k\oplus z)\notin N^k$ 
\end{itemize}
In other words,
\begin{align*}
    &K=\{(x,y,z)\,:\,(|x|<m\,,\,x\in L)\,\vee\,(|x|\geq m\,,\,(y_1\oplus z,\dots,y_k\oplus z)\notin N^k)\}&
\end{align*}
Since the initial $m$ $x$'s are irrelevant, and for the remaining the acceptance condition is testing if a particular computation is accepting, $K\in P$.
We will now show that 
\begin{align*}
    &x\in L\iff \exists y\in\{0,1\}^{p\cdot k}\,:\,\forall z\in\{0,1\}^{p}\,\,(x,y,z) \in K&
\end{align*}
which proves that membership to $L$ can be expressed as a $\Sigma_2^p$ predicate.

``$\Rightarrow$''\\
Suppose $x\in L$. Then the fraction of wrongly accepting paths is smaller than $2^{p-n}$, i.e., $|N|< 2^{p-n}$. Define $A:=\{(w_1-z,\dots,w_k-z)\,:\,\overline w\in N^k\,,\,z\in\{0,1\}^p\}$. Since 
\begin{align*}
    &|A| = |N|^k2^p < 2^{kp-kn+p} = 2^{pk + (p-nk)} \leq 2^{pk}&
\end{align*}
(the last inequality uses the fact that $k\geq p/n$), there must be some $y=(y_1,\dots,y_k)\in\{0,1\}^{pk}\setminus A$. Fix this $y$ and let $z\in\{0,1\}^p$.

If $|x|<m$, then, since $x\in L$, $(x,y,z)\in K$; if $|x|\geq m$, then $(y_1\oplus z,\dots,y_k\oplus z)\notin N^k$, since, otherwise, $y$ would be in $N$. Thus, for this $y$, we have that, for every $z$, $(x,y,z)\in K$

``$\Leftarrow$''\\
Suppose $x\notin L$. Then, $|N|>2^p - 2^{p-n}$. Let $y\in \{0,1\}^p$ and define, for $i\in\{1,\dots, k\}$, $A_i=\{z\in\{0,1\}^p\,:\,y_i+z\in N\} = \{w-y_i\,:\,w\in N\}$.

Then, $|A_i|=|N|>2^p-2^{p-n}$ and $|\{0,1\}^p\setminus A_i|<2^{p-n}$.
\begin{align*}
    &\left|\{0,1\}^p\setminus\bigcap_{i=1}^k A_i\right| = \left|\bigcup_{i=1}^k \{0,1\}^p\setminus A_i\right| < \sum_{i=1}^k2^{p-n} = k2^{p-n} < 2^p&
\end{align*}
Thus, the intersection $\cap_{i=1}^k A_i$ cannot be empty, whence there is some $z\in\{0,1\}^p$ that is in every $A_i$, i.e., such that $(y_1+z,\dots,y_k+z)\in N^k$. From here, $(x,y,z)\notin K$.

To recap this last inclusion: we chose an arbitrary $y$ and proved that there was always some $z$ such that $(x,y,z)\notin K$.
\end{proof}


Thus, if we identify BPP with the class of feasible problems, we are bounded above by $\Sigma_2^p\cap \Pi_2^p$. More recently Goldreich and Zuckerman \cite{goldreich:11} placed this upper bound in $ZPP(NP)$.


Another interesting result relating $BPP$ with $NP$ is the following: 

\begin{prop}
If $NP\subseteq BPP$, then $NP=RP$. 
\end{prop}
\begin{proof}
Suppose $NP\subseteq BPP$ and let $M$ be a BPP machine that solves SAT.


\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $(F,\nu)$\;
\eIf{$fv(F)=\emptyset$}{
    Accept iff $\nu(F)=1$\;
}{
    \For{$i = 1,\dots,\# fv(F)$}{
        \If{$M(F[x_i/j])=1,\,j\in\{0,1\}$}{
            \Return $RP\_SAT(F,\nu)$
        }
    }
    Reject
}
\caption{RP\_SAT}
\end{algorithm}
\end{center}

If the base case is reached, then, our algorithm only accepts when a satisfying assignment has been found. Thus, a ``yes'' answer is always correct. 

Now, the sources for error are two: we can reject in the base case, even though there was some accepting assignment, just not the one we produced, or we rejected in the recursive step, because all $M(F[x_i/j])$ returned $0$, when they should have returned $1$. 

If the error is in the base case, it means that one $M(F[x_i/j])$ returns $1$, when it should have returned $0$. Since we can amplify the BPP algorithm, we can assume that this happens, for each $x_i$, with probability bounded by $1/{6n}$, so the probability that it happens for some $x_i$ is bounded by $1/6$.

The error in the recursive step happens with probability $1/{6n}^m$, where $m$ is the current number of free variables of $F$.

These probabilities are thus both bounded by $1/6$, whence the probability that an error occurs is bounded by $1/3$. Thus, SAT is in RP, as we wanted to show.
\end{proof}



\section{Uniform diagonalization}

\subsection{The main theorem}

In this section we present a powerful technique, the Uniform Diagonalization Theorem, proven by Shöning \cite{schoning:82}, which can be used for proving the existence of certain types of diagonal recursive sets and that certain complexity classes are not recursively presentable (Definition \ref{def:recursively_presentable}). This theorem recovers as corollaries the results by Ladner \cite{ladner:75}, Landweber et al \cite{landweber:78} and Selman \cite{selman:79}.


\begin{defn}\label{def:recursively_presentable}
A class $\Cc$ of sets is recursively presentable if there is an effective enumeration $M_1, M_2,\dots$ of deterministic Turing machines which halt on all their inputs, and such that $\Cc = \{L(M_i) \,:\,i\geq 1\}$. 
\end{defn}


\begin{defn}
Let $r :\N \to \N$ be a recursive function such that $r(m) > m$ for all $m$. Define the set $G[r]$ as
\begin{align*}
    &G[r] = \{x\in\Sigma^*\,:\,r^n(0)<|x|<r^{n+1}(0)\,,n\in2\N\}&
\end{align*}
\end{defn}


The main theorem, known as uniform diagonalization, is the following.


\begin{thm}\label{thm:uniform_diagonalization}
Let $A_1$, $A_2$ be recursive sets and $\Cc_1$, $\Cc_2$ be classes of recursive sets with the following properties:
\begin{itemize}
    \item $A_1\notin\Cc_1$
    \item $A_2\notin\Cc_2$
    \item $\Cc_1$ and $\Cc_2$ are recursively presentable and closed under finite variations
\end{itemize}
Then, there exists a recursive set $A$ with the following property:
\begin{itemize}
    \item $A\notin\Cc_1$
    \item $A\notin\Cc_2$
    \item $A\leq_m A_1\oplus A_2$
\end{itemize}
\end{thm}
\begin{proof}
(TODO)
\end{proof}

\subsection{Applications}


The theorem has many applications, some of which we present below. 

The first result is proven in Ladner \cite{ladner:75} and Landweber et al \cite{landweber:78}, but we will prove it as a consequence of the uniform diagonalization theorem.


\begin{prop}\label{prop:NP_complete_minus_P}
If $P\neq NP$, there is a recursive set not in $P$ and not $NP$-complete.
\end{prop}
\begin{proof}
We take $\Cc_1$ and $\Cc_2$ to be $P$ and the $NP$-complete sets, respectively, $A_1 = SAT$ and $A_2 = \emptyset$. All the conditions from Theorem \ref{thm:uniform_diagonalization} are verified, so there is some set $A$, not in $P$ and not $NP$-complete, such that $A\leq_m SAT\oplus\emptyset\simeq SAT$, which implies that $A\in NP$.
\end{proof}


The proposition below is proven in Landweber \cite{landweber:78}.


\begin{prop}\label{NP_minus_P_not_recursively_presentable}
If $P\neq NP$, then $NP\setminus P$ is not recursively presentable.
\end{prop}
\begin{proof}
Suppose that $NP\setminus P$ is recursively presentable and let $\Cc_1 = NP\setminus P$, $\Cc_2 = P$, $A_1 = \emptyset$ and $A_2 = SAT$. Then, there is $A\leq_m SAT\oplus\emptyset\simeq SAT$ such that $A\notin P$ and $A\notin NP\setminus P$. This is a contradiction, whence the statement follows.
\end{proof}


Schöning \cite{schoning:82} presents many other applications of the uniform diagonalization theorem, with complexity classes as co-NP, PSPACE and PH. It is proven, for example, that if PH does not equal PSPACE, there are sets not PSPACE-complete that are not in PH, an analogue of Proposition \ref{prop:NP_complete_minus_P}. 






\section{Counting classes}\label{sec:complexity_fch}

Counting classes were introduced by Valiant \cite{valiant:79}, to explain the apparent difficulty in computing the permanent of a matrix. We start by introducing another generalization of the Turing machine. 

\begin{defn}
A counting Turing machine is a standard nondeterministic Turing machine, with an auxiliary output device that prints in binary notation, on a special tape, the number of accepting computations induced by the input. It has time-complexity $f(n)$ if the longest accepting computation induced by the set of all inputs of size $n$ takes $f(n)$ steps.
\end{defn}

For example, the class \#P (sharp P) is the class of functions that can be computed by counting Turing machines of polynomial time complexity, i.e., $\#P=\{f_M\,:\,M$ is an NP machine$\}$, where $f_M(x)$ gives the number of accepting configurations of $M$ on input $x$.

Analogously to the polynomial time hierarchy, if we endow \#P with an oracle for \#P we get a potentially infinite hierarchy of classes.

Valiant \cite{valiant:74}, Simon \cite{simon:75} and Berman and Hartmanis \cite{berman:77} observe that, for almost all pairs of NP-complete problems, there exist polynomial transformations between them that preserve the number of solutions. These problems are then equivalent, not only in the existence of solutions, but also for counting the solutions. 


A related class is $\oplus P$ (parity P), introduced by Papadimitriou and Zachos \cite{papadimitriou:82a}, which consists of the problems solvable by NP Turing machines with an odd number of accepting configurations. This class is called EP by Goldschlager and Parberry \cite{goldschlager:86}.

\subsection{Connections to Probabilistic classes}

We show some connections between counting classes and probabilistic classes.


\begin{prop}\label{prop:PP_sharpP}
$P(\#P) = P(PP)$.
\end{prop}
\begin{proof}
If a polynomial time Turing machine has access to an oracle for \#P, it can count the number of accepting configurations of a nondeterministic Turing machine, accepting if they amount to more than half of the overall final configurations. Thus, the first inclusion holds.

Now, suppose that $f$ is a function in \#P that counts the number of accepting configurations of an NP Turing machine $M$. We design an probabilistic Turing machine $N$ that accepts $(x,w_m)$ if and only if $f(x)>m$. Thus, by a binary search algorithm, a P machine can compute $f$ using the algorithm below as an oracle.

\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $(x,m)$\;
Generate a random string $y \in\{0,1\}^*$ of length $|x|$\;
Generate a random bit $z\in\{0,1\}$\;
\eIf{$z= 0$}{
    Accept $(x,m)$ iff $y$ is one of the first $2^{|x|} - m - 2^{|x|-1}$ strings of length $|x|$
}{
    Accept $(x,m)$ iff $M$ accepts $x$
}
\caption{Counting with a probabilistic machine}
\label{alg:probability_counting}
\end{algorithm}
\end{center}
Note that, to test if $M$ accepts $x$, we are making use of Proposition \ref{prop:np_pp_pspace}, which states that $NP\subseteq PP$. Moreover, if $M(x)$ has $f(x)$ accepting configuration, the simulated version of $M$ has $f(x) + 2^{|x|-1}$, because we toss a coin in the beginning, to decide whether to accept $x$ straightaway, or to simulate $M$ on $x$ (recall that each nondeterministic step is turned into a coin flip step with probability $1/2$). Thus, for $|x|=n$, the fraction of paths where $N$ accepts $(x,m)$ is given by
\begin{align*}
    &\frac{(2^{n} - m - 2^{n-1}) + (f(x) + 2^{n-1})}{2^{n+1}} = \frac{f(x)-m}{2^{n+1}} + 1/2&
\end{align*}
which is greater than $1/2$ iff $f(x)>m$.
\end{proof}


\subsection{Connections to the Polynomial Time hierarchy}

We show some connections between counting classes and the polynomial time hierarchy.


The first result was proven by Papadimitriou and Zachos \cite{papadimitriou:82a} and, independently, by Valiant (also in Papadimitriou and Zachos \cite{papadimitriou:82a}).

\begin{prop}
$\oplus P(\oplus P) = \oplus P$.
\end{prop}
\begin{proof}
Let $M$ be an $\oplus P$ machine that queries another $\oplus P$ machine $M'$. We construct $N$ that accepts the same set as $M$ but is only an $\oplus P$ machine.

$N$ simulates $M$ until it reaches an oracle state with query $x'$. When it does, $N$ chooses to do one of the following: (a)~simulate $M'$ on $x'$ and, if an accepting configuration is reached, simulate $M$ with oracle response ``yes''; (b)~choose to accept or to simulate simulate $M'$ on $x'$ and, if an accepting configuration is reached, simulate $M$ with oracle response ``no''.

This means that the number of accepting configurations of $M$ with answer ``yes'' is multiplied by $\#acc_{M'}(x')$ and the number of accepting configurations of $M$ with answer ``no'' is multiplies by $\#acc_{M'}(x')+1$.

For each leaf $v$ of $M$, for which there are $n(v)$ oracle queries until $v$ is reached, there will be made $n(v)$ choices in $N$. Every time a choice is made this will yield a number of accepting configurations and the product of all the $n(v)$ choices gives the number of accepting configurations given to $N$ by $v$.

Now, if a leaf in $N$ corresponds to choosing the oracle answer \textit{wrongly}, then one of the following options must occur: either in the product there is a wrong ``yes'', in which case $\#acc_{M'}(x')$ was even, and so the number of leaves given to $N$ by $v$ is multiplied by an even number, or there is a wrong ``no'', in which case $\#acc_{M'}(x')$ was odd but, because we add $1$ in the choice, we still multiply the number of leaves given to $N$ by $v$ is multiplied by an even number.

Thus a leaf $v$ of $M$ contributes an odd number to the total count iff it corresponds to correct answers to all the queries asked along the path. Therefore, the number of accepting configurations of $N$ is odd if and only if the number of accepting configurations of $M$ is odd.
\end{proof}


Thus, if $NP\subseteq \oplus P$, the entire Polynomial hierarchy is contained in $\oplus P$ as well. 
Cai and Hemachandra \cite{cai:89b} show that $\oplus P$ contains many sub-classes of NP, such as FewNP and FewCH.

We prove here the weaker inclusion FewP $\subseteq\oplus P$. The class FewP, from Allender and Rubinstein \cite{allender:88}, is defined as the class of problems solvable by NP machines for which an input $x$ generates, at most, $p(|x|)$ accepting configurations, for $p$ some polynomial.

\begin{prop}
FewP $\subseteq\oplus P$.
\end{prop}
\begin{proof}
Let $q$ be the polynomial that bounds the number of accepting configurations of $N$. Then, $M$ proceeds as follows on input $x$: for every $i=1,\dots,q(|x|)$, non-deterministically generate an $i$-tuple of computation paths of $N$; accept $x$ if and only if all the $i$ computations are accepting.

So, if $N(x)$ has $m$ accepting configurations, the greatest tuple has size $m$ and, for each $i\leq m$, there are $\binom{m}{i}$ different ways of choosing $i$-tuples of configurations of $N$. Thus, $M$ has $\sum_{i=1}^m\binom{m}{i}=2^m-1$ accepting configurations, which is odd if and only if $m>0$.
\end{proof}



We now prove an interesting result from Toda \cite{toda:89} stating that PH can be reduced to $\oplus P$ under ``probabilistic reduction''.
First, we introduce the BP operator, defined by Schöning \cite{schoning:89}. 

\begin{defn}\label{def:operator_BP}
For $\Cc$ an arbitrary class of languages, we denote by $BP\cdot \Cc$ the class of languages $A$ for which there is a a language $B\in\Cc$,  a polynomial $p$ and a constant $\alpha\in(0,1)$ and $\vare>0$ such that, for all strings $x$,
\begin{align*}
    &x\in A\,\,\Rightarrow\,\,P(\cod{x,y}\in B)\geq\alpha+\vare/2&\\
    &x\notin A\,\,\Rightarrow\,\,P(\cod{x,y}\in B)\leq\alpha-\vare/2&
\end{align*}
where the probabilities are taken over all $y$ with $|y|=p(|x|)$.
\end{defn}

By definition, $BP\cdot P = BPP$. Applying BP to other classes we can form ``probabilistic versions'' of these classes.


The result below was stated (in an equivalent way) in Valiant and Vazirani \cite{valiant:85}.

\begin{prop}
$NP\subseteq BP\cdot\oplus P$.
\end{prop}
\begin{proof}
It is enough to show that SAT can be probabilistically reduced to $\oplus SAT$, the parity version of SAT.

We will give a probabilistic algorithm, which, receiving an input formula $F$, produces a formula $F'$ with the property that $F\in SAT\,\,\Rightarrow\,\,P(F'\in\oplus SAT)>1/p(|F|)$ and $F\notin SAT\,\,\Rightarrow\,\,F'\notin\oplus SAT$.

For $S\subseteq\{1,\dots,k\}$, denote by $[S]$ the formula $\bigoplus_{i\in S}x_i$, i.e., the formula that is true whenever the number of $x_i$'s that are $1$ is odd. The probabilistic algorithm that converts $F$ into $F'$ is the following.

\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $F$\;
Guess $k\in\{0,\dots,n-1\}$\;
Guess $S_1,\dots, S_{k+2}\subseteq \{1,\dots,n\}$\;
Return $F' := F\wedge[S_1]\wedge\dots\wedge[S_{k+2}]$
\caption{Probabilistic reduction from SAT to $\oplus$SAT}
\end{algorithm}
\end{center}

% k+2 subsetes is because it comes inhandy

If $F\notin SAT$, then $F'$ is not satisfiable, so it has an even number of accepting configurations, whence $F'\notin\oplus SAT$.

Now, if $F\in SAT$, we will see that the probability that $F'$ has exactly \textit{one} assignment is at least $1/{8n}$, which will conclude the result. Suppose that $F$ has $m$ accepting configurations. 

First, since $k$ is chosen from $\{0,\dots,n-1\}$, the probability that $2^k\leq m \leq2^{k+1}$ is at least $1/n$.
Now, let $k$ be fixed and fix a satisfying assignment for $F$, say, $b$.
Since the $S_i$'s are chosen at random, the probability that $b$ is also a satisfying assignment for $F'$ is $1/2^{k+2}$.

Now, the probability that any other assignment is also satisfying for $F'$ is also $1^{k+2}$, so the probability that $b$ is the only satisfying assignment of $F'$, amongst the $m$ satisfying assignments of $F$, is
\begin{align*}
    &\frac{1}{2^{k+2}}\left(1-\sum_{b'}\frac{1}{2^{k+2}}\right)  = \frac{1}{2^{k+2}}\left(1-\frac{m-1}{2^{k+2}}\right) \geq \frac{1}{2^{k+2}}\left(1-\frac{2^{k+1}}{2^{k+2}}\right) = 1/2^{k+3}&
\end{align*}
Finally, the probability that there is such a an assignment $b$ that is the \textit{only} satisfying assignment for $F'$ is at least
\begin{align*}
    &\sum_b\frac{1}{2^{k+3}} = \frac{m}{2^{k+3}} \geq \frac{2^k}{2^{k+3}} = \frac{1}{8}&
\end{align*}
\end{proof}

One can further show that $\exists\oplus P\subseteq BP\cdot\oplus P$.
Toda \cite{toda:89} showed that it is possible to strengthen this result for the entire polynomial time hierarchy.
We first have to prove some properties of the BP operator.

The first property is to ensure that the operator BP does not ruin what we already had. It is a rather natural property to consider and we will always assume it.

\begin{prop}
If $\Cc$ is class of languages, such that, for any $A \in \Cc$, we the set $\{\cod{x, y}\,:\,x \in A, y \in \Sigma^*\} \in\Cc$, then $\Cc \subseteq BP \cdot \Cc$.
\end{prop}
\begin{proof}
Let $A\in \Cc$ and denote by $A_\Cc$ the set $\{\cod{x, y}\,:\,x \in A, y \in \Sigma^*\}$. By hypothesis, $A_\Cc\in\Cc$. Now, if $x\in A$, then $P(\cod{x,y}\in A_\Cc)=1$ and, if $x\notin A$, $P(\cod{x,y}\in A_\Cc)=1$. Thus, by definition, $A\in BP\cdot\Cc$.
\end{proof}


We now show that, under some conditions, the BP operator is idempotent.

\begin{defn}
A class $\Cc$ is said to have probability amplification if the probabilities from Definition \ref{def:operator_BP} can be taken to be $2^{1-q(|x|)}$ and $2^{-q(|x|)}$, for any polynomial $q$.
\end{defn}

For example, P has probability amplification since, for BPP we can run an algorithm multiple times and return the most frequent outcome.


\begin{prop}
If $\Cc$ has probability amplification, then $BP\cdot BP\cdot \Cc = BP\cdot \Cc$.
\end{prop}
\begin{proof}
From the previous result, we only have to deal with one of the inclusions.
Let $A\in BP\cdot BP\cdot \Cc$. Then, there is $B\in BP\cdot \Cc$ such that
\begin{align*}
    &x\in A\,\,\Rightarrow\,\,P(\cod{x,y}\in B)\geq\alpha+\vare/2&\\
    &x\notin A\,\,\Rightarrow\,\,P(\cod{x,y}\in B)\leq\alpha-\vare/2&
\end{align*}
Since $\Cc$ has probability amplification and $B\in BP\cdot\Cc$, there is $C\in\Cc$ such that
\begin{align*}
    &\cod{x,y}\in B\,\,\Rightarrow\,\,P(\cod{x,y,z}\in C)\geq1-\vare/4&\\
    &\cod{x,y}\notin B\,\,\Rightarrow\,\,P(\cod{x,y,z}\in C)\leq\vare/4&
\end{align*}

First, note that $P(\cod{x,y,z}\in C)\geq P(\cod{x,y,z}\in C\,\wedge\,\cod{x,y}\in B)$. Thus, if $x\in A$, and assuming that $\alpha+\vare/2\in(0,1)$, we have that
\begin{align*}
    P(\cod{x,y,z}\in C)&\geq  P(\cod{x,y,z}\in C\,\wedge\,\cod{x,y}\in B)&\\
    &= P(\cod{x,y}\in B)\cdot P(\cod{x,y,z}\in C\,|\,\cod{x,y}\in B) &\\
    &\geq (\alpha+\vare/2)\cdot(1-\vare/4) = (\alpha+\vare/2) - (\alpha+\vare/2)\cdot\vare/4&\\
    &\geq (\alpha+\vare/2) -\vare/4 = \alpha + \vare/4&
\end{align*}
Secondly, note that $P(\cod{x,y,z}\in C) \leq P(\cod{x,y}\in B) + P(\cod{x,y,z}\in C\,|\,\cod{x,y}\notin B)$ (we are, essentially, ignoring some terms that would be multiplied and would reduce the probability). 
Thus, if $x\notin A$,
\begin{align*}
    P(\cod{x,y,z}\in C) &\leq P(\cod{x,y}\in B) + P(\cod{x,y,z}\in C\,|\,\cod{x,y}\notin B)&\\
    &\leq (\alpha - \vare/2) + \vare/4 = \alpha - \vare/4&
\end{align*}
Putting this together, we get that
\begin{align*}
    x\in A&\Rightarrow P(\cod{x,y,z}\in C) \geq \alpha + \vare/4&\\
    x\notin A&\Rightarrow P(\cod{x,y,z}\in C) \leq \alpha - \vare/4&\\
\end{align*}
whence $A\in BP\cdot C$.
Note how we had to use the fact that $\Cc$ has probability amplification to get those bounds $1-\vare/4$ and $\vare/4$.
\end{proof}

We will now state a result but only prove a particular case, which is the one we need, for when the operator is $\exists$.

\begin{prop}
Let $Op$ be an operator on complexity classes with the following property: if $D$ is a class of languages and $A\in Op\cdot D$, there is a polynomial $p$ and a language $B\in D$ such that $x\in A$ depends only on the initial segment of $B$ up to strings of length $p(|x|)$. Furthermore, let $\Cc$ be a class for which probability amplification is possible. Then, $Op\cdot BP\cdot \Cc = BP\cdot Op\cdot \Cc$.
\end{prop}
\begin{proof}
As we mentioned, we will do this for the operator $\exists$, which is the one for which we will need this \textit{swapping lemma}, in Proposition \ref{prop:ph_in_Bp_sumP}. For simplicity, we will set the bounds from BP to $3/4$ and $1/4$.

Let $A\in \exists BP\cdot\Cc$. Then, there is a ser $B\in BP\cdot\Cc$ such that $x\in A$ if and only if there is a $y$ whose size is bounded by $p(|x|)$ such that $\cod{x,y}\in B$. From the definition of BP, this means that there is a set $C\in\Cc$ such that
\begin{align*}
    &x\in A\,\,\Rightarrow\,\,\exists^{p(|x|)}y\,:\,P(\cod{x,y,z}\in C)\geq 1-2^{-q(|x|)}&\\
    &x\notin A\,\,\Rightarrow\,\,\forall^{p(|x|)}y\,,\,P(\cod{x,y,z}\in C)\leq 2^{-q(|x|)}&
\end{align*}
What we want to show is that
\begin{align*}
    &x\in A\,\,\Rightarrow\,\,P(\exists^{p(|x|)}y\,:\,\cod{x,y,z}\in C)\geq 3/4&\\
    &x\notin A\,\,\Rightarrow\,\,P(\exists^{p(|x|)}y\,:\,\cod{x,y,z}\in C)\leq1/4&
\end{align*}
% probabilities taken over z
If $x \in A$, there exists a $y$ such that the probability that $z$ is chosen such that $\cod{x,y,z}\in C$ is greater than $3/4$ (we can choose $q$ such that $1-2^{-q(|x|)}\geq 3/4$).
We then have that, if we first choose $z$, the same $y$ guarantees that
\begin{align*}
    &x\in A\,\,\Rightarrow\,\,P(\exists^{p(|x|)} y \,:\,\cod{x, y, z}\in C)\geq 3/4&
\end{align*}
where the so the first case follows.

Now, if $x\notin A$, we have to use probability amplification. First, note that there are $2^{p(|x|)}$ possible $y$'s. If we choose $q$ such that $q(|x|)>p(|x|)+2$, we then have that
\begin{align*}
    &P(\exists y\,:\,\cod{x,y,z}\in C) \leq \sum_yP(\cod{x,y,z}\in C) = \sum_y2^{-q(|x|)} = 2^{p(|x|)-q(|x|)}\leq1/4&
\end{align*}
\end{proof}


We can now prove that PH reduces ``probabilistically'' to $\oplus P$.


\begin{prop}\label{prop:ph_in_Bp_sumP}
$PH\subseteq BP\cdot\oplus P$.
\end{prop}
\begin{proof}
We prove this by induction that, for all $k\geq 0$, $\Sigma_k^p\cup\Pi_k^p$ is contained in $BP\cdot\oplus P$.
%and the error probability can be made to be less than $2^{q(n)}$ for any polynomial $q$.

The base case is trivial, so suppose $k\geq 0$. Since $BP\cdot\oplus P$ is closed under complement, it is enough to show that $\Sigma_k^p\subseteq BP\cdot\oplus P$.


Let $L\in\Sigma_{k+1}^p = \exists \Pi_k^p$. Then, $L\in\exists BP\cdot\oplus P$. Since we can swap the BP and the $\exists$ operators, $L\in BP\cdot\exists\oplus P$. Since, $\exists\oplus P\subseteq BP\cdot\oplus P$, we have that $L\in BP\cdot BP\cdot \oplus P$ which can be condensed into a single BP, whence $L\in BP\cdot\oplus P$.
\end{proof}

Toda \cite{toda:89} also showed that $BP\cdot\oplus P$ is contained in $P(\#P)$, thus showing that $P(\#P)$ (and, consequently, $P(PP)$ -- Proposition \ref{prop:PP_sharpP}) contains the entire polynomial time hierarchy.

\begin{prop}
$PH\subseteq P(\#P)$.
\end{prop}
\begin{proof}
Let $A\in BP\cdot\oplus P$ be decided by a $BP\cdot\oplus P$ machine $M$ (recall that this is just an NP machine with a different acceptance criteria).
We will show how to construct an NP machine $N$, such that, from the value of $\#acc_N(x)$, we can determine whether or not $x\in A$ in polynomial time.
By definition, there is $C\in P$ such that
\begin{align*}
    &x\in A\,\,\Rightarrow\,\,P(\#\{y\,:\,|y|\leq p(|x|)\,\wedge\,\cod{x,y,z}\in C\}\text{ is odd})\geq 3/4&\\
    &x\notin A\,\,\Rightarrow\,\,P(\#\{y\,:\,|y|\leq p(|x|)\,\wedge\,\cod{x,y,z}\in C\}\text{ is odd})\leq 1/4&
\end{align*}
This means that $x\in A$ if and only if, for ``most'' strings  $y$, there is an odd number of $z$ such that $\cod{x,y,z}\in C$. We can view the acceptance criteria for $A$ as a tree with two steps. First, we generate the possible $y$'s (say, with a machine $M_1$); then, we generate the possible $z$'s (with a machine $M_2$). 
The acceptance of a word is not determined by the amount of $z$'s that make $\cod{x,y,z}\in C$ be satisfied, but rather how they are organized, with respect to each $y$.

For example, if $00$ and $01$ are accepting paths and $10$ and $11$ rejecting, then $0$ and $1$ will be rejecting, since they both lead to an even number of accepting configurations; if, however, $00$ and $10$ are accepting and $10$ and $11$ rejecting, $0$ and $1$ will be accepting, since they both lead to an odd number of accepting configurations. Thus, in the first scenario, the input would be rejected, but, in the second, accepted, even though the number of accepting configurations is the same for both scenarios.

For an input $x$, let $p = p(|x|)$ be the number of possible $y$'s to be generated and consider the NP machine $N$ that generates $(\#acc_{M_2}(x)^p+1)^p$ accepting configurations (for example, by nondeterministically simulating $M_2$ from $p$ different paths, we raise the number of accepting configurations to $p$). Then, $\#acc_{M_2}(x)$ is odd, then $\#acc_{N}(x)=0$ mod $2^p$; otherwise $\#acc_{N}(x)=1$ mod $2^p$. 
% It's just algebra

Thus, if we consider the machine that generates the possible $y$'s and then runs $N$ (call this machine $N'$), we have that $x\in A$ if and only if $\#acc_{N'}(x) < 2^{p(|x|)}/2$ mod $2^{p(|x|)}$. Taking $f$ as the \#P function that returns the value of $\#acc_{N'}$, the result follows.
\end{proof}





\section{The Boolean Hierarchy}

We define the Boolean Hierarchy as follows:
\begin{align*}
    &NP(1)=NP&\\ 
    &NP(2i) = \{ L_1\cap \overline{L_2}\,:\, L_1 \in NP(2i - 1), L_2 \in NP\}&\\ 
    &NP(2i+1)= \{ L_1\cup L_2\,:\, L_1 \in NP(2i), L_2 \in NP\}&\\
    &BH = \cup_k NP(k)&
\end{align*}

The second level, $NP(2) = D^p$ was defined by Papadimitriou and Yannakakis \cite{papadimitriou:82}. The remaining hierarchy was studied, independently, by Cai and Hemachandra \cite{cai:86}, Köbler \cite{kobler:85} and Wagner and Wechsung \cite{wagner:85}.

Clearly, each level is contained in the next, but it is unknown whether any inclusion is proper. Note that, if any inclusion is strict, then $NP\neq co-NP$, since, otherwise, the hierarchy would collapse onto NP. Many of the properties of PH are also valid for BH. For example, it is easy to see that if $NP(k) = NP(k+1)$, for any $k$, then $BH = NP(k)$.






\subsection{Connections to probabilistic classes}


Since $NP\subseteq PP$ and $PP$ is closed under complement and intersection, $D^p \subseteq PP$, which was first observed by Papadimitriou and Yannakakis \cite{papadimitriou:82}. It follows that the entire boolean hierarchy is contained in PP, which is stated in Balcázar, Díaz and Gabarró \cite{balcazar:88} as an exercise.

% Coisa do Russo está o structural complexity symmetric difference (Proposition 6.3). Também tem prova de que é fechado para m-reducibility



\subsection{Connections to the polynomial time hierarchy}


We present the result by Kadin \cite{kadin:88,kadin:88b}, that PH collapses if BH collapses.

We will only present the case BH = NP(2), because it involves less technicalities.


\begin{prop}
If BH = NP(2), the polynomial time hierarchy collapses.     
\end{prop}
\begin{proof}
Let $L_2 = \{(F_1,F_2)\,:\,F_1\in SAT\,\wedge\,F_2\in\overline{SAT}\}$. Suppose BH = NP(2). Then, $L_2\leq_m^p\overline{L_2}$, % are u sure?
so there is a function $f$ that turns formulas $F_1,F_2$ in formulas $G_1,G_2$ such that $F_1\in SAT$ and $F_2\in\overline{SAT}$ if and only if $G_1\in\overline{SAT}$ or $G_2\in SAT$.

This allows us to establish the following NP algorithm, which decides a subset of $\overline{SAT}$.

\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $F$\;
Guess $F'$ with $|F|=|F'|$ and guess a satisfying assignment for $F'$\;
Compute $(G_1,G_2) = f(F',F)$\;
Accept $F$ if $G_2\in SAT$.    
\caption{Easy algorithm for $\overline{SAT}$}
\end{algorithm}
\end{center}

If $G_2\in SAT$, we have that $F'\in SAT$ and $F\in\overline{SAT}$, by the reduction assumption, so we can recognize that $F\in \overline{SAT}$. The algorithm, however, does not allow us to decide any formula in $\overline{SAT}$, but only those which, together with $F'$, get mapped into a pair $(G_1,G_2)$ with $G_2\in SAT$. In the case where $G_1\in\overline{SAT}$ we still cannot (in principle) decide that $F\in \overline{SAT}$.

We will call this algorithm the \textit{easy} algorithm and the formulas $F$ in the above conditions the \textit{easy} formulas. We call a formula $F$ \textit{hard} if it unsatisfiable and not easy, i.e., if, $F\in\overline{SAT}$ and, for every $F'$ with $|F'|=|F|$, $f(F',F)=(G_1,G_2)$ and $G_2\in \overline{SAT}$. Note how this property is a $\Pi_1^p$ predicate.


We now specify an algorithm, which, given a hard function $\hat F$, is able to decide if formulas $F$ with the same size as $\hat F$ are in $\overline{SAT}$.


\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $F$\;
Compute $(G_1,G_2) = f(F,\hat F) $\;
Accept $F$ if $G_1\in SAT$. 
\caption{Using hard formulas as advice}
\end{algorithm}
\end{center}

The algorithm works because of the following observation. Since $\hat F$ is a hard formula, $\hat F\in \overline{SAT}$ and $G_2\in\overline{SAT}$, so, by definition of the reduction, $F\in SAT$ if and only if $G_1\in\overline{SAT}$, or, equivalently, $F\in \overline{SAT}$ if and only if $G_1\in SAT$.

Note how the algorithm works by using $n$ bits of information (the codification of $\hat F$). We have thus shown that $\overline{SAT}\in NP/lin$ (see Section \ref{sec:non_uniform_complexity}). While this is not enough to show that $\overline{SAT}\in NP$, it is enough to prove the collapsing of PH.


Let $L=\{x\,:\,\exists u\forall v\exists w\,(x,u,v,w)\in A\}$ be a set in $\Sigma_3^p$. We will see that we can decide $L$ with only two queries to $\Sigma_2^p$.

First, let $L' = \{(x,u,v)\,:\,\exists w\,(x,u,v,w)\in A\}$. Then, $L'\in NP$ whence there is a reduction $g$ from $L'$ to SAT. Thus, we can rewrite $L$ as
\begin{align*}
    L &= \{x\,:\,\exists u\forall v\,(x,u,v)\in L'\}
    =\{x\,:\,\exists u\forall v\,g(x,u,v)\in SAT\}&\\
    &=\{x\,:\,\exists u\forall v\,g(x,u,v)\notin \overline{SAT}\}&
\end{align*}

We will now use the algorithms we developed to show how we can replace this condition with $2$ queries to $\Sigma_2^p$. Since the Query and Boolean hierarchy intertwine, we get that $\overline{SAT}$ is in the bollean closure of $\Sigma_2^p$.

First, some predicates that will be used:
\begin{itemize}
    \item Recall that a formula being hard is a $\Pi_1^p$ predicate. Thus, the predicate ``there is a hard formula of size $n$'' is in $\Sigma_2^p$. Call this predicate $B$;
    \item Since the easy algorithm is in NP, its complement is in co-NP. Thus, the predicate ``there is $u$ such that for all $v$ $g(x,u,v)$ is \textit{not} accepted by the easy algorithm'' is in $\Sigma_2^p$. Call this predicate $C$\;
    \item Since the $\hat F$ algorithm is in NP, its complement is in co-NP. Since co-NP is closed under intersection, we can intersect the property of being hard with not being accepted by the algorithm. Thus, the predicate ``there is $\hat F$ and $u$ such that, for every $v$, $\hat F$ is hard and $g(x,u,v)$ is not accepted by the $\hat F$ algorithm'' is in $\Sigma_2^p$. Call this predicate $D$.
\end{itemize}


%there are $u,v$ such that there is a hard formula of size $g(x,u,v)$
Let $m$ be a polynomial in $|x|$ that gives the size of $g(x,u,v)$. Then, the following algorithm decides $\overline{SAT}$.
\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $x$\;
\eIf{$m(|x|)\in B$}{
    Accept $x$ if $x\in C$\;
}{
    Accept $x$ if $x\in D$;
}
Reject $x$
\caption{Deciding $L$ with a constant number of queries to $\Sigma_2^p$}
\end{algorithm}
\end{center}


If there is no hard formula of size $m(|x|)$, the (not) belonging of $g(x,u,v)$ to $\overline{SAT}$ can be made using the easy algorithm; 
if there is, it can be done through the $\hat F$ algorithm.
This concludes the proof.
\end{proof}








\section{Bounded query hierarchies}\label{sec:bounded_queries}

%\cite{schoning:90}
% Truth table reduction of something


Let $P(NP[k])$ denote the set of languages accepted by deterministic polynomial time machines, which make no more than $k$ queries to an oracle in NP. The Query Hierarchy, QH, is given by 
\begin{align*}
    &QH = \cup_k P(NP[k])&
\end{align*}
This hierarchy was first defined by Amir and Gasarch \cite{amir:88}.


Note that QH and BH are intertwined, therefore QH is finite if and only if BH is, so if QH collapses, so does PH. For classes of functions, Krentel \cite{krentel:86} shows that, if $FP(NP[k]) = FP(NP[k+1])$, then P = NP.




\subsection{Connections to counting classes}


The following result was proven by Papadimitriou and Zachos \cite{papadimitriou:82a}.


\begin{prop}
$P(NP[\log])\subseteq\#P$
\end{prop}
\begin{proof}
Suppose that $L$ is a language recognized by a $P(NP)$ machine $M$, in time $p(n)$ and with, at most, most $\log(n)$
oracle queries an $NP$ machine $M'$. We will assume that the machine always makes exactly $\log(n)$ queries to the oracle. We design a \#P algorithm that decides $L$.


The idea is the following: By multiplying the number of leaves in different subtrees of the computation tree of $M$ by large numbers, we can encode the outcomes of all the oracle calls along all possible computation paths of $M$. Notice that $M$, although deterministic, has a polynomial number of computation paths, due to the oracle steps.

Based on $M$ and $x$ the algorithm will construct a nondeterministic Turing machine $N$, programmed like $M$, except in the query configurations. Suppose the $i$th query is $x'$, the ``yes'' configuration is $C_1$ and the ``no'' configuration is $C_0$. Then, $N$ nondeterministically chooses one of the following possibilities:
\begin{itemize}
    \item Simulate $M'$ on $x'$ and stop
    \item Assume the answer is ``yes'' and amplify the subsequent computation by a factor of $k(i)$
    \item Assume the answer is ``no'' and amplify the answer by $k(i)^2$ 
\end{itemize}
where $k(i)$ is recursively defined as
\begin{align*}
    &k(\log(n)) = 2^{p(n)}&\\
    &k(i) = k(i + 1)^4&
\end{align*}
(recall that we assume that exactly $\log(n)$ queries are made).

The full \#P algorithm is the following.

\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $(M,x)$ 
Construct $N$ as described above\; 
Let $n$ be the number of accepting configurations of $N$ on $x$\;
$i = 1$\;
\While{$n>1$}{
    \eIf{$n \mod k(i) = 0$}{
        $n = n/k(i)^2$\;
    }{
        $n = (n\mod k(i)^2)/k(i)$\;
    }
    $i = i+1$\;
}
Accept iff $n = 1$
\caption{Simulating oracle queries by counting}
\label{alg:queries_counting}
\end{algorithm}
\end{center}

We show that the algorithm above accepts $x$ iff $M$ does, by showing, by induction on $i$, that the number $n$ in the beginning of the $i$th iteration of the main loop of the algorithm denotes the number of accepting leaves of the computation subtree of $N$, which corresponds to the computation subtree of $M$ starting at the $i$th query.

For $i=1$ this is immediate. Assume now that it holds after the first $i-1$ iterations and suppose that the $i$th query has answer ``no''. This means that there is no accepting path of $M'$ on the queried $x'$. Since such paths are the only paths in the subtree of $N$ which are not multiplied by a multiple of $k(i)$, and since the number of such paths cannot exceed $k(i)-1$, this is equivalent to saying that the value of $n$ before the $i$th iteration (by induction hypothesis the total number of such paths) is divisible exactly by $k(i)$.

Thus, the \textbf{then} branch of the test is taken and the new value of $n$ is $n/k(i)^2$, which is exactly the number of accepting computations in the subtree of $N$ corresponding to $C_0$. This is because the growth of the $k(i)$'s is such that the total number of leaves in the subtree of $N$ corresponding to $C_0$ (or for that matter to $C_1$) is less than $k(i)$. Similarly, if the answer to the $i$th query is ``yes'', then the \textbf{else} branch is taken, and $n$ becomes the number of accepting leaves in a subtree of $N$ that corresponds to $C_1$. 

Therefore, after the last execution of the loop, the value of $n$ is the number of accepting computations of $M$ after the last oracle step. This number is of course $1$ or $0$, depending on whether $x$ is accepted by $M$ or not.
\end{proof}

A similar technique is used by Hemachandra and Wechsung \cite{hemachandra:88} to show the stronger inclusion $P(NP[\log])\subseteq PP$.

The class $P(NP[\log])$ contains many important optimization problems solvable by binary search method, such as finding maximal clique size in a graph.



\section{Non-uniform complexity}\label{sec:non_uniform_complexity}


\subsection{Introduction}


In the previous sections, we talked about complexity from the perspective of the amount of resources that are consumed during a computation. This means that, from the uniform complexity point of view, finite sets are all in the same class, of sets that can be decided in constant time. 
Non-uniform complexity provides tools for dealing with finiteness, by considering the \textit{size} of programs. 


A non-uniform complexity class is a way of characterising families $\{C_n\}_{n\in\N}$, of finite machines (such as circuirs), where each element $C_n$ decides a restriction of some problem to inputs of size $n$. Non-uniformity arises because, for $n \neq m$, $C_n$ may be unrelated to $C_m$, so there might be no computable way to call upon each $C_n$ for every possible input size.

In $1980$, Karp and Lipton \cite{karp:80} introduced the notion of an \textit{advice} function, which could be used to unify a non-uniform class under a single algorithm, by providing it with enough additional information. The sequence would thus be divided into a uniform part, given by the program, and a non-uniform part, given by the advice function.
We can see the size of the advice as a \textit{measure} of the non-uniformity of a sequence.
In this section we will introduce non-uniform complexity classes, by means of an advice function and Boolean circuits. 


\begin{defn} 
Let $\mathcal{C}$ be a class of languages and $\mathcal{F}$ a class of integer-valued functions. The non-uniform class $\Cc/\F$ is defined as the sets $B$ for which there is $f\in\F$ and $A\in\Cc$ such that $w\in B$ if and only if $\cod{w,f(|w|)}\in A$.
\end{defn}

So if $\Cc$ is a complexity class, the sets $B\in\Cc/\F$ are those for which a function in $\F$ provides enough additional information to decide $B$ within the bounds specified by $\Cc$. Note that we are not saying that the set $B$ is in $\Cc$; instead, we are saying that the decision problem ``is $w$ in $B$'', is equivalent to the decision problem ``is $\cod{w,f(|w|)}$ in $A$'', for some $A\in\Cc$.

The examples we will be interested in are $P/poly$ and $P/\log$, of the sets that can be decided by a Turing machine, clocked in polynomial time, with a polynomial or logarithmic advice function, respectively. Note that, in this case, allowing for the advice function to be polynomial yields a strict increase in computational power, i.e., $P/\log \subsetneq P/poly$. 
If we consider $exp$, the set of advice functions bounded in size by functions in the class $2^{O(n)}$, then $P/exp$ contains all sets.


Finally, it is important to note that these non-uniform complexity classes contain non-decidable sets. For example, $P/poly$ contains the sparse halting set, which is defined as $\{0^{n}\,:\,n\in Halt\}$. However, as shown in \cite{balcazar:88}, there are sets in $EXPSPACE$, and therefore decidable, that are not in $P/Poly$.
It is also interesting to note that, since we can just simulate a computable advice within the algorithm, a computable advice can only result in a computable behaviour, with a possible speed up.



\subsection{Circuit complexity}


%We will consider as model of parallel complexity the uniform circuit complexity, introduced by Borodin \cite{borodin:77}. For other formal models of parallel computation see Cook \cite{cook:81} and Vishkin \cite{vishkin:83}.

%Fischer and Pippenger \cite{fischer:74} have shown that a T(n) time bounded Turing machine can be simulated on $n$ bits by a circuit with $\Oo(T(n)\log T(n))$ gates (see also Schnorr \cite{schnorr:76}). 
%Borodin \cite{borodin:77} shows that nondeterministic S(n) space bounded Turing machines can be simulated by circuits of depth $\Oo(S(n)^2)$, thus relating the power of nondeterminism for space bounded computations to the depth required for the transitive closure problem.

%The depth of a circuit is sometimes referred to as ``time'', but here time implicitly means \textit{parallel time}, since several gates in a circuit can operate in parallel.

%The uniform Boolean circuit family, of circuits with a poly-log depth and a polynomial sizes, was first identified by Pippenger \cite{pippenger:79} and is now commonly called NC, for ``Nick's Class'' (see Cook \cite{cook:79}). We denote by $NC^k$ is the class of decision problems solvable by a uniform family of Boolean circuits, with polynomial size, depth $\Oo(\log^k(n))$ and fan-in $2$. It is known that $NC\subseteq P$, but the inclusion is not known to be proper.

%Wilson \cite{wilson:87} proves that the above inclusion and the relationship between each level of $NC^k$ does not relativize.





To discuss the circuit characterization of non-uniform complexity, we will make a brief introduction to circuit complexity.

\begin{defn}
For $\Delta = \{x_1,\dots,x_n\}$ a set of boolean variables, a computation chain over $\Delta$ is a sequence $g_1,\dots,g_k$, where each $g_j$ is one of the following:
\begin{itemize}
    \item A variable in $\Delta$ or a constant ($0$ or $1$) -- these are called the source elements
    \item $(\neg, g_l)$, $(\wedge,g_l,g_m)$ or $(\vee,g_l,g_m)$, for $1\leq l,m< j$ -- these are called the gates
\end{itemize}
\end{defn}

Associated with a gate $g$ is the function $result(g)$, which implements the logical connective represented by the gate.

\begin{defn}
For $f:\{0,1\}^n\to\{0,1\}^m$ such that $f = (f_1,\dots,f_m)$, where each $f_i$ ranges over $\{0,1\}$, we say that a circuit $C = (g_1,\dots,g_k)$ computes $f$ if, for every $i=1,\dots,m$, there is $j=1,\dots,k$ such that $f_i = result(g_j)$.
\end{defn}


A family of circuits $\{C_n\}_{n\geq1}$ is said to accept a language $L$ over $\{0, 1\}$ if, for each $n$, $C_n$ is an $n$-input circuit whose output is $1$ for precisely the $n$-length strings of $L$ ($0$ for other n-length strings). 
In the first case we say that $C$, accepts the input, in the other it rejects it. We say that a language $L$ has 
$\Oo(f(n))$ circuits if $\{C_n\}_{n\geq1}$ accepts $L$ and there is a constant $k$ such that $s(C_n) \leq kf(n)$, for all but finitely many $n$. 

A language has small circuits if it has $\Oo(n^k)$-size circuits for some fixed $k$. A class of languages has small 
circuits if every member of it does. These definitions are made in Karp and Lipton \cite{karp:80}.

Note that even though a language $L$ over $\{0,1\}$ may have small circuits $\{C_n\}_{n\geq 1}$, the function $n\mapsto C_n$ may not even be recursive. 

For this reason, the circuit-size of a language is referred to as a \textit{nonuniform measure} of complexity (Borodin, 1977; Cook, 1980) as opposed to the time needed to accept the language on a Turing machine which is called a \textit{uniform measure}.

The terminology refers to the fact that in the case of Turing machines, there is one fixed device which uniformly accepts $L \cap \{0, 1\}^n$ for each $n$, and not so in the case of circuits. 

In spite of its nonuniformity, circuit-size is an interesting measure, since one may prefabricate these devices for values of $n$ ranging from $1$ to some practically attained upper bound. Indeed in cryptographic applications, nonuniform complexity is accepted as a measure of how difficult a code is to crack.
%(Goldwaser and Micali, 1982 and Yao, 1982). 


We will discuss two ways of measuring the complexity of a circuit: the circuit depth, the length of its longest path, and the circuit cost, its, the number of its gates. Given a function $f$, we denote by $c(f)$ (resp. $d(f)$) the smallest possible circuit cost (resp. depth) of a circuit that computes $f$.


For $A$ and infinite set, we denote by $c_n(A)$ (resp. $d_n(A)$) the circuit complexity (resp. depth) of $A$ when restricted to its elements of length $n$. We will see how to relate the circuit complexity of a set with its belonging to a non-uniform complexity class.


\subsection{Connections to the Polynomial Time hierarchy}\label{sec:non_uniform_poly_hierarchy}


The first two results are from Karp and Lipton \cite{karp:80}. The first was obtained jointly with Kannan and the second is an improvement by Sipser.


\begin{prop}
If $SAT\in P/\log$, then $P = NP$.
\end{prop}
\begin{proof}
Suppose that $SAT\in P/\log$, meaning that there is some function $f$, whose size is bounded by $c\log$, for some constant $c$, and a set $B\in P$, such that $F\in SAT$ iff $(F,f(|F|))\in B$. Algorithm \ref{alg:SAT_P/log} runs in polynomial time and can decide SAT. 

The idea is that, the size of the advice is bounded by $c\log(|F|)$, there are $2^{c\log(|F|)} = |F|^c$ possible advices, through which we can cycle in polynomial time. Note that getting the wrong advice is not problematic, since we can always test if the assignment indeed satisfies the formula.

\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $F$\;
\For{$w\in \{0,1\}^*$ of length bounded by $c\log(|F|)$}{
    $z:=F$\;
    \While{$fv(F)\neq\emptyset$}{
        $x:=$ first free variable of $F$\;
        \eIf{$(z|_{x:=i},w)\in B$, for $i=0,1$}{
            $z = z|_{x:=i}$\;
        }{
            Exit While\;
        }
    }
    \If{$z= 1$}{
        Accept\;
    }
}
Reject
\label{alg:SAT_P/log}
\caption{Simulating a logarithmic advice}
\end{algorithm}
\end{center}
\end{proof}


We will now prove a similar result for when NP is contained in $P/poly$. We start with three results.


\begin{prop}\label{prop:np_p/poly}
If $NP\subseteq P/poly$, then, $NP/poly = P/poly$.
\end{prop}
\begin{proof}
Suppose that $NP\subseteq P/poly$ and let $A\in NP/poly$. Then, there is $B\in NP$ and $f\in poly$, such that $x\in A$ iff $\cod{x,f(x)}\in B$. Now, since $B\in NP\subseteq P/poly$, there is $C\in P$ and $g\in poly$, such that $x\in A$ iff $\cod{\cod{x,f(x)},g(\cod{x,f(x)})}\in C$. Putting $f$ and $g$ together, we get that $A\in P/poly$.
\end{proof}


\begin{prop}\label{prop:np/poly}
If $A \in NP(B)$ and $B\in P/poly$, then $A \in NP/poly$.
\end{prop}
\begin{proof}
(TODO)
\end{proof}



\begin{prop}\label{prop:PH_P/poly}
If $NP\subseteq P/poly$, then, $PH \subseteq P/poly$.
\end{prop}
\begin{proof}
We prove the result by induction. The base case is implied by hypothesis, so consider $k\geq1$ and let $A\in\Sigma_{k+1}^p = NP(\Sigma_k^p)$. Then, by induction hypothesis, $\Sigma_k^p\subseteq P/poly$, whence, by Propositions \ref{prop:np/poly} and \ref{prop:np_p/poly}, $A\in NP/poly = P/poly$. Thus, $\Sigma_{k+1}^p\subseteq P/poly$, whence the statement follows.
\end{proof}

From the previous statements, if $NP\subseteq P/poly$, the entire polynomial time hierarchy will also be contained in $P/poly$. We now show that this implies a collapsing of PH onto its second level.


\begin{prop}\label{prop:np_ppoly_collapse}
If $NP\subseteq P/poly$, then $PH = \Sigma_2^p$.
\end{prop}
\begin{proof}
If $NP\subseteq P/poly$, then, by Proposition \ref{prop:PH_P/poly}, $PH\subseteq P/poly$. Consider the $\Sigma_k^p$ version of SAT, called $SAT_k$, in which the formulas considered can have $k$ alternating bounded quantifiers (see Section \ref{sec:complete_sat_qbf} from the Appendix). Then, there is $B\in P$ and $f\in poly$ such that $F\in SAT_k$ iff $\cod{F,f(|F|)}\in B$. The idea now will be to create a alternating machine that guesses the advice (using an existential state), tests if it is correct (using a universal state) and then uses the advice to decide $SAT_k$. This will prove that $SAT_k\in \Sigma_2^k$. Let $p$ be the polynomial that bounds the size of $f$.

\begin{center}
\begin{algorithm}[H]
\,\\
\textbf{Input:} $F$\;
Guess (existential state) $w$ with $|w|\leq p(|F|)$\;
Test (universal state) if the following holds:
\begin{itemize}
    \item $\cod{0,w}\notin B$ and $\cod{1,w}\in B$
    \item If $F = \exists x F'$, then $\cod{F,w}\in B$ iff $\cod{F|_{x:=i},w}\in B$, for some $i\in\{0,1\}$\;
    \item If $F = \forall x F'$, then $\cod{F,w}\in B$ iff $\cod{F|_{x:=i},w}\in B$, for $i= 0,1$\;
\end{itemize}
Accept $F$ if and only if $\cod{F,w}\in B$.
\,\\
\caption{Deciding $SAT_k$}
\label{alg:sat_k}
\end{algorithm}
\end{center}
\end{proof}

Mahaney \cite{mahaney:82} shows that, if NP has a sparse oracle, which is further in NP, then PH collapses onto $\Delta_2^p$. Yap \cite{yap:83} shows that, if $\Sigma_i^p/poly = \Pi_i^p/poly$, then $PH = \Sigma_{i+1}^p$.


Since $P\subseteq P/poly$, one way of showing that $P\neq NP$ would be to show that $NP\not\subseteq P/poly$, i.e., to show that there is an NP problem with a super-polynomial circuit complexity. Unfortunately, Razborov and Rudich \cite{razborov:94} prove that any ``natural proof'' cannot be used to establish super-polynomial lower bounds.



The following result if from Kannan \cite{kannan:82}.


\begin{prop}\label{prop:l_k_not_small_circuits}
For each integer $k$, there is a language $L_k$ in $\Sigma_4^p\cap\Pi_4^p$, such that $L_k$ does not have small circuits.
\end{prop}
\begin{proof}
(TODO)
\end{proof}

From Propositions \ref{prop:np_ppoly_collapse} and \ref{prop:l_k_not_small_circuits}, we get the following result.

\begin{prop}\label{prop:l_k_not_small_circuits_part_2}
For any integer $k$, there is a language $L_k$ in $\Sigma_2^p\cap\Pi_2^p$ such that $L_k$ does not have small circuits.
\end{prop}
\begin{proof}
If NP has small circuits, then $PH = \Sigma_2^p$, so the language $L_k \in \Sigma_4^p\cap\Pi_4^p$ is also in $\Sigma_2^p\cap\Pi_2^p$.

If NP does not have small circuits, then there is a language $L\in NP$ which does not have $\Oo(n^k)$ circuits, for any $k$. Since $L$ is in NP, it is also in $\Sigma_2^p\cap\Pi_2^p$, whence the result follows.
\end{proof}

Note that, if the second case holds, then SAT does not have small circuits, whence the language $L_k$ from the above proposition is either SAT or the language $L_k$ from Proposition \ref{prop:l_k_not_small_circuits}. But, of course, we will not know which it is unless we settle some really hard problems. 

Also, note that, if NP has circuits $\Oo(n^k)$, for some fixed $k$, then there is a language in the polynomial time hierarchy which does not, so the hierarchy cannot collapse onto NP, implying that $P\neq NP$. This is interesting when we combine it with the remark after Proposition \ref{prop:np_ppoly_collapse}. If $NP$ has small circuits, for a fixed $k$, then $P\neq NP$; and if $NP$ does not have small circuits, then $P\neq NP$ as well.


The following is due to Meyer.

\begin{prop}
If $EXPTIME \subseteq P/poly$, then $EXPTIME = \Sigma_2^p$. 
\end{prop}
\begin{proof}
(TODO)
\end{proof}


As a consequence, we have that, if $EXPTIME \subseteq P/poly$, then $P\neq NP$, since, otherwise, $PH$ would collapse onto $P$. Together with the above result, we would have that $P = EXPTIME$, contradicting the time hierarchy theorem.



\subsection{Connections to Probabilistic classes}



Adleman \cite{adleman:78} proves that RP has polynomial size circuits. The fact that $BPP$ also does was first stated by Bennett and Gill \cite{bennett:81}. This implies, for example, that, if $NP\subseteq BPP$, then $NP$ also has polynomial size circuits, which, by Proposition \ref{prop:np_ppoly_collapse} implies that $PH=\Sigma_2^p$. Zachos \cite{zachos:88} later improved this result by showing that if $NP\subseteq NP$, then $PH=BPP$. Essentially, he observes that $BPP(BPP) = BPP$, so, if $NP\subseteq BPP$, then $NP(NP)\subseteq BPP(BPP) = BPP$ and so on.

\begin{prop}
$BPP\subseteq P/poly$.
\end{prop}
\begin{proof}
Let $M$ be a BPP machine deciding $A$ and let $p$ be the polynomial bounding the computation time of $M$. For a size $n$, we can this of the BPP computation as yielding a sequence of choices $r\in\{0,1\}^p(n)$, which may lead to the correct output, or not. We will see that the probability that $r$ gives the right output for any $x$ of size $n$ is greater than $0$, whence, for every $n$, such an $r$ must exist.


Since we can reduce the error probability for $A$, suppose it is bounded by $2^{-(n+1)}$. Let $M(r,x)$, denote the computation of $M$ with \textit{choice} $r$. Then,
\begin{align*}
    P(\forall r\,,\,M(r,x)\text{ is correct})&= 1 - P(\exists r\,,\,M(r,x)\text{ is wrong})&\\
    &\geq 1 - \sum_{r}P(M(r,x) \text{ is wrong})&\\
    & \geq 1 - 2^n\cdot2^{-(n+1)} \geq 1/2&
\end{align*}

Thus, the fact that $BPP\subseteq P/poly$ is witnessed by the function which, given a size $n$, returns the sequence of decisions $r$ that produce the correct output for any $x$ of size $n$. 
\end{proof}



The following is a result by Vinochandran \cite{vinodchandran:05}. Aaronson \cite{aaronson:06} shows that it is non-relativizing.

\begin{prop}
For every $k$, $PP$ does not have boolean circuits of size $n^k$.
\end{prop}
\begin{proof}
(TODO)
\end{proof}



\subsection{Lower bounds on monotone classes}

% [n] = \{1,\dots,n\}

This comes from Arora and Barak \cite{arora:09}, but mostly from \url{https://www.styopa.space/pdfs/monotone-circuits.pdf}.

A circuit is called monotone if it contains no NOT gates. Not every Boolean function can be computed by monotone circuits.

\begin{defn}
For $x, y\in \{0, 1\}^n$, we denote $x \precl y$, if every bit that is $1$ in $x$ is also $1$ in $y$.
A function $f\,:\,\{0, 1\}^n \to \{0, 1\}$ is monotone if $f(x) \leq f(y)$, for every $x \precl y$.
\end{defn}

Alternatively, a boolean function $f$ is monotone increasing if, for every input $x$, changing a bit of $x$ from $1$ to $0$ does not decrease the value of $f$.


\begin{prop}
A Boolean function $f$ is computable by a monotone circuit if and only if it is monotone increasing.    
\end{prop}
\begin{proof}``$\Rightarrow$''\\
Since monotone circuits don't have negations, changing an input value of a variable from $0$ to $1$ does not decrease its value, whence functions computed by monotone circuits are monotone increasing.  

``$\Leftarrow$''\\
We prove this by induction on the number of variables of $f$.
If $f$ has no variable, it is a constant function and thus computable by a monotone circuit.
Now, suppose that $f$ has $n$ variables $x_1,\dots,x_n$. As $f(x_1/0)\leq f(x_1/1)$, we can represent $f$ as $(x_1 \wedge f(x_1/1)) \vee f(x_1/0)$, which, by the inductive hypothesis, are computable by monotone circuits, whence so is $f$.
\end{proof}


For example, CLIQUE is a monotone function, since adding an edge to the graph cannot destroy any clique that existed in it. In this section we show that the CLIQUE function cannot be computed by polynomial sized monotone circuits. This was first proven by Razborov \cite{razborov:85} and shows that monotone NP is not contained in monotone P/poly.

Most people also believe that CLIQUE does not have polynomial-size circuits, even allowing NOT gates.
A seemingly plausible approach to proving this might be to show that the monotone circuit complexity of monotone functions is polynomially related to its general circuit complexity, but this was refuted by Razborov \cite{razborov:85b} and, later, by Tardos \cite{tardos:88}.

Denote by $CLIQUE_{k,n}\,:\,\{0, 1\}^{\binom{n}{2}}\to\{0, 1\}$ the function which, given an adjacency matrix of an
$n$-vertex graph $G$, outputs $1$ iff $G$ contains a $k$-vertex clique.
To prove a lower bound on the monotone complexity of $CLIQUE_{k,n}$, we will show two things: first, that we need more than $n^{-\sqrt k/20}$ \textit{clique indicators} (defined below) to decide $CLIQUE_{k,n}$; secondly, that any monotone circuit built to decide $CLIQUE_{k,n}$ can be approximated by less than $n^{-\sqrt k/20}$ \textit{clique indicators}, which we will have shown not to be enough.

Let $S \subseteq [n]$. The \textit{clique indicator} of $S$ is the function $C_S\,:\,\{0, 1\}^{\binom{n}{2}}\to\{0,1\}$ that outputs $1$ on a graph $G$ iff $S$ is a clique in $G$. 
Note that we can represent $C_S$ by $\bigwedge_{i\neq j\in S}x_{ij}$.

$CLIQUE_{k,n}$ can be decided by an OR of clique indicators, i.e., $CLIQUE_{k,n} = \bigvee_{S\subseteq[n],|S|=k} C_S$, but, as we will show, we need at least $n^{\sqrt k/20}$ clique indicators in this OR.

We first have to introduce some concepts. 

\begin{defn}
An $(l,m)$-function is a function that is the OR of at most $m$ clique indicators, each with a size bounded by $l$.
\end{defn}

\begin{defn}
The sets $Z_1,\dots,Z_i$ are in a \textit{sunflower formation} if there is some $Z$ that equals their pairwise intersection.
\end{defn}

Essentially, this will mean that we can replace an AND of sets in a sunflower formation with the set corresponding to their intersection. To do this, we will have to use a Lemma from Erd{\"o}s and Rado \cite{erdos:60}.

\begin{lemm}[Sunflower Lemma]
Let $\cal{Z}$ be a collection of distinct sets each of cardinality at most $l$. If $|\cal{Z}| > (p - 1)^ll!$, then there are $p$ sets $Z_1,\dots, Z_p \in \cal{Z}$ and a set $Z$ (not necessarily in $\cal{Z}$), such that $Z_i \cap Z_j = Z$, for every $1 \leq i< j \leq p$.
\end{lemm}
\begin{proof}
The proof is by induction on $l$. 
The case $l=1$ is trivial -- their intersection is just the empty set.
For $l > 1$, let $\cal{M}$ be a maximal subcollection of $\cal{Z}$ containing only disjoint sets.
If $|\cal{M}|\geq p$, the sets in $\cal{M}$ make up a sunflower with an empty intersection, so suppose that $|\cal{M}|\leq p-1$, whence $|\cup\cal M| \leq l(p - 1)$.

Because of $\cal{M}$'s maximality, we have that, for every $Z \in \cal{Z}$, there exists $x \in \cup \cal M$, such that $x \in Z$. Thus, there is an $x \in \cup\cal M$ that appears in at least a $1/l(p-1)$ fraction of the sets in $\cal Z$.
Let $Z_1,\dots, Z_k$ be the sets containing $x$. Then, from the cardinality of $\cal{Z}$,
\begin{align*}
    &k > \frac{(p - 1)^ll!}{l(p-1)} = (p - 1)^{l-1}(l - 1)!&
\end{align*}
whence, by induction hypothesis, there are $p$ sets among the sets $Z_1\setminus\{x\},\dots, Z_k\setminus\{x\}$ (which all have a size bounded by $l-1$) that form a sunflower.
Adding back $x$, we get the desired sunflower among the original sets.
\end{proof}

This means that, if we have more than $m = (p - 1)^ll!$ elements of size $l$, we can always find $p$ of them to ``condense'' (defined below) until we have at most $m$ elements. This observation is the key aspect for the operations below to work.

\begin{defn}
Let $m = (p - 1)^ll!$, for some $p$ and consider two $(l,m)$-function, $f\vee_{i=1}^mC_{S_i}$ and $g=\vee_{j=1}^mC_{T_j}$. We describe two operations: $f\sqcup g$ (union) and $f\sqcap g$ (intersection). 

\textbf{Union:}
Let $h = C_{Z_1}\cup\dots\cup C_{Z_{2m}} := C_{S_1}\cup\dots\cup C_{S_m}\cup C_{T_1}\cup\dots\cup C_{T_m}$, which is a $(l,2m)$-function.
As long as there are more than $m$ distinct sets in $h$, find $p$ subsets $Z_{i_1},\dots,Z_{i_p}$ that are in a sunflower formation, with intersection at $Z$, and replace the functions $C_{Z_{i_1}},\dots,C_{Z_{i_p}}$ with $C_Z$. Repeat this until we obtain an $(l,m)$-function $h'$ and define $f \sqcup g = h'$.

\textbf{Intersection:}
Let $h = \vee_{1\leq i,j\leq m}C_{S_i\cup T_j}$. First, discard any $C_Z$, if $|Z|>l$; then, reduce the number of functions to $m$ by applying the sunflower lemma as above.    
\end{defn}

\,

We will show that clique indicators are bad in two specific types of $n$-vertex graphs, generated by the following distributions:
\begin{itemize}
    \item $\cal{N}$: choose a function $c\,:\,[n] \to [k - 1]$ (a colouring) at random, and place an edge between $u$ and $v$ iff $c(u) \neq c(v)$.
    \item  $\cal{V}$: choose a set $K\subseteq [n]$ with $|K| = k$ at random and output the graph that has a clique on $K$ and no other edges. 
\end{itemize}

Note that, for $n\geq k -1$, $\cal{N}$ does not outputs a graph with a clique of size $k$, since there are only $k-1$ possible different values for $c$ and two vertices $u$ and $v$ will only be connected if $c(u)\neq c(v)$.

On the other hand, $\cal{V}$ gives the sparsest graphs that still has a clique in $K$.

Intuitively, if we have an $(l,m)$-function $f$ with a ``small'' clique indicator (much smaller than $k$) $C_S$, it is likely that $f$ will wrongly identity cliques in a graph with no cliques, since it will receive a ``yes'' from $C_S$.

We formalize this below.

\begin{lemm}\label{lemm:prob_f_N}
Let $f$ be an $(l,m)$-function. Then, it either rejects all graphs, or the probability that it accepts a graph generated by $\cal{N}$ is, at least, $1 - l^2/(k - 1)$.   
\end{lemm}
\begin{proof}
If $f$ has no clique indicators, it rejects all graphs, so suppose there is a clique indicator $C_S$ in $f$, with $|S|\leq l$. 
We will look at the probability that it rejects a graph and then take the complement.
There are $\binom{|S|}{2}$ pairs of vertices in $S$ and, for each of these, $(k-1)^{n-1}$ colorings that assign them the same element.
Thus, the number of colorings where two vertices in $S$ are given the same color is
\begin{align*}
    &\binom{|S|}{2}\cdot (k-1)^{n-1} \leq l^2(k-1)^{n-1}&
\end{align*}
whence the fraction of wrongly accepted graph is at least
\begin{align*}
    &1 - \frac{l^2(k-1)^{n-1}}{(k-1)^n} = 1 - l^2/(k - 1)&
\end{align*}
Since $f$ is an OR of $l$ sized clique indicators, it wrongly accepts at least as many graphs as $C_S$.
\end{proof}

%Note that, as $l^2$ approaches $k-1$, the smaller the fraction of wrongly accepted graphs. However, we get the reverse problem 


\begin{lemm}\label{lemm:too_many_clique_indicators}
Let $C_S$ be a clique indicator with $|S|\geq l+1$. Then, $C_S$ accepts at most $\binom{n-l-1}{k-l-1}$ graphs generated by $\cal{V}$.  
\end{lemm}
\begin{proof}
Recall that $\cal{V}$ outputs a graph that has a clique on a randomly chosen $K\subseteq [n]$ and no other edges. Thus, $C_S$ accepts $\cal{V}$ iff $S$ is contained in the set $K$ chosen by $\cal{V}$, since a subset of a clique is also a clique.

Thus, if $l+1$ elements of $S$ are already chosen, there are $\binom{n-l-1}{k-l-1}$ ways of choosing a set $K\subseteq[n]$ such that $S\subseteq K$.
\end{proof}

We will now prove that monotone circuits of size $s$ can be approximated by $(l,m)$-functions on instances of $\cal{V}$ and $\cal{N}$. From here, we will derive a lower bound on $s$.

The approximation is constructed as follows:

\begin{defn}
Let $C$ be a monotone circuit of size $s$ and represent $C$ as a sequence of monotone functions $f_1,\dots,f_s$, where each $f_k$ is either an input variable or the OR or AND of functions that come before $f_k$ in the sequence, such that $C$ computes the function $f_s$.

We construct the approximation $\tilde f_S$ of $C$ inductively as follows: 
if $f_k$ is an input variable, we define $\tilde f_k = f_k$; 
if $f_k$ is the OR (resp. AND) of previous functions, we define $\tilde f_k$ as $\sqcup$ (resp. $\sqcap$) of their approximations.
\end{defn}


\begin{lemm}\label{lemm:good_at_yes}
$\tilde f_s$ rejects, at most, $sm^2\binom{n-l-1}{k-l-1}$ graphs from $\cal{V}$.
\end{lemm}
\begin{proof}
We will show that, in each approximation, the number of graphs that $\tilde f_k$ wrongly rejects is, at most, $m^2\binom{n-l-1}{k-l-1}$. Since $\tilde f_s$ is constructed from $s$ approximations, the result follows by taking the union bound.

\textbf{First case:} $f_k$ is approximating an OR gate:\\
Suppose that $Z_1,\dots, Z_p$ are replaced with $Z$, as in the sunflower lemma.
Since $Z$ is contained in each $Z_i$, $C_{Z_i}(G) = 1$ implies that $C_Z(G) =1$, since a subgraph of a clique is also a clique. Thus, the operation $\sqcup$ does not introduce false negatives.

\textbf{Second case:} $f_k$ is approximating an AND gate:\\
By the distributive law $f \cap g = \vee_{i,j} (C_{S_i} \cap C_{T_j})$.
If $\cal{V}$ outputs a graph $G$, it has a clique over some set $K$ and no other edges. Thus, $C_{S_i} \cap C_{T_j}$ holds iff $S_i, T_j \subseteq K$, whence $C_{S_i} \cap C_{T_j}$ holds iff $C_{S_i\cup T_j}$ holds.
Thus, we can only introduce a false negatives when we discard functions of the form $C_Z$, with $|Z|>l$.

Now, from Lemma \ref{lemm:good_at_yes}, the $C_Z$'s that are discarded accept at most $\binom{n-l-1}{k-l-1}$ graphs  from $\cal{V}$ -- recall that $|Z|\geq l+1$, whence, since we discard, at most, $m^2$ such sets (in the case where all the $S_i\cup T_j$ are greater than $l$), the bound follows.
\end{proof}



\begin{lemm}\label{lemm:good_at_no}
$\tilde f_s$ accepts, at most, $sm^2l^{2p}(k - 1)^{n-p}$ graphs from $\cal{N}$.
\end{lemm}
\begin{proof}
We will show that, in each approximation, the probability that $\tilde f_k$ wrongly outputs $1$ is at most $m^2l^{2p}(k - 1)^{-p}$. Since $\cal{N}$ generates $(k-1)^n$ graphs, the bound will follow.

\textbf{First case:} $f_k$ is approximating an OR gate:\\
We can introduce false positives if the randomly chosen $c$ is one to one on $Z$ (denote this event by $B$) but not on any $Z_i$ (denote these events by $A_i$). 

Conditioned on $B$, the events $A_i$ are independent, since they depend on the values of $c$ on disjoint sets (the intersecting parts are in $Z$), so
\begin{align*}
    &P(A_1\wedge\dots\wedge A_p\wedge B) \leq P(A_1\wedge\dots\wedge A_p\,|\, B) = \prod_{i=1}^pP(A_i\,|\,B)\leq \prod_{i=1}^pP(A_i)&
\end{align*}
Now, the probability that there is a collision in $Z_i$ is bounded by $l^2/(k-1)$ -- Lemma \ref{lemm:prob_f_N}, so the above probability is bounded by $l^{2p}/(k-1)^p$.
Since we apply the sunflower reduction at most $m$ times, we get the desired bound.


\textbf{Second case:} $f_k$ is approximating an AND gate:\\
Since $C_{S\cup T}$ implies both $C_S$ and $C_T$, we can't introduce false positives by moving from $f\cap g$ to
$\vee_{i,j} C_{S_i\cup T_j}$ and we can't introduce false positives by discarding functions from the OR.
Thus, the only place where we can introduce false positives is where we replace the clique indicators of a sunflower with the clique indicator of the common intersection, which we already covered in the first case.
\end{proof}

So, given a monotone circuit $C$ of size $s$, deciding $CLIQUE_{n,k}$, we can construct an $(l,m)$-function approximating $C$, with an error given by Lemmas \ref{lemm:good_at_yes} and \ref{lemm:good_at_no}. 
Let's see how this allows us to derive a lower bound on $s$.


\begin{thm}\label{thm:clique_lower_monotone}
There exists an $\vare>0$ such that, for large enough $n$ and $k$ such that $k \leq n^{1/4}$, any monotone circuit computing  $CLIQUE_{n,k}$ has size at least $n^{\vare\sqrt k}$.
\end{thm}
\begin{proof}
Let $C$ be a monotone circuit of size $s$ computing $CLIQUE_{n,k}$ and set $l = \sqrt{k-1}/2$, $p = \sqrt k\log n$ and $m = (p-1)^ll!$ (recall the sunflower lemma). Then, we can construct an $(l,m)$-approximation $f$ of $C$.

First, note that, after some $n$,
\begin{align*}
    m^2&=\left((p-1)^ll!\right)^2 < p^{2l}(l!)^2 \leq (pl)^{2l} = (\sqrt{k-1}/2 \cdot \sqrt k\log n)^{\sqrt{k-1}}&\\
    &\leq (k\log n)^{\sqrt{k}} \leq (n^{1/4}\log n)^{\sqrt{k}} \leq (n^{1/3})^{\sqrt{k}} = n^{\sqrt k/3}&
\end{align*}


From Lemma $\ref{lemm:prob_f_N}$, $f$ either rejects all graphs or wrongly accepts at least $1-l^2(k-1)= 1/2$ of the graphs coming from $\cal{N}$.

In the first case, since $f$ rejects all graphs, it must reject all the graphs coming from $\cal{V}$, which are $\binom{n}{k}$. Thus, from Lemma \ref{lemm:good_at_yes}, we get that
\begin{align*}
    &s \geq \frac{\binom{n}{k}}{m^2\binom{n-l-1}{k-l-1}} = \frac{1}{m^2}\frac{n\cdots(n-l)}{k\cdots (k-l)} \geq \frac{1}{m^2}\left(\frac{n}{k}\right)^{l+1} \geq \frac{(n^{3/4})^{\sqrt{k-1}/2+1}}{n^{\sqrt k/3}} \geq n^{\sqrt k/1000}&
\end{align*}


In the second case, we have, from Lemma \ref{lemm:good_at_no}, that $\tilde f_s$ accepts at most $sm^2l^{2p}(k - 1)^{n-p}$ graphs from $\cal{N}$. Thus,
\begin{align*}
    &s \geq \frac{(k-1)^{p}}{2m^2l^{2p}} \geq \frac{4^{\sqrt k\log n}}{2n^{\sqrt k/3}}\frac{(k-1)^{\sqrt k\log n}}{(k-1)^{\sqrt k\log n}} = \frac{n^{2\sqrt k}}{2n^{\sqrt k/3}} \geq n^{\sqrt k}&
\end{align*}
\end{proof}





\subsection{Natural Proofs}

We present Razborov and Rudich \cite{razborov:94} Natural proofs, following the exposition by Arora and Barak \cite{arora:09}.

Firstly, we give their definition of a ``natural proof''. This is a proof which uses a natural combinatorial property

\begin{defn}
A property $\Phi$ is a map from boolean functions to $\{0,1\}$. A P-natural property useful against P/poly is a property $\Phi$ such that:
\begin{itemize}
    \item $\Phi(f) = 1$ for at least a $1/2^n$ fraction of all boolean functions on $n$ bits (there are $2^{2^n}$ such functions)
    \item $\Phi(f) = 1$ implies that $f\notin P/poly$ (or more concretely, that $f$ has circuit complexity at least $n^{\log n}$, say)
    \item $\Phi$ is computable on $n$-bit functions in $2^{\Oo(n)}$ time (i.e., polynomial in the length of the function's truth table)
\end{itemize}
\end{defn}





\section{Average case complexity}

(TODO)



\section{Interactive proofs and zero knowledge}


In $1986$, Goldwasser, Micali and Rackoff \cite{goldwasser:85} introduced the notion of an interactive proof system. Using these systems, one can define probabilistic generalizations of the class NP. A zero-knowledge protocol is able to provide convincing evidence that a proof of a statement exists, without disclosing any information about the proof itself.


We want to consider proofs as a communication problem between a \textit{prover}, who knows the proof, and a \textit{verifier}, who does not, but is supposed to be convinced of its correctness. 

We will consider that the verifier is a P machine, but that the prover has no complexity restriction. Each machine can read the input (the statement of the theorem) and has a private working tape. The machines can communicate using a tape they can both access.


The communication protocol proceeds in rounds, in each of which only of the machines is active. After a round has ended, at most a polynomial amount of information can been written on the communication tape. The number of rounds is polynomial in the size of the input.

The newly computed information at each round can be considered a function of the input and all previously communicated information.



A language $A$ is represented by an interactive proof system, if there is a BPP verifier $V$ such that, if $x\in A$, there is a prover strategy that the verifier accepts with probability greater than $2/3$ and, if $x\notin A$, then, for any strategy the prover uses, the verifier accepts with a probability smaller than $1/3$. Note that the value $2/3$ and $1/3$ are irrelevant, due to probability amplification.


Since membership to NP can be decided in polynomial time, when a witness is given, $NP\subseteq IP$. Since we took the verifier to be in BPP, we also trivially have that $BPP\subseteq IP$.
One example of a problem in IP that is not known to be in NP is the complement of graph isomorphism

Here's the communication protocol:
\begin{itemize}
    \item \textbf{Verifier:} guess $i\in\{1,2\}$ and a permutation $\pi$ of $\{1,\dots,n\}$, where $n$ is the number of nodes of $G_1$ (and $G_2$). Compute $H = \pi(G_1)$ and send $H$ to the prover
    \item \textbf{Prover:} Send to the verifier the $j\in\{1,2\}$ such that $H$ and $G_j$ are isomorphic
    \item \textbf{Verifier:} accept if $i=j$
\end{itemize}

So if $G_1$ and $G_2$ \textit{are not} isomorphic, the prover can choose the $G_j$ that is isomorphic to $H$ and $j$ will equal $i$, so the verifier will accept.
Now, if $G_1$ and $G_2$ \textit{are} isomorphic, the prover has only a $1/2$ chance of choosing the right $j$. If the verifier sends more than one permutation, the probability that the prover can trick the verifier decreases exponentially.

This means that $\overline{GI}\in IP(2)$ (IP with two rounds).


Goldwasser and Sipser \cite{goldwasser:86} showed that any protocol as above can be replaced by one where the verifier does not need to compute anything privately before communicating with the prover. He can just choose a random number and send it to the prover directly.



\subsection{IP = PSPACE}



Take an IP protocol between a prover $P$ and a verifier $V$. Since the information communicated between them is polynomial, we can, in polynomial space, simulate the whole process, testing every possible answer from the prover and counting the number of accepting configurations of the verifier. If there is strategy where the verifier accepts with probability higher than $2/3$, we accept the input; otherwise, we reject it. Thus $IP\subseteq PSPACE$. We prove below that it is actually equal. 


The strategy is to give an interactive protocol for QBF', consisting of the satisfiable quantified boolean formulas, where negation is only applied to variables, interpreting formulas arithmetically.
Assume that variables take on integer values. Then AND operations will be interpreted as multiplication, OR as addition and negation as $1-x$. 
For a formula of the form $\forall x F$, every free occurrence of $x$ in $F$ is replaced once with a $0$ and once with a $1$, and each resulting formula is arithmetized. Let $a_0$ and $a_1$ be the resulting values. Then the value of $\forall x F$ is $a_0\cdot a_1$. For $\exists x F$ we use $a_0+a_1$ instead.
We denote by $bool(F)$ the boolean value of $F$ and by $arith(F)$ its arithmetical value.

The idea now is that the prover will convince the verifier of the arithmetical value of a formula $F$. It is precisely this more difficult task, which demands more information from the prover, that makes an IP for QBF' possible. 
Note that, if the prover was only demanded to give, for example, an assignment, then he could not convince the verifier that a formulas was \textit{not} satisfiable.

The first technical problem is that, since universal quantification introduces multiplication in the arithmetical value, the size of $arith(F)$ can grow exponentially. We can fix this by computing this value modulo some $k$ of size $2^{\Oo(n)}$. We must, however, make sure that the arithmetical value of a formula is greater than $0$ if and only if the formula is satisfiable.

\begin{lemm}
For every $a$ with $0<a<2{2^n}$, there is a prime number $k\in[2^n,2^{3n}]$, such that $a\not\equiv0$ (mod $k$).
\end{lemm}
\begin{proof}
(TODO)
\end{proof}


The protocol for QBF' begins as follows:
\begin{itemize}
    \item \textbf{Prover:} compute $a=arith(F)$; compute (if possible -- $a$ can be $0$) $k$ as in the statement of the above lemma; let $\hat a = a$ (mod $k$); find a proof $b$ for $k$ being prime; send $\hat a,k,b$ to the verifier.
    \item \textbf{Verifier:} Check that $\hat a > 0$, that $k\in [2^n,2^{3n}]$ and that $b$ is a proof that $k$ is prime
\end{itemize}
Now, throughout the protocol, the prover has the task of convincing the verifier that the arithmetical value of $F$ modulo $k$ is indeed $\hat a$.

The protocol now proceed in rounds. In each round, the formula $F$ will be decomposed into its subformulas and the prover will then send the verifier the arithmetical values of the subformulas modulo $k$. For example, if $F = F_1\wedge F_2$, the prover sends two numbers, $a_1$ and $a_2$, and proofs that these are the arithmetical values of $F_1$ and $F_2$. The verifier will check that $a_1\cdot a_2 = \hat a$ (mod $k$). For a disjunction, the process is analogous.

Now, when $F$ is of the form $\forall x G$, the variable $x$ occurs freely in $G$, so its arithmetical value is not a number, but a polynomial $p_G$, whose coefficients the prover will send to the verifier. The verifier then checks hat $p_G(0)\cdot p_G(1) = \hat a$ (mod $k$), selects a random number $z\in\{0,\dots,k-1\}$ and communicates it with the prover. The prover is now expected to provide a proof that the arithmetic value of $G$ (with $x$ replaced with $z$) is $p_G(z)$. 
When $F$ is of the form $\exists x G$, everything proceeds the same way, except that the verifier checks that $p_G(0)+p_G(1) = \hat a$ (mod $k$).

We eventually arrive at the innermost part of $F$, its variables, which have been replaced by random numbers in previous rounds. Now the verifier only has to check that the arithmetical value given by the prover (modulo $k$) is correct. 

















\section{Kolmogorov complexity}\label{sec:kolmogorov}

The first ideas about measuring the amount of information in a string of bits were expressed in a description given by M. Minsky of the work of R. Solomonoff, in Minsky \cite{minsky:62}. 
The Kolmogorov complexity of finite strings, as we will defined it, was introduced, independently, by Kolmogorov \cite{kolmogorov:65,kolmogorov:68} and Chaitin \cite{chaitin:66,chaitin:69,chaitin:74}.
For an overview see Li and Vit{\'a}nyi \cite{li:97}.


Let $U$ be a fixed universal Turing machine.
The Kolmogorov complexity of a string $w$ (relative to $U$) is the length of the shortest pair $x = \cod{M,y}$, which, when given as input to $U$, will lead $U$ to write down $w$ as output. Formally:
\begin{align*}
    &K_U(w) = \min\{|\cod{M,y}\,:\,U(\cod{M,y}) = w\}&
\end{align*}

Since we can always input $w$ to the identity function, $K_U(w)$ is never empty and $K_U(w)\leq |w|+c$, where $c$ is the size of the smallest encoding of the identity function (which depends only on $U$). Now consider another universal Turing machine $U'$ and let $c$ be size of the smallest program that allows $U'$ to simulate $U$. Then, $K_{U'}(w)\leq K_U(w)+c$. Thus, as long as we are willing to ignore constant additive factors, we can consider \textit{the} definition of Kolmogorov complexity of a string $w$. 


\begin{prop}\label{prop:high_kolmogorov}
There are at most $2^{m+1}-1$ strings $w$, having Kolmogorov complexity $K(w)\leq m$.
\end{prop}
\begin{proof}
Since there are at most $2^k$ programs of size $k$, the number of possible programs of bounded by $m$ is at most $2^{m+1}-1$. Each of these programs allows the universal machine to write down at most one string, whence the statement follows.
\end{proof}


Given any number $n$, we can trivially produce a program that outputs it, so the size of \textit{some} program that outputs $n$ can be computationally found.
The situation is very different when we ask if we can find the \textit{smallest} such program.
As we saw in the previous chapter, notions regarding the equivalence of programs are usually undecidable. We thus have the (not so surprising) result below. 

%Since there are at most $2^k$ programs of length $k$, there can be at most $2^k$ strings with $K(w) = k$. Thus, the maximum number of strings with Kolmogorov complexity $\leq k$ is $2^{k+1}-1$.
%Note that, if we were restricting ourselves to Primitive Recursive Programs, then we could simply enumerate every program and test which one is the first that outputs $n$ on input $\vare$. When we deal with a universal programming language, this cannot be done, because we cannot even ever be sure that the functions we are enumerating even halt.
%Note that, if we know the size of the smallest program that outputs $w$, we can find the said program by taking all programs of the given size and running them in parallel, to test which one outputs $w$.


\begin{prop}\label{prop:K_not_computable}
The function $x\mapsto K(x)$ is not computable.
\end{prop}
\begin{proof}
We will show something stronger: no partial computable function $\phi$ defined on an infinite set of points can coincide with $K(x)$ over the whole of its domain.

As we showed in the first chapter (Proposition \ref{prop:infinite_recursive_subset}), every infinite r.e. set contains an infinite computable subset.
Select an infinite computable subset $A$ in the domain of $\phi$. The function $\psi(m) = \min\{x\,:\, K(x) \geq m, x \in A\}$ is (total) computable (since $K(x) = \phi(x)$ on $A$), and takes arbitrarily large values, since it cannot be bounded for infinitely many $x$.

By definition of $\psi$, we have $K(\psi(m)) \geq m$.
On the other hand, by definition of $K$, $K(\psi(m)) \leq |m|+ |\cod{\psi}|$, where $|\cod{\psi}|$ is the size of the code of some Turing machine computing $\phi$. 
But then we have that $m \leq |m| + |\cod{\psi}|$, which is false from some $m$ onward, since $|m|\in O(\log(m))$ and $\cod{\psi}$ is fixed.
\end{proof}


% TODO - Da para fazer redução ao Hlating problem o que é giro


\subsection{Applications in number theory}


We start with some examples of how Kolmogorov complexity may be used to prove results from number theory.
First, we prove that there are infinitely many prime numbers. For a number $n$, let $bin(n)$ denote its binary representation.

\begin{prop}
There are infinitely many prime numbers.
\end{prop}
\begin{proof}
Suppose that there are only finitely many prime numbers, say $p_1,\dots,p_k$. Then, since each natural number is uniquely defined by its prime factorization, every number $w$ can be described by the $k$ exponents to which the prime numbers have to be raised, in order for $w$ to be obtained.
Since these exponents must be bounded by $\log(|w|)$, their length must be logarithmic in the length of $w$.

A universal machine can reconstruct $w$ from a program of constant size, containing a table with the $k$ prime numbers, and the exponents, which, together, have a size bounded by $k\cdot\log(|w|)$. 
Fix a size $n$. Then, for every $w$, with $|w|\leq n$, $K(w)\leq s + k\cdot\log(n)$. 
However, by Proposition \ref{prop:high_kolmogorov}, there are at most $2^{s+1}|n|^k+1$ words of length bounded by $|n|$ with such a Kolmogorov complexity, so, for $n$ great enough, we get a contradiction.
\end{proof}

This argument can be pushed further. The Prime Number Theorem is a famous theorem in number theory, which says that $\pi(n)$, the number of prime numbers below $n$, is asymptotic to $n/\ln(n)$. We prove something weaker with Kolmogorov complexity.


\begin{prop}
For infinitely many $n$, $\pi(n)\geq n/\log^2(n)$.
\end{prop}
\begin{proof}
Let $p_m$ be the $m$th prime number. It is sufficient to show that, for infinitely many $m$, $p_m\leq m\log^2(m)$, since this implies that $\pi(m)$ primes can be found below $m/\log^2(m)$.

Now let $n$ be a number such that $K(bin(n)) \geq \log(n)$ and let $p_m$ be the largest prime number that divides $n$. The number $n$ can be (algorithmically) reconstructed from $m$ and $n/p_m$, so a bit string that codes these two numbers is a sufficient description of $n$. Thus, $\log(n)\leq K(bin(n)) \leq |$encoding of $m$ and $n/p_m|$.

If we simply concatenate $m$ and $n/p_m$, we won't know where one ends and the next begins, so we must encode them somehow. Different encodings will yields different bounds for the theorem. 

We say that an encoding scheme is self-terminating if, from any string of the form $code(w)v$, it is possible to recover $code(w)$ algorithmically. For $w = a_1\dots a_k$, let $\overline{w} = a_10a_20\dots a_{n-1}0a_n1$. The coding of $w$, $code(w) = \overline{bin(|w|)}w$ is self-terminating and $|code(w)| = 2\log(|w|) + |w|$. Thus, if we encode $m$ and $n/p_m$ as $code(m)bin(n/p_m)$, we get
\begin{align*}
    \log(n)&\leq K(bin(n))\leq 2\log(|m|) + |m| + \log(n/p_m)&\\
    &\leq \log(\log^2(m)) + \log(m) + \log(n/p_m)&\\
    &= \log(m\log^2(m)) + \log(n) - \log(p_m)&
\end{align*}
whence
\begin{align*}
    &\log(p_m) \leq \log(m\log^2(m))\,\,\Leftrightarrow\,\,p_m\leq m\log^2(m)&
\end{align*}
\end{proof}


Consider, for example, that we encode $m$ and $n/p_m$ as $\overline{bin(m)}bin(n/p_m)$. Then, we would get the bound
\begin{align*}
    &\log(n)\leq K(bin(n))\leq 2\log m + \log(n/p_m) \,\,\Leftrightarrow\,\,p_m\leq m^2&
\end{align*}
which gives a weaker version of the above theorem: $\pi(n)\geq \sqrt{n}$. 
It is interesting to note that any improvement in the length of self-terminating codes can be translated directly into a sharper bound in this ``Weak Prime Number Theorem''. 



\begin{prop}
If $n$ is a sufficiently large number and $K(bin(n))\geq\log n$, $n$ is not a prime number
\end{prop}
\begin{proof}
%Suppose that $K(bin(n)) \geq \log n$ and 
The $m$th prime number $p_m$ can be obtained by searching for the first number that is not divisible by either of the first $m-1$ prime numbers. The Kolmogorov complexity of a prime number must then be smaller than some constant plus the codification of smaller prime numbers, whence $K(bin(n))\leq \log(\pi(n)) + C$.

Since $\pi(n) \sim n/\ln n$, there about less than $n/\log n$ prime numbers below $n$, so, if $n$ is prime, $K(bin(n))$ is bounded above by $\log(n) - \log(\log(n)) + C$.
\end{proof}





\subsection{Applications in proving lower bounds}

The method of using Kolmogorov complexity to establish lower bounds works as follows: Suppose we want to prove a lower bound on the running time of a Turing machine to perform a certain task. Let $x$ be a sufficiently long Kolmogorov random string, i.e., with $K(x) \geq |x|$. 
Now we assume that the lower bound we are seeking is violated, e.g., there is a Turing machine that performs the given task more quickly than the stated bound. 
Then, perhaps there is a way to use this Turing machine to describe the string $x$ with fewer than $|x|$ bits, which is a contradiction that establish the lower bound. Laplante \cite{laplante:06} contains an overview of lower bounds with Kolmogorov complexity.


We will use the concept of a crossing sequence. Let $M$ be a Turing machine and let $i$ be some boundary between two adjacent cells.
On a given input $x$, the crossing sequence for $x$ and $i$, $CS_M(x,i)$, is the concatenation of the states of $M$, whenever its head crosses the boundary $i$. Note that
\begin{align*}
    &\sum_{i=-\infty}^{\infty} CS_M(x,i) = time_M(x)&
\end{align*}
whence crossing sequences can give us information about running times. Before giving the first example of a lower bounds, we start with two auxiliary results.

\begin{prop}
Let $i = |x| = |x'|$. 
\begin{itemize}
    \item If $CS_M(xy,i) = CS_M(xz,i)$, then $xy \in L(M)$ if and only if $xz \in L(M)$.
    \item If $c = CS_M(xy, i) =  CS_M(x'y', i)$, then $c = CS_M(xy', i) = CS_M(x'y, i)$
\end{itemize}
\end{prop}
\begin{proof}
For the first statement, suppose WLOG that the Turing machine always $M$ ends its computation with the head in the $0$th position. This means that, when $i$ is crossed for the last time, the head must be heading left, otherwise it could not reach cell $0$ without passing $i$ again. But this means that, if $CS_M(xy,i) = CS_M(xz,i)$, the state in which $M$ is in when it last crosses $i$ is the same for both inputs, and, from that point on, it will only read the $x$ part of the input, whence $xy\in L(M)$ if and only if $xz \in L(M)$.


For the second statement, suppose that $M$, on input $xy'$, crosses the boundary $i$ to the right, for the first time. Then, since $CS_M(xy, i) =  CS_M(x'y', i)$, $M$ must be in the state it was, when it crossed $i$ for the first time on input $x'y'$. But since $|x'|=|x|=i$, this means that $M$, for both inputs, is now reading the same thing, in the same state, whence its next moves (while it stays to the right of $i$) must be the same. 
Thus, the next (possible) state to be added to $CS_M(xy', i)$ is the same as teh one that is added to $CS_M(x'y', i)$.
Analogously, the stated equality follows.
\end{proof}

We now prove some lower bounds.
%We will restrict our attention to the crossing sequences that occur in the middle $|x|/3$ positions of an input $x$. This is the portion that should consists entirely of $0$'s.

\begin{prop}
Any one-tape Turing machine that decides the language $L = \{w0^{|w|}w \,:\, w\in \{0,1\}^* \}$ requires, at least, quadratic time.
\end{prop}
\begin{proof}
To show that $time_M(x) = \Omega(n^2)$, it is sufficient to show that $n$ crossing sequences for $x$ each have length at least $\Omega(n)$. 

Let $M$ be a Turing machine that accepts $L$. Let $w0^{|w|}w$ be an input for $M$ and let $|w|\leq i\leq 2|w|$. By the previous proposition, we know that, for distinct $w$ and $w'$ with $|w| = |w'|$, the crossing sequences $CS_M(w0^{|w|}w, i)$ and $CS_M(w'0^{|w'|}w', i)$ must also be distinct, otherwise $CS_M(w0^{|w'|}w', i)$ and $CS_M(w0^{|w|}w, i)$ would be equal, whence $w0^{|w'|}w$ would be in $L(M)$, which is not true.

Since each $w$ yields a different $CS_M(w0^{|w|}w, i)$, we can design an algorithm which, given as input a machine $M$, two number $m$ and $m\leq i\leq 2m$ and a crossing sequence $c$, outputs the string $w$ of length $m$ for which $CS_M(w0^{|w|}w, i) = c$. 

Thus, $w$ can be described using information of size $\Oo(\log(|w|))$ and a crossing sequence, whence $K(w) \leq \Oo(\log |w|) + |CS_M(w0^{|w|}w, i)|$. If $w$ is chosen such that $K(w) \geq |w|$, we get that $|c| \geq |w| - \Oo(\log n)$. 

This means that at least $|w|$ crossing references have a size at least $|w| - \Oo(\log n)$, whence the lower bound follows.
\end{proof}

Analogously, we get that the language $L = \{w0^{|w|}w \,:\, w\in \{0,1\}^* \}$ requires quadratic time.
When we consider languages without the segment $0^{|w|}$ we have to perform some alterations to the proof, since the $n$ crossing sequences were obtained by taking $i$ between $|w|$ and $2|w|$.


For the languages such as $L = \{ww^R \,:\, w\in \{0,1\}^* \}$, we don't have the segment $0^{|w|}$, but can still the same word as in the proof above, since, for even length words $w0^{|w|}w^R\in L$.



\section{Complexity and Ordinary Differential Equations}


In this section we study the expressive and computational power of discrete Ordinary Differential Equations (ODEs).



\subsection{Programming with discrete ODE}


The discrete derivative of $f(x)$ is defined as $\Delta f(x) = f(x+1)-f(x)$. We will also write $f'$ for $\Delta f(x)$ to help to understand statements with respect to their classical continuous counterparts.


We will focus in on discrete ODE on functions with several variables, that is to say for example on equations of the (possibly vectorial) form:
\begin{align*}
    &\frac{\partial f(x,y)}{\partial x} = h(f(x,y),x,y)&
\end{align*}
When some initial value for $f(0,y)$ is given, this is called an Initial Value Problem (IVP), also called a Cauchy Problem. Observe that an IVP always admits a unique solution over $\N$, since $f$ can be defined inductively with
$f(0, y) = g(y)$ and $f(x + 1, y) = f(x, y) + h(f(x, y), x, y)$.

Consider, for example the IVP
\begin{align*}
    &f'(x) = -f(x)+1&\\
    &f(0) = 0&
\end{align*}
The solution to it is the sign function $sg(x)$ over $\N$. Using it, we can define $if_{\N}(x, y, z)$ as $y + \overline{sg}_{\N}(x)\cdot(z - y)$.

The minimum of a function $\min f\,:\, x \mapsto \min\{f(y)\,:\,0 \leq y \leq x\}$ is given by $F(x, x)$ where $F$ can be computed recursively by
\begin{align*}
    &F(0) = f(0)&\\
    &F(t + 1, x) = if(F(t, x) < f(x), F(t, x), f(x))&
\end{align*}
This can be interpreted as a discrete ordinary differential equation
\begin{align*}
    &\frac{\partial F(t,x)}{\partial t} = 
    \begin{cases}
        0 & F(t, x) < f(x)\\
        -F(t, x) + f(x) & F(t, x) \geq f(x)
    \end{cases}&
\end{align*}

Notice that this algorithm is not polynomial as this basically takes time $x$ to compute $\min f$, i.e. not polynomial with respect to the usual convention for measuring complexity based on the binary length of arguments.


Let $f, h$ be some functions with $h$ being non decreasing and let $some_h(x) := y$ such that $|f(x) - h(y)|$ is minimal. The function $some_h$ can be computed as a solution of an ODE as in the preceding example.
However, there is a more efficient way to do it based on what one usually does with classical ordinary differential equations: performing a change of variable so that it becomes logarithmic in $x$.
Indeed, we write $some_h(x) = G(l(x), x)$, where $G(t,x)$ is the solution to
\begin{align*}
    &\frac{\partial G(t,x)}{\partial t} = 
    \begin{cases}
        2^{l(x)-t-1} & h(G(t, x)) > g(x)\\
        0 & h(G(t, x)) = g(x)\\
        -2^{l(x)-t-1} & h(G(t, x)) < g(x)
    \end{cases}& 
\end{align*}

This is a differential equation whose solution converges in polynomial time to what we want. Reformulating what we just did, we wrote $some_h(x) = G(l(x), x)$ using the solution of the above discrete ODE. This provides a polynomial time algorithm to solve our problems using a new parameter $t = l(x)$ logarithmic in $x$. Such techniques will be at the heart of the coming results.

After this discussion, the rest of this section aims at discussing which problems can be solved using discrete ordinary differential equations, and with which complexity.


\begin{defn}
Given $g\,:\,\N^p\to \N$ and $h\,:\,\Z \times \N^{p+1}\to\Z$, we say that $f$ is is defined by discrete ODE solving from $g$ and $h$, denoted by $f = ODE(g, h)$, if $f\,:\,\N^{p+1}\to\Z$ corresponds to the (necessarily unique) solution of IVP
\begin{align*}
    &\frac{\partial f(x,y)}{\partial x} = h(f(x,y),x,y)&\\
    &f(0,y) = g(y)&
\end{align*}
\end{defn}


Consider that $f$ is defined by primitive recursion from $g$ and $h$. Then $f = ODE(g, \overline{h})$, where $\overline{h}\,:\,\N^{p+2}\to\Z$ is defined by $\overline{h}(f(x, y), x, y) = h(f(x, y), x, y) - f(x, y)$. Indeed,
\begin{align*}
    &f(x+1,y) = h(f(x, y), x, y) = \overline{h}(f(x, y), x, y) + f(x,y)&\\
    &\Rightarrow \frac{\partial f(x,y)}{\partial x} = \overline{h}(f(x, y), x, y)&
\end{align*}


This observation leads to the following characterization.

\begin{prop}
The set of primitive recursive functions is the intersection with $\N^\N$ of the smallest set of
functions that contains the zero, projection, addition and subtraction functions, and that is closed under composition and discrete ODE schemata.
\end{prop}


Actually, this is even possible to be more precise, and provide a characterization of the various subrecursive classes introduced up to now.

\subsection{Characterization of FP}

\begin{defn}
Given $g\,:\,\N^p \to \N$, $a\,:\,\N^{p+1}\to\Z$ and $b : N^{p+1}\to\Z$, we say that $f$ is obtained by linear ODE solving from $g$, $a$ and $b$, denoted by $f = LI(g, a, b)$, if $f\,:\,\N^{p+1} \to \Z$ corresponds to the (necessarily unique) solution of the IVP
\begin{align*}
    &\frac{\partial f(x,y)}{\partial x} = a(x, y) \cdot f(x, y) + b(x, y)&\\
    &f(0,y) = g(y)&
\end{align*}
\end{defn}

A vectorial function (resp. a matrix or a vector) is said to be a $sg$-polynomial expression if all its coordinates
(resp. coefficients) are.
It is said to be essentially constant if all its coefficients are.
A (possibly vectorial) $sg$-polynomial expression $g(f(x, y), x, y)$ is said to be \textit{essentially linear} in $f(x, y)$ if it is of the form
\begin{align*}
    &A[f(x, y), h(x, y), x, y] \cdot f(x, y) + B[f(x, y), h(x, y), x, y]&    
\end{align*}
where $A$ and $B$ are $sg$-polynomial expressions essentially constant in $f(x, y)$.

\begin{defn}
A function $f$ is linear $\L$-ODE definable (from $u$ and $g$) if it corresponds to the solution of $\L$-IVP
\begin{align*}
    &\frac{\partial f(x,y)}{\partial \L} = u(f(x, y), h(x, y), x, y)&\\
    &f(0,y) = g(y)&
\end{align*}
where $u$ is essentially linear in $f(x, y)$. When $\L(x, y) = l(x)$, such a system is called linear length-ODE.
\end{defn}



\begin{defn}
Let $\mathbb{DL}$ be the smallest subset of functions, that contains 0, 1, projections, the length function $l(x)$, the addition function $x+y$, the subtraction function $x-y$, the multiplication function $x \times y$ (often denoted $x \cdot y$), the sign function $sg(x)$ and closed under composition (when defined) and linear length-ODE scheme.
\end{defn}


We will split the proof of the characterization in two.

\begin{prop}
Suppose that $f(0, y) = g(y)$ and $h$ are computable in polynomial time and that there exist $c\in \N$, such that, for each $y$, $|Jump_\L(y)| \leq l(x)^c$. Then, if $f(x,y)$ is the solution of a linear length-ODE scheme with $g$ and $h$, $f$ can be computed in polynomial time.
\end{prop}
\begin{proof}
(TODO)
\end{proof}





\section{The graph isomorphism problem in quasi-polynomial time}


The GI is a problem not known to be NP-hard and not known to have a polynomial time algorithm. We describe here the advances made by Babai \cite{babai:16} to show that GI can be computed in quasi-polynomial time $n^{polylog(n)}$, where $n$ is the number of vertices of a graph. This came from \cite{grohe:20}.


\subsection{Permutation Groups}

A permutation group acting on a set $\Omega$ is a subgroup $\Gamma\leq Sym(\Omega)$ of the symmetric group. 
The size of the permutation domain $\Omega$ is called the degree of $\Gamma$.
Throughout this section, the degree is denoted by $n := |\Omega|$.
If $\Omega = [n]$, then we also write $S_n$ instead of $Sym(\Omega)$.
The alternating group on a set $\Omega$ is denoted by $Alt(\Omega)$.
As before, we write $A_n$ instead of $Alt(\Omega)$ if $\Omega = [n]$.
For $\gamma\in\Gamma$ and $\alpha\in\Omega$ we denote by $\alpha^\gamma$ the image of $\alpha$ under the permutation $\gamma$.
The set $\alpha^\Gamma := \{\alpha^\gamma\,:\, \gamma\in\Gamma\}$ is the orbit of $\alpha$.
The group $\Gamma$ is transitive if $\alpha^\Gamma = \Omega$ for some (and therefore every) $\alpha\in\Omega$.

For $\alpha\in\Omega$, the group $\Gamma\alpha := \{\gamma \in\Gamma\,:\, \alpha^\gamma = \alpha\}\leq \Gamma$ is the stabilizer of $\alpha$ in $\Gamma$.
The group $\Gamma$ is semi-regular if $\Gamma\alpha = \{id\}$ for every $\alpha\in\Omega$.
The pointwise stabilizer of a set $A \subseteq \Omega$ is the subgroup $\Gamma(A):= \{\gamma \in \Gamma\,:\,\forall \alpha \in A\,\,\alpha^\gamma = \alpha\}$.

For $A \subseteq \Omega$ and $\gamma \in \Gamma$ let $A^\gamma:= \{\alpha\,:\,\alpha \in A\}$.
The set $A$ is $\Gamma$-invariant if $A^\gamma = A$ for all $\gamma\in\Gamma$.
The setwise stabilizer of a set $A \subseteq \Omega$ is the subgroup $\Gamma A := \{\gamma \in\Gamma\,:\,A^\gamma = A\}$.
For $A \subseteq \Omega$ and a bijection $\theta\,:\,\Omega\to\Omega'$, we denote by $\theta[A]$ the restriction of $\theta$ to the domain $A$.
For a $\Gamma$-invariant set $A\subseteq \Omega$, we denote by $\Gamma[A] := \{\gamma[A]\,:\,\gamma\in\Gamma\}$ the induced action of $\Gamma$ on $A$, i.e., the group obtained from $\Gamma$ by restricting all permutations to $A$.

Let $\Gamma\leq Sym(\Omega)$ be a transitive group. A block of $\Gamma$ is a nonempty subset $B \subseteq\Omega$ such that $B^\gamma = B$ or $B^\gamma \cap B = \emptyset$ for all $\gamma\in\Gamma$.
The trivial blocks are $\Omega$ and the singletons $\{\alpha\}$, for $\alpha\in\Omega$.
The group $\Gamma$ is called primitive if there are no non-trivial blocks.

If $B \subseteq \Omega$ is a block of $\Gamma$, then $\frak B := \{B^\gamma\,:\,\gamma\in\Gamma\}$ builds a block system of $\Gamma$.
Note that $\frak B$ is an equipartition of $\Omega$.

The group $\Gamma_{(\frak B)} := \{\gamma\in\Gamma\,:\,\forall B \in \frak B B^\gamma = B\}$ denotes the subgroup stabilizing each block $B \in \frak B$ setwise.
Moreover, the natural action of $\Gamma$ on the block system $\frak B$ is denoted by $\Gamma[\frak B] \leq Sym(\frak B)$.
Let $\frak B, \frak B'$ be two partitions of $\Omega$. We say that $\frak B$ refines $\frak B'$, denoted $\frak B \precl \frak B'$, if, for every $B \in \frak B$ there is some $B'\in\frak B'$ such that $B \subseteq B'$. 
If additionally $B \neq B'$ we write $\frak B \prec \frak B'$.

A block system $\frak B$ is minimal if there is no non-trivial block system $\frak B'$ such that $\frak B \prec \frak B'$. A block system $\frak B$ is minimal if and only if $\Gamma[\frak B]$ is primitive.

Let $\Gamma \leq Sym(\Omega)$ and $\Gamma' \leq Sym(\Omega')$.
A homomorphism is a mapping $\varphi\,:\,\Gamma\to\Gamma'$ such that $\varphi(\gamma)\varphi(\delta) = \varphi(\gamma\delta)$ for all $\gamma,\delta\in\Gamma$.

For $\gamma\in\Gamma$ we denote by $\gamma^\varphi$ the $\varphi$-image of $\gamma$.
Similarly, for $\Delta\leq\Gamma$ we denote by $\delta^\varphi$ the $\varphi$-image of $\Delta$ (note that $\Delta^\varphi$ is a subgroup of $\Gamma'$).


\subsection{The Weisfeiler-Leman Algorithm}

\subsubsection{The Colour Refinement Algorithm}

An isomorphism between graphs $G$ and $H$ is a map between its vertices, that preserves adjacencies and non-adjacencies, i.e., $vw\in E(G)$ if and only if $\varphi(v)\varphi(w)\in E(H)$.

The procedures will revolve around coloring. Given a graph $G=(V,E)$, a coloring is a function $\chi_V$ (resp. $\chi_E$), which assigns, to each vertex (resp. arc) a color. 
Some notation: $\{\{\}\}$ denotes a multiset (there may be repeated elements) and $N_G(v)$ denotes the set of neighbors of a vertex $v$.

An assignment of colours to the vertices of a graph is stable if any two vertices of the same colour have identically coloured neighbourhoods. The goal of the Colour Refinement Algorithm, which was first proposed in \cite{morgan:65}, is to find a stable colouring that uses a minimum number of colours.

The algorithm works by finding successive refinements of an initial coloring of $G$. If $\chi_1(v)=\chi_1(w)$ implies $\chi_2(v)=\chi_2(w)$, we say that $\chi_1$ refines $\chi_2$, denoted by $\chi_1\precl\chi_2$.


\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $(G,\chi_V,\chi_E)$\;
$\chi_{(0)}(G) = \chi_V$\;
\While{$\chi_{(i)}(G)\neq\chi_{(i+1)}(G)$}{
    $\chi_{(i)}(G)(v) = (\chi_{(i-1)}(G)(v),\{\{(\chi_{(i-1)}(G)(v),\chi_E(v,w),\chi_E(w,v))\,:\,w\in N_G(v)\}\})$
}
\Return $\chi_{(i)}(G)$
\caption{Colour Refinement Algorithm}    
\end{algorithm}
\end{center}

Note how the ``color'' in $\chi_{(i+1)}$ is a pair formed from the previous color and the multiset of the colors of its neighbors.
One can prove that CRA terminates in time $\Oo((n+m)\log(n))$ (see \cite{berkholz:17}), where $n$ is the number of vertices and $m$ the number of edges of a graph $G$.

Note that the refinement property trivially holds, since, if $\chi_{(i)}(G)(v) = \chi_{(i)}(G)(w)$, then the first component of the pair defining $\chi_{(i)}$ must be equal and this is $\chi_{(i-1)}$.

\subsubsection{Higher Dimensions}


In the $k$-dimensional Weisfeiler-Leman algorithm (k-WL), we color $k$-tuples of vertices, instead of vertices. To describe it, we use the notion of atomic type. $apt(G,\overline v) = apt(H,\overline w)$ if and only if the mapping $v_i\to w_i$ is an isomorphism from $G[\overline v]$ to $H[\overline w]$.

\begin{center}
\begin{algorithm}[H]
\textbf{Input:} $(G,\chi_V,\chi_E)$\;
$\chi_{(0)}^k(G)(\overline v) = apt(G,\overline v)$\;
\While{$\chi_{(i)}^k(G)\neq\chi_{(i+1)}^k(G)$}{
    $\chi_{(i)}(G)(\overline v) = (\chi_{(i-1)}^k(G)(v),\{\{(apt(G,\overline vv),\chi_{(i-1)}^k(G)(\overline v[v/1]),\dots,\chi_{(i-1)}^k(G)(\overline v[v/k]))\,:\,v\in V(G)\}\})$
}
\Return $\chi_{(i)}^k(G)$
\caption{$k$-dimensional Weisfeiler-Leman}    
\end{algorithm}
\end{center}

Here, $\overline vv$ denotes concatenation and $\overline{v}[v/i]$ is $\overline{v}$ with the $i$th entry replaced with $v$. 
The Color Refinement algorithm is essentially $1$-WL, since
\begin{align*}
    &\chi_{(i)}(G)(v)=\chi_{(i)}(H)(w)\,\,\Leftrightarrow\,\,\chi_{(i)}^1(G)(v)=\chi_{(i)}^1(H)(w)&
\end{align*}
and we will no longer distinguish between the two.


The idea now is to try to use this algorithm to distinguish graphs, by comparing their color patterns.

We say that $k$-WL distinguishes graphs $G$ and $H$ if there is a color $c$ such that
\begin{align*}
    &\big|\left\{\overline v \in V(G)^k\,:\,\chi_{(\infty)}^k(G)(\overline v) = c\right\}\big|\neq \big|\left\{\overline w \in V(H)^k\,:\,\chi_{(\infty)}^k(H)(\overline w) = c\right\}\big|&
\end{align*}
This is an incomplete isomorphism test: if it distinguishes two graphs, we know that they are non-isomorphic, but there are non-isomorphic graphs that $k$-WL does not distinguish.\footnote{For example, a cycle of size $6$ and two disjoint triangles are not distinguishable by $1$-KW.}
Let us call two graphs that are not distinguished by the $k$-WL $k$-\textit{indistinguishable}.

By Cai, Fürer and Immerman \cite{cai:92}, for every $k \geq 1$ there exist non-isomorphic $3$-regular graphs $G_k$, $H_k$ of order $|G_k| = |H_k| = \Oo(k)$, that are $k$-indistinguishable.
While this shows the limitation of the Weisfeiler-Leman algorithm, it still remains quite powerful. For example $1$-WL identifies almost all graphs, in the sense that in the limit, the fraction of $n$-vertex graphs identified by
$1$-WL is $1$ as $n$ goes to infinity \cite{babai:80}, and $2$-WL identifies almost all $d$-regular graphs \cite{bollobas:82}.

% A aprtir de seciton 4


\subsection{Luks's algorithm}

In order to apply group-theoretic techniques to the Graph Isomorphism Problem, Luks \cite{luks:82} introduced a more general problem that allows to build recursive algorithms along the structure of the permutation groups involved. To disucss this problem, we have to introduce some notation.

A string is a mapping $\mathfrak{r}\,:\,\Omega\to\Sigma$ where $\Omega$ and $\Sigma$ are finite sets. Let $\gamma \in Sym(\Omega)$ be a permutation. The permutation $\gamma$ can be applied to the string $\mathfrak{r}$ by defining
\begin{align*}
    \mathfrak{r}^\gamma\,:\,&\Omega\longrightarrow\Sigma&\\
    &\alpha\longmapsto\mathfrak{r}(\alpha^{\gamma^{-1}})&
\end{align*}
Let $\mathfrak{n}\,:\,\Omega\to\Sigma$ be another string.
The permutation $\gamma$ is an isomorphism from $\frak{r}$ to $\frak{n}$, denoted $\gamma\,:\,\frak{r}\cong\frak{n}$, if $\frak{r}^\gamma = \frak{n}$.

Let $\Gamma\leq Sym(\Omega)$ (sub-group). A $\Gamma$-isomorphism from $\frak r$ to $\frak n$ is a permutation $\gamma\in\Gamma$ such that $\gamma\,:\,\frak{r}\simeq\frak{n}$. The strings $\frak r$ and $\frak n$ are $\Gamma$-isomorphic, denoted $\frak r\simeq_\Gamma \frak n$, if there is a $\Gamma$-isomorphism between them.

\textit{The String Isomorphism Problem}: given two strings $\frak r,\frak n:\Omega\to\Sigma$ and a generating set for a group $\Gamma\leq Sym(\Omega)$, assert whether $\frak r$ and $\frak n$ are $\Gamma$-isomorphic.


\begin{prop}
There is a polynomial-time many-one reduction from the Graph Isomorphism Problem to the String Isomorphism Problem.
\end{prop}
\begin{proof}
Let $G$ and $H$ be two graphs and assume without loss of generality $V(G) = V(H) = V$. Let $\Omega =\binom{V}{2}$ and $\Gamma\leq Sym(\Omega)$ be a natural action of $Sym(V)$ on the two-element subsets of $V$.

Let $\frak r,\frak n: \Omega\to\{0,1\}$ with $\frak r(v w) = 1$ (resp. $\frak n(vw) = 1$) if and only if $vw \in E(G)$ (resp. $vw \in E(H)$). Then, $G \cong H$ if and only if $\frak r \cong \frak n$. 
% and let $\varphi: Sym(V) \to \Gamma$ be the natural homomorphism. %\gamma^\varphi : 
\end{proof}

The main advantage of the String Isomorphism Problem is that it naturally allows for algorithmic approaches based on group-theoretic techniques.


\subsubsection{Recursion mechanisms}


For $K\subseteq Sym(\Omega)$ and $W\subseteq\Sigma$ let
\begin{align*}
    &Iso_K^W(\frak r,\frak n):=\{\gamma\in K\,:\,\forall\alpha\in W\,\,\frak r(\alpha) = \frak n(\alpha^\gamma)\}&
\end{align*}
In what follows, $K$ will always be a coset $\Gamma\gamma$ and $W$ will always be $\Gamma$-invariant. In this case, $Iso_K^W$ is either empty or a coset of $Aut_\Gamma^W(\frak r) = Iso_\Gamma^W(\frak r,\frak r)$, so we can represent $Iso_K^W$ with a generating set for $Aut_\Gamma^W(\frak r)$ and a permutation $\gamma$. 

Moreover, if $K=\Gamma\gamma$, 
\begin{align}
    &Iso_{\Gamma\gamma}^W(\frak r,\frak n) = Iso_\Gamma^W(\frak r,\frak n^{\gamma^{-1}})\gamma&\label{eq:K_group}
\end{align}
Using this identity, it is possible to restrict to the case where $K$ is actually a group.


For the first type of recursion, suppose $K = \Gamma \leq Sym(\Omega)$ is not transitive on $W$ and let $W_1,\dots, W_l$ be the orbits of $\Gamma[W]$.
Then, the strings can be processed, orbit by orbit, as follows: We iterate over all $i \in [l]$ and update $K := Iso_K^{W_i}(x, y)$ in each iteration.
In the end, $K = Iso_\Gamma^W(x, y)$.
This recursion mechanism is referred to as \textit{orbit-by-orbit processing}.

The set $Iso^{W_i}_K (x, y)$ can be computed by making one recursive call to the String Isomorphism Problem over domain size $n_i := |W_i|$.
Indeed, using Equation \eqref{eq:K_group}, it can be assumed that $K$ is a group and $W_i$ is $K$-invariant. Then,
\begin{align}
    &Iso^{W_i}_K(x, y) = \{\gamma \in K\,:\, \gamma[W_i] \in Iso_{K[Wi]}(\frak r[Wi],\frak n[Wi])\} &
\end{align}
where $\frak r[W_i]$ denotes the induced substring of $\frak r$ on the set $W_i$, i.e., $\frak r[W_i]: W_i \to
\Sigma\,:\, \alpha\mapsto\frak r(\alpha)$.
So, overall, if $\Gamma$ is not transitive, the set $Iso^W_\Gamma(x, y)$ can be computed by making $l$ recursive calls over window size $n_i = |W_i|$, $i \in [l]$.

For the second recursion mechanism let $\Delta\leq\Gamma$ and let $T$ be a transversal\footnote{A set $T \subseteq \Gamma$ is a transversal for $\Delta$ in $\Gamma$ if $|T| = |\Gamma|/|\Delta|$ and $\{\Delta\delta\,:\,\delta\in T \} = \{\Delta\delta\,:\,\delta\in\Gamma\}$ (such a set always exists).} for $\Delta$ in $\Gamma$.
Then,
\begin{align}
    &Iso^W_\Gamma(\frak r,\frak n) =\bigcup_{\delta\in T} Iso_{\Delta\delta}^W(\frak r,\frak n) = \bigcup_{\delta\in T} Iso_{\Delta}^W(\frak r,\frak n^{\delta^{-1}})\delta&
\end{align} 
Luks applied this type of recursion when $\Gamma$ is transitive (on $W$), $\frak B$ is a minimal block system for $\Gamma$, and $\Delta = \Gamma(\frak B)$.
In this case $\Gamma[\frak B]$ is a primitive group.

Let $t = |\Gamma[\frak B]|$ be the size of a transversal for $\Delta$ in $\Gamma$.
Note that $\Delta$ is not transitive (on $W$). Indeed, each orbit of $\Delta$ has size at most $n/b$ where $b = |\frak B|$.
So by combining both types of recursion the computation of $Iso^W_\Gamma(x, y)$ is reduced to $t\cdot b$ many instances of the String Isomorphism Problem over window size $|W|/b$.
This specific combination of types of recursion is referred to as standard Luks reduction.

Observe that the time complexity of standard Luks reduction is determined by the size of the primitive group $\Gamma[\frak B]$.
%Overall, Luks’s algorithm is formulated in Algorithm 1. The running time of this algorithm heavily depends on the size of the primitive groups involved in the computation.









\subsection{Babai's algorithm}



















\section{Philosophical aspects of Complexity}

(TODO)
